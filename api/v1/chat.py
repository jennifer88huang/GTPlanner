import asyncio
from typing import Any, Optional, List, Dict
import re

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from utils.call_llm import call_llm_async
from utils.multilingual_utils import determine_language

# å¯¼å…¥è§„åˆ’ç›¸å…³çš„åŠŸèƒ½
from api.v1.planning import (
    short_planning, long_planning,
    short_planning_stream, long_planning_stream,
    ShortPlanningRequest, LongPlanningRequest
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])


def stream_data(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆSSEæ ¼å¼è¦æ±‚ï¼‰
    ä½¿ç”¨å ä½ç¬¦ä¿æŠ¤markdownæ¢è¡Œç¬¦ï¼Œé¿å…ä¸SSEåè®®å†²çª
    """
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…ä¸SSEæ¶ˆæ¯åˆ†éš”ç¬¦å†²çª
    protected_content = content.replace('\n', '<|newline|>')
    return f"data: {protected_content}\n".encode('utf-8')


def stream_data_block(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®å—ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œå¹¶æ·»åŠ ç»“æŸæ ‡è®°
    """
    return f"data: {content}\n\n".encode('utf-8')


class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    message_type: Optional[str] = "message"  # 'message', 'plan', 'document'
    timestamp: Optional[int] = None


class ConversationRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    session_id: Optional[str] = None
    language: Optional[str] = None
    action: Optional[str] = None  # 'generate_document', 'optimize_plan'
    context: Optional[Dict[str, Any]] = None


class ConversationAction(BaseModel):
    type: str  # 'plan', 'document', 'suggestion'
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    language: Optional[str] = None
    user_id: Optional[str] = None


class IntentAnalysisResult(BaseModel):
    intent: str  # 'requirement' or 'conversation'
    confidence: float
    reasoning: str

async def process_intent_actions(intent: str, message: str, current_plan: str, actions: List[Dict], language: str) -> List[ConversationAction]:
    """
    æ ¹æ®æ„å›¾å¤„ç†ç‰¹æ®Šæ“ä½œï¼Œå¦‚ç”Ÿæˆè§„åˆ’æˆ–æ–‡æ¡£
    """
    processed_actions = []

    try:
        if intent == "requirement":
            # ç”ŸæˆçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="é¡¹ç›®è§„åˆ’" if language == "zh" else "Project Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "version": 1
                    }
                ))

        elif intent == "optimization" and current_plan:
            # ä¼˜åŒ–ç°æœ‰è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="ä¼˜åŒ–è§„åˆ’" if language == "zh" else "Optimized Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan,
                        "version": 2
                    }
                ))

        elif intent == "document_generation" and current_plan:
            # ç”Ÿæˆé•¿æ–‡æ¡£
            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            doc_result = await long_planning(doc_request)

            if "flow" in doc_result:
                processed_actions.append(ConversationAction(
                    type="document",
                    content=doc_result["flow"],
                    title="è®¾è®¡æ–‡æ¡£" if language == "zh" else "Design Document",
                    metadata={
                        "language": doc_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan
                    }
                ))

        # å¤„ç†LLMå»ºè®®çš„å…¶ä»–actions
        for action in actions:
            if isinstance(action, dict) and "type" in action and "content" in action:
                processed_actions.append(ConversationAction(
                    type=action.get("type", "suggestion"),
                    content=action.get("content", ""),
                    title=action.get("title"),
                    metadata=action.get("metadata", {})
                ))

    except Exception as e:
        print(f"Error processing intent actions: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå»ºè®®action
        processed_actions.append(ConversationAction(
            type="suggestion",
            content="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚" if language == "zh" else "There was an issue processing your request. Please try rephrasing your needs.",
            title="å»ºè®®" if language == "zh" else "Suggestion"
        ))

    return processed_actions


async def handle_action(action_content: str, original_message: str, current_plan: str, language: str):
    """
    å¤„ç†ACTIONæ ‡ç­¾ï¼Œè°ƒç”¨ç›¸åº”çš„æµå¼planningæ¥å£å¹¶è¿”å›æµå¼ç»“æœ
    """
    try:
        # è§£æactionç±»å‹å’Œå†…å®¹
        if action_content.startswith("short_plan:"):
            requirement = action_content[11:].strip() or original_message

            # è°ƒç”¨æµå¼çŸ­è§„åˆ’æ¥å£
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("optimize_plan:"):
            optimization = action_content[14:].strip() or original_message

            # è°ƒç”¨æµå¼è§„åˆ’ä¼˜åŒ–
            planning_request = ShortPlanningRequest(
                requirement=optimization,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("long_doc:"):
            doc_requirement = action_content[9:].strip() or original_message

            # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆ
            doc_request = LongPlanningRequest(
                requirement=doc_requirement,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in long_planning_stream(doc_request):
                if isinstance(chunk, str):
                    # long_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in handle_action: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ å¤„ç†æ“ä½œæ—¶å‡ºç°å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
        yield stream_data_block("[ERROR_END]")


async def generate_stream_response(
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    ç”ŸæˆåŸºäºæ ‡ç­¾çš„æµå¼å“åº”
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        context_messages = []
        for msg in conversation_history:
            msg_type = f" [{msg.message_type}]" if msg.message_type != "message" else ""
            context_messages.append(f"{msg.role}{msg_type}: {msg.content}")

        context_str = "\n".join(context_messages) if context_messages else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"

        # æ„å»ºç»Ÿä¸€å¯¹è¯æç¤ºè¯ï¼Œä½¿ç”¨æ ‡ç­¾è¾“å‡ºæ ¼å¼
        if language == "zh":
            prompt = f"""ä½ æ˜¯GTPlannerçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œé¡¹ç›®è§„åˆ’å’Œè®¾è®¡ã€‚

å¯¹è¯å†å²ï¼š
{context_str}

ç”¨æˆ·å½“å‰æ¶ˆæ¯ï¼š{message}

å½“å‰è§„åˆ’å†…å®¹ï¼š
{current_plan if current_plan else "æš‚æ— è§„åˆ’å†…å®¹"}

è¯·åˆ†æç”¨æˆ·çš„æ„å›¾å¹¶ç›¸åº”å›åº”ã€‚å¯èƒ½çš„æ„å›¾åŒ…æ‹¬ï¼š

1. **é¡¹ç›®éœ€æ±‚(requirement)**ï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ›å»ºã€å¼€å‘ã€è®¾è®¡ã€æ„å»ºå…·ä½“çš„ç³»ç»Ÿ/åº”ç”¨/é¡¹ç›®ï¼Œæˆ–æè¿°äº†å…·ä½“çš„åŠŸèƒ½éœ€æ±‚å’ŒæŠ€æœ¯æ–¹æ¡ˆã€‚

2. **ä¼˜åŒ–æ”¹è¿›(optimization)**ï¼šç”¨æˆ·å¯¹ç°æœ‰è§„åˆ’æå‡ºä¿®æ”¹å»ºè®®æˆ–ä¼˜åŒ–æ„è§ã€‚
   {"å½“å‰è§„åˆ’å†…å®¹ï¼š" + current_plan + "..." if current_plan else ""}

3. **æ–‡æ¡£ç”Ÿæˆ(document_generation)**ï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè¯¦ç»†æ–‡æ¡£ã€è®¾è®¡æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£ï¼Œæˆ–è€…è¯´"åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆæ–‡æ¡£"ã€"ç”Ÿæˆè®¾è®¡æ–‡æ¡£"ç­‰ã€‚

4. **æ™®é€šå¯¹è¯(conversation)**ï¼šç”¨æˆ·åªæ˜¯é—®å€™ã€æ„Ÿè°¢ã€è¯¢é—®æ¦‚å¿µã€å¯»æ±‚å»ºè®®æˆ–è¿›è¡Œä¸€èˆ¬æ€§è®¨è®ºã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡ç­¾æ ¼å¼è¾“å‡ºï¼Œæ³¨æ„æ ‡ç­¾æ ¼å¼å¿…é¡»å®Œå…¨æ­£ç¡®ï¼š

- æ™®é€šå¯¹è¯å›å¤ï¼š[TEXT_START]æ‚¨çš„å›å¤å†…å®¹ï¼ˆå¯åŒ…å«å»ºè®®ã€æç¤ºç­‰ï¼‰[TEXT_END]
- è§¦å‘çŸ­è§„åˆ’ï¼š[SHORT_PLAN_ACTION]ç”¨æˆ·éœ€æ±‚æè¿°[/SHORT_PLAN_ACTION]
- è§¦å‘é•¿æ–‡æ¡£ï¼š[LONG_DOC_ACTION]åŸºäºç°æœ‰è§„åˆ’ç”Ÿæˆæ–‡æ¡£[/LONG_DOC_ACTION]
- è§¦å‘è§„åˆ’ä¼˜åŒ–ï¼š[OPTIMIZE_PLAN_ACTION]ä¼˜åŒ–å»ºè®®[/OPTIMIZE_PLAN_ACTION]

æ³¨æ„ï¼šå»ºè®®å’Œæç¤ºå†…å®¹åº”è¯¥ç›´æ¥åŒ…å«åœ¨TEXTæ ‡ç­¾å†…ï¼Œä¸è¦ä½¿ç”¨å•ç‹¬çš„SUGGESTIONæ ‡ç­¾ã€‚

âš ï¸ æ ‡ç­¾æ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. æ™®é€šæ ‡ç­¾ç»“æŸæ ¼å¼ï¼š[TEXT_END] - ç»å¯¹ä¸è¦åŠ æ–œæ 
2. ACTIONæ ‡ç­¾ç»“æŸæ ¼å¼ï¼š[/SHORT_PLAN_ACTION] - å¿…é¡»æœ‰æ–œæ 
3. ä¸è¦åœ¨æ ‡ç­¾åå‰åæ·»åŠ ä»»ä½•é¢å¤–å­—ç¬¦
4. æ ‡ç­¾å¿…é¡»ç‹¬ç«‹æˆè¡Œæˆ–ç´§è´´å†…å®¹ï¼Œä¸è¦æœ‰å¤šä½™ç©ºæ ¼

æ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©åˆé€‚çš„æ ‡ç­¾è¾“å‡ºã€‚å¦‚æœæ˜¯é¡¹ç›®éœ€æ±‚ï¼Œå…ˆç”¨TEXTå›å¤ç„¶åè§¦å‘ç›¸åº”ACTIONï¼›å¦‚æœæ˜¯æ–‡æ¡£ç”Ÿæˆè¯·æ±‚ï¼Œå…ˆå›å¤ç„¶åè§¦å‘LONG_DOC_ACTIONï¼›å¦‚æœæ˜¯æ™®é€šå¯¹è¯ï¼Œåªç”¨TEXTæ ‡ç­¾å›å¤ã€‚"""


        else:
            prompt = f"""You are GTPlanner's AI assistant, specialized in helping users with project planning and design.

Conversation history:
{context_str}

Current user message: {message}

Please analyze the user's intent and respond accordingly. Possible intents include:

1. **Project Requirement (requirement)**: User explicitly requests to create, develop, design, or build specific systems/applications/projects, or describes specific functional requirements and technical solutions.

2. **Optimization (optimization)**: User provides feedback or suggestions to improve an existing plan.
   {f"Current plan content: {current_plan[:200]}..." if current_plan else ""}

3. **Document Generation (document_generation)**: User explicitly requests to generate a detailed document or to create documentation based on an existing plan.

4. **Normal Conversation (conversation)**: User is just greeting, thanking, asking about concepts, seeking advice, or having general discussion.

Please strictly follow the tag format below, ensuring the tag format is completely correct:

- Normal conversation: [TEXT_START]Your response content (can include suggestions, tips, etc.)[TEXT_END]
- Trigger short planning: [SHORT_PLAN_ACTION]user requirement description[/SHORT_PLAN_ACTION]
- Trigger long documentation: [LONG_DOC_ACTION]generate document based on existing plan[/LONG_DOC_ACTION]
- Trigger plan optimization: [OPTIMIZE_PLAN_ACTION]optimization suggestions[/OPTIMIZE_PLAN_ACTION]

Note: Suggestions and tips should be included directly within TEXT tags, do not use separate SUGGESTION tags.

âš ï¸ Tag Format Requirements (Must Follow Strictly):
1. Regular tag end format: [TEXT_END] - absolutely no slash
2. ACTION tag end format: [/SHORT_PLAN_ACTION] - must have slash
3. Do not add any extra characters before or after tag names
4. Tags must be on separate lines or directly adjacent to content, no extra spaces

Choose the appropriate tag based on user intent. For project requirements, reply with TEXT first then trigger corresponding ACTION; for document generation requests, reply first then trigger LONG_DOC_ACTION; for normal conversation, only use TEXT tags."""

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«å’Œå“åº”ç”Ÿæˆ
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        yield stream_data_block("[STATUS_END]")

        # å¯¼å…¥æµå¼LLMè°ƒç”¨
        from utils.call_llm import call_llm_stream_async

        # æµå¼è¾“å‡ºLLMçš„å“åº”å¹¶ç›‘å¬ACTIONæ ‡ç­¾
        action_buffer = []
        in_action = False
        action_type = None
        content_buffer = ""
        pending_output = ""  # ç¼“å†²å¾…è¾“å‡ºçš„å†…å®¹

        async for chunk in call_llm_stream_async(prompt):
            if chunk:
                content_buffer += chunk
                pending_output += chunk

                # å¤„ç†ç¼“å†²åŒºä¸­çš„å®Œæ•´æ ‡ç­¾
                while True:
                    # æŸ¥æ‰¾æ ‡ç­¾å¼€å§‹
                    start_pos = pending_output.find('[')
                    if start_pos == -1:
                        # æ²¡æœ‰æ ‡ç­¾ï¼Œè¾“å‡ºæ‰€æœ‰å†…å®¹
                        if pending_output and not in_action:
                            yield stream_data(pending_output)
                        elif pending_output and in_action:
                            action_buffer.append(pending_output)
                        pending_output = ""
                        break

                    # è¾“å‡ºæ ‡ç­¾å‰çš„å†…å®¹
                    if start_pos > 0:
                        before_tag = pending_output[:start_pos]
                        if not in_action:
                            yield stream_data(before_tag)
                        else:
                            action_buffer.append(before_tag)

                    # æŸ¥æ‰¾æ ‡ç­¾ç»“æŸ
                    end_pos = pending_output.find(']', start_pos)
                    if end_pos == -1:
                        # æ ‡ç­¾ä¸å®Œæ•´ï¼Œä¿ç•™ä»æ ‡ç­¾å¼€å§‹çš„æ‰€æœ‰å†…å®¹
                        pending_output = pending_output[start_pos:]
                        break

                    # æå–å®Œæ•´æ ‡ç­¾
                    complete_tag = pending_output[start_pos:end_pos + 1]
                    remaining_content = pending_output[end_pos + 1:]

                    # å¤„ç†æ ‡ç­¾
                    if complete_tag == "[SHORT_PLAN_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "short_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[LONG_DOC_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "long_doc"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[OPTIMIZE_PLAN_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "optimize_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag in ["[/SHORT_PLAN_ACTION]", "[/LONG_DOC_ACTION]", "[/OPTIMIZE_PLAN_ACTION]"] and in_action:
                        # å¤„ç†ACTIONå†…å®¹
                        action_content = ''.join(action_buffer).strip()
                        if action_content:
                            # æ ¹æ®action_typeæ„é€ action_content
                            full_action_content = f"{action_type}:{action_content}"
                            async for planning_chunk in handle_action(full_action_content, message, current_plan, language):
                                yield planning_chunk
                        in_action = False
                        action_buffer = []
                        action_type = None
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    else:
                        # éACTIONæ ‡ç­¾ï¼Œæ­£å¸¸å‘é€
                        if not in_action:
                            yield stream_data(complete_tag)
                        else:
                            action_buffer.append(complete_tag)

                    # ç»§ç»­å¤„ç†å‰©ä½™å†…å®¹
                    pending_output = remaining_content

        # å¤„ç†å¯èƒ½å‰©ä½™çš„å†…å®¹
        if pending_output:
            if not in_action:
                yield stream_data(pending_output)
            else:
                action_buffer.append(pending_output)

        yield stream_data_block("")  # ç©ºè¡Œè¡¨ç¤ºç»“æŸ

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_stream_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ ç”Ÿæˆå›å¤æ—¶å‡ºç°å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
        yield stream_data_block("[ERROR_END]")


async def generate_direct_action_response(
    action: str,
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    æ ¹æ®æ˜ç¡®çš„actionç›´æ¥ç”Ÿæˆå“åº”ï¼Œè·³è¿‡AIæ„å›¾è¯†åˆ«
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        if action == "generate_document":
            # ç›´æ¥è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆ
            if not current_plan:
                yield stream_data("[ERROR_START]")
                yield stream_data("âŒ æ²¡æœ‰æ‰¾åˆ°å½“å‰è§„åˆ’ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            yield stream_data("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯¦ç»†è®¾è®¡æ–‡æ¡£...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆæµå¼æ¥å£
            from api.v1.planning import long_planning_stream, LongPlanningRequest

            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in long_planning_stream(doc_request):
                yield chunk

        elif action == "optimize_plan":
            # ç›´æ¥è°ƒç”¨è§„åˆ’ä¼˜åŒ–
            if not current_plan:
                yield stream_data("[ERROR_START]")
                yield stream_data("âŒ æ²¡æœ‰æ‰¾åˆ°å½“å‰è§„åˆ’ï¼Œæ— æ³•è¿›è¡Œä¼˜åŒ–")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            yield stream_data("ğŸ”§ æ­£åœ¨ä¼˜åŒ–è§„åˆ’...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨çŸ­è§„åˆ’ä¼˜åŒ–æµå¼æ¥å£
            from api.v1.planning import short_planning_stream, ShortPlanningRequest

            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in short_planning_stream(planning_request):
                yield chunk

        else:
            yield stream_data("[ERROR_START]")
            yield stream_data(f"âŒ ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}")
            yield stream_data_block("[ERROR_END]")

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_direct_action_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ å¤„ç†æ“ä½œæ—¶å‡ºç°å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
        yield stream_data_block("[ERROR_END]")


@chat_router.post("/unified")
async def unified_conversation(body: ConversationRequest):
    """
    ç»Ÿä¸€å¯¹è¯æ¥å£ï¼šé›†æˆæ„å›¾è¯†åˆ«ã€å¯¹è¯å›å¤ã€è§„åˆ’ç”Ÿæˆå’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½
    å®Œå…¨ä½¿ç”¨æµå¼å“åº”
    """
    message = body.message
    conversation_history = body.conversation_history or []
    session_id = body.session_id
    language = body.language
    action = body.action
    context = body.context or {}

    if not message:
        async def error_stream():
            yield stream_data("[ERROR_START]")
            yield stream_data("âŒ Missing message in request body.")
            yield stream_data_block("[ERROR_END]")
        return StreamingResponse(error_stream(), media_type="text/plain")

    # ç¡®å®šè¯­è¨€
    if not language:
        language = determine_language(message, None, None)

    # æ ¹æ®actionå­—æ®µç›´æ¥è°ƒç”¨ç›¸åº”åŠŸèƒ½ï¼Œæˆ–ä½¿ç”¨AIæ„å›¾è¯†åˆ«
    if action:
        return StreamingResponse(
            generate_direct_action_response(action, message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )
    else:
        # è¿”å›æµå¼å“åº”ï¼ˆä½¿ç”¨AIæ„å›¾è¯†åˆ«ï¼‰
        return StreamingResponse(
            generate_stream_response(message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )