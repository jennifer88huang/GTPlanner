from typing import Any, Optional, List, Dict

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from utils.multilingual_utils import determine_language
# from utils.context_manager import optimize_conversation_context  # å·²ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½

# å¯¼å…¥è§„åˆ’ç›¸å…³çš„åŠŸèƒ½
from api.v1.planning import (
    short_planning, long_planning,
    short_planning_stream, long_planning_stream,
    ShortPlanningRequest, LongPlanningRequest
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])


def stream_data(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆSSEæ ¼å¼è¦æ±‚ï¼‰
    ä½¿ç”¨å ä½ç¬¦ä¿æŠ¤markdownæ¢è¡Œç¬¦ï¼Œé¿å…ä¸SSEåè®®å†²çª
    """
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…ä¸SSEæ¶ˆæ¯åˆ†éš”ç¬¦å†²çª
    protected_content = content.replace('\n', '<|newline|>')
    return f"data: {protected_content}\n".encode('utf-8')


def stream_data_block(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®å—ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œå¹¶æ·»åŠ ç»“æŸæ ‡è®°
    """
    return f"data: {content}\n\n".encode('utf-8')


class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    message_type: Optional[str] = "message"  # 'message', 'plan', 'document'
    timestamp: Optional[int] = None


class ConversationRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    session_id: Optional[str] = None
    language: Optional[str] = None
    action: Optional[str] = None  # 'generate_document'
    context: Optional[Dict[str, Any]] = None


class ConversationAction(BaseModel):
    type: str  # 'plan', 'document', 'suggestion'
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    language: Optional[str] = None
    user_id: Optional[str] = None


class IntentAnalysisResult(BaseModel):
    intent: str  # 'requirement' or 'conversation'
    confidence: float
    reasoning: str

async def process_intent_actions(intent: str, message: str, current_plan: str, actions: List[Dict], language: str) -> List[ConversationAction]:
    """
    æ ¹æ®æ„å›¾å¤„ç†ç‰¹æ®Šæ“ä½œï¼Œå¦‚ç”Ÿæˆè§„åˆ’æˆ–æ–‡æ¡£
    """
    processed_actions = []

    try:
        if intent == "requirement":
            # ç”ŸæˆçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="é¡¹ç›®è§„åˆ’" if language == "zh" else "Project Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "version": 1
                    }
                ))

        elif intent == "optimization" and current_plan:
            # ä¼˜åŒ–ç°æœ‰è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="ä¼˜åŒ–è§„åˆ’" if language == "zh" else "Optimized Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan,
                        "version": 2
                    }
                ))

        elif intent == "document_generation" and current_plan:
            # ç”Ÿæˆé•¿æ–‡æ¡£
            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            doc_result = await long_planning(doc_request)

            if "flow" in doc_result:
                processed_actions.append(ConversationAction(
                    type="document",
                    content=doc_result["flow"],
                    title="è®¾è®¡æ–‡æ¡£" if language == "zh" else "Design Document",
                    metadata={
                        "language": doc_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan
                    }
                ))

        # å¤„ç†LLMå»ºè®®çš„å…¶ä»–actions
        for action in actions:
            if isinstance(action, dict) and "type" in action and "content" in action:
                processed_actions.append(ConversationAction(
                    type=action.get("type", "suggestion"),
                    content=action.get("content", ""),
                    title=action.get("title"),
                    metadata=action.get("metadata", {})
                ))

    except Exception as e:
        print(f"Error processing intent actions: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå»ºè®®action
        processed_actions.append(ConversationAction(
            type="suggestion",
            content="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚" if language == "zh" else "There was an issue processing your request. Please try rephrasing your needs.",
            title="å»ºè®®" if language == "zh" else "Suggestion"
        ))

    return processed_actions


async def handle_action(action_content: str, original_message: str, current_plan: str, language: str, conversation_history: Optional[List[ChatMessage]] = None):
    """
    å¤„ç†ACTIONæ ‡ç­¾ï¼Œè°ƒç”¨ç›¸åº”çš„æµå¼planningæ¥å£å¹¶è¿”å›æµå¼ç»“æœ
    """
    try:
        # è§£æactionç±»å‹å’Œå†…å®¹
        if action_content.startswith("short_plan:"):
            requirement = action_content[11:].strip() or original_message

            # è°ƒç”¨æµå¼çŸ­è§„åˆ’æ¥å£
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk



        elif action_content.startswith("long_doc:"):
            # LONG_DOC_ACTIONä¸ä½¿ç”¨æ ‡ç­¾å†…å®¹ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡æ¡£ç”Ÿæˆéœ€æ±‚
            doc_requirement = "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†çš„è®¾è®¡æ–‡æ¡£" if language == "zh" else "Generate detailed design document based on current plan"

            # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨context.current_plan
            doc_request = LongPlanningRequest(
                requirement=doc_requirement,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in long_planning_stream(doc_request):
                if isinstance(chunk, str):
                    # long_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("full_flow:"):
            requirement = action_content[10:].strip() or original_message

            # å…ˆæ‰§è¡ŒçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )

            # æ”¶é›†çŸ­è§„åˆ’çš„ç»“æœ
            plan_result = ""
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    yield chunk.encode('utf-8')
                    # æå–è§„åˆ’å†…å®¹ï¼ˆå»é™¤SSEæ ¼å¼ï¼‰
                    if chunk.startswith("data: ") and not chunk.startswith("data: ["):
                        plan_content = chunk[6:].replace('<|newline|>', '\n').strip()
                        if plan_content:
                            plan_result += plan_content + "\n"
                else:
                    yield chunk

            # ç­‰å¾…çŸ­è§„åˆ’å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘é•¿æ–‡æ¡£ç”Ÿæˆ
            if plan_result.strip():
                # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨åˆšç”Ÿæˆçš„è§„åˆ’ä½œä¸ºprevious_flow
                doc_request = LongPlanningRequest(
                    requirement=requirement,
                    previous_flow=plan_result.strip(),
                    language=language
                )
                async for chunk in long_planning_stream(doc_request):
                    if isinstance(chunk, str):
                        yield chunk.encode('utf-8')
                    else:
                        yield chunk

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in handle_action: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while processing the action. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_stream_response(
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    ç”ŸæˆåŸºäºæ ‡ç­¾çš„æµå¼å“åº”
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        # ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¯¹è¯å†å²
        # TODO: åç»­ç ”ç©¶æ›´æ™ºèƒ½çš„å‹ç¼©æ–¹æ¡ˆæ—¶å¯ä»¥é‡æ–°å¯ç”¨ä»¥ä¸‹ä»£ç 
        # try:
        #     # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        #     history_dicts = []
        #     for msg in conversation_history:
        #         history_dicts.append({
        #             "role": msg.role,
        #             "content": msg.content,
        #             "message_type": msg.message_type,
        #             "timestamp": msg.timestamp
        #         })
        #
        #     # ä¼˜åŒ–ä¸Šä¸‹æ–‡
        #     context_str, context_stats = await optimize_conversation_context(
        #         history_dicts, message
        #     )
        # except Exception as e:
        #     print(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")

        # ç›´æ¥å¤„ç†åŸå§‹å¯¹è¯å†å²ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·èŠå¤©è®°å½•
        user_conversation_history = []
        for msg in conversation_history:
            # åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ¶ˆæ¯å’Œè§„åˆ’æ¶ˆæ¯ï¼Œæ’é™¤ç³»ç»Ÿæ¶ˆæ¯ã€æ–‡æ¡£å’Œåˆ†ææ¶ˆæ¯
            # planæ¶ˆæ¯ç›¸å¯¹ç®€æ´ä¸”æ˜¯å¯¹è¯çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œåº”è¯¥ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­
            # documentå’Œanalysisæ¶ˆæ¯é€šå¸¸å¾ˆé•¿ï¼Œä¼šå ç”¨è¿‡å¤štokenï¼Œå› æ­¤æ’é™¤
            if msg.role in ["user", "assistant"] and msg.message_type in ["message", "plan"]:
                msg_content = f"{msg.role}: {msg.content}"
                user_conversation_history.append(msg_content)

        # æ„å»ºçº¯å‡€çš„ç”¨æˆ·å¯¹è¯å†å²å­—ç¬¦ä¸²
        context_str = "\n".join(user_conversation_history) if user_conversation_history else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"

        # æ„å»ºåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡
        if language == "zh":
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """ä½ æ˜¯GTPlannerçš„AIåŠ©æ‰‹ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æ„å›¾ï¼Œå¹¶æ ¹æ®é¢„è®¾æ ¼å¼è¾“å‡ºæŒ‡ä»¤ã€‚

#### **1. æ ¸å¿ƒä»»åŠ¡**
åˆ†æç”¨æˆ·åœ¨å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„æ„å›¾ï¼Œä»ä»¥ä¸‹å››ç§ç±»å‹ä¸­é€‰æ‹©ä¸€ç§å¹¶æŒ‰è¦æ±‚å›åº”ï¼š
*   **é¡¹ç›®è§„åˆ’ (Project Planning)**: ç”¨æˆ·æå‡ºæ–°é¡¹ç›®ã€æ–°åŠŸèƒ½æˆ–ä¼˜åŒ–å»ºè®®ã€‚
*   **æ–‡æ¡£ç”Ÿæˆ (Document Generation)**: ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè®¾è®¡æˆ–æŠ€æœ¯æ–‡æ¡£ã€‚
*   **å®Œæ•´æµç¨‹ (Full Flow)**: ç”¨æˆ·æå‡ºå¤æ‚é¡¹ç›®éœ€æ±‚ï¼Œé€‚åˆä¸€æ¬¡æ€§ç”Ÿæˆè§„åˆ’å’Œæ–‡æ¡£ã€‚
*   **æ™®é€šå¯¹è¯ (Conversation)**: æ—¥å¸¸é—®å€™ã€æ„Ÿè°¢ã€æˆ–ä¸€èˆ¬æ€§æé—®ã€‚

#### **2. è¾“å‡ºæ ¼å¼ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)**
*   **æ™®é€šå¯¹è¯**: `[TEXT_START]ä½ çš„å›å¤å†…å®¹[TEXT_END]`
*   **é¡¹ç›®è§„åˆ’**: `[SHORT_PLAN_ACTION_START]å®Œæ•´çš„æœ€ç»ˆéœ€æ±‚åˆ—è¡¨[SHORT_PLAN_ACTION_END]`
*   **æ–‡æ¡£ç”Ÿæˆ**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (æ ‡ç­¾å†…æ— å†…å®¹)
*   **å®Œæ•´æµç¨‹**: `[FULL_FLOW_ACTION_START]å®Œæ•´çš„æœ€ç»ˆéœ€æ±‚åˆ—è¡¨[FULL_FLOW_ACTION_END]`

**æ ¼å¼è§„åˆ™**:
1.  æ‰€æœ‰æ ‡ç­¾éƒ½ä½¿ç”¨ `[TAG_START]` å’Œ `[TAG_END]` çš„é…å¯¹æ ¼å¼ã€‚
2.  æ ‡ç­¾å‰åä¸è¦æœ‰ä»»ä½•å¤šä½™å­—ç¬¦æˆ–ç©ºæ ¼ã€‚
3.  ç¡®ä¿å¼€å§‹å’Œç»“æŸæ ‡ç­¾ä¸¥æ ¼é…å¯¹ã€‚

#### **3. å·¥ä½œæµç¨‹ä¸å†³ç­–**
1.  **è¯†åˆ«æ„å›¾**ï¼Œå¹¶æ ¹æ®æ„å›¾é€‰æ‹©`ACTION`ã€‚
2.  **å¤„ç†é¡¹ç›®è§„åˆ’**: å¦‚æœæ„å›¾æ˜¯â€œé¡¹ç›®è§„åˆ’â€ï¼Œåˆ™**å¿…é¡»**éµå¾ªç¬¬4èŠ‚çš„è§„åˆ™ã€‚
3.  **ç¦æ­¢å¯¹è¯**: å½“æ„å›¾ä¸ºâ€œé¡¹ç›®è§„åˆ’â€æˆ–â€œæ–‡æ¡£ç”Ÿæˆâ€æ—¶ï¼Œ**ç¦æ­¢**ä½¿ç”¨ `[TEXT_START]` è¿›è¡Œå›å¤ã€‚

#### **4. éœ€æ±‚å¤„ç†è§„åˆ™ (æ ¸å¿ƒä¸­çš„æ ¸å¿ƒ)**

**ä½ çš„å”¯ä¸€å·¥ä½œæ¨¡å¼æ˜¯ç»´æŠ¤ä¸€ä¸ªé¡¹ç›®çš„â€œæœ€ç»ˆçŠ¶æ€â€ã€‚æ¯ä¸€æ¬¡è¾“å‡ºï¼Œéƒ½å¿…é¡»æ˜¯åŸºäºå†å²æ‰€æœ‰å¯¹è¯çš„ã€ä¸€ä¸ªå…¨æ–°çš„ã€å®Œæ•´çš„ã€æœ€ç»ˆçš„è®¡åˆ’ã€‚**

1.  **å¼ºåˆ¶æ€§åˆå¹¶å·¥ä½œæµ**:
    *   **ç¬¬ä¸€æ­¥ï¼šå›é¡¾å†å²** - æŸ¥çœ‹**ä¸Šä¸€è½®**çš„å®Œæ•´è§„åˆ’æ˜¯ä»€ä¹ˆã€‚
    *   **ç¬¬äºŒæ­¥ï¼šç†è§£æ–°å¢** - åˆ†æç”¨æˆ·**å½“å‰**çš„æ¶ˆæ¯æå‡ºäº†ä»€ä¹ˆæ–°çš„éœ€æ±‚ç‚¹æˆ–ä¿®æ”¹ã€‚
    *   **ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå…¨æ–°** - å°†æ–°çš„éœ€æ±‚ç‚¹**æ— ç¼æ•´åˆ**è¿›æ—§çš„è§„åˆ’ä¸­ï¼Œç”Ÿæˆä¸€ä¸ª**å…¨æ–°çš„ã€è¦†ç›–æ‰€æœ‰éœ€æ±‚**çš„å®Œæ•´åˆ—è¡¨ã€‚ä½ çš„è¾“å‡ºå¿…é¡»åƒæ˜¯ç¬¬ä¸€æ¬¡è§åˆ°è¿™ä¸ªé¡¹ç›®ï¼Œç„¶åä¸€æ¬¡æ€§æŠŠæ‰€æœ‰äº‹æƒ…éƒ½åˆ—å‡ºæ¥ã€‚

2.  **è¾“å‡ºå†…å®¹å‡†åˆ™**:
    *   **ç»å¯¹å®Œæ•´æ€§**: `ACTION` æ ‡ç­¾å†…çš„å†…å®¹ï¼Œ**æ°¸è¿œæ˜¯åˆå¹¶äº†æ‰€æœ‰å†å²éœ€æ±‚åçš„æœ€ç»ˆå®Œæ•´æ¸…å•**ã€‚å®ƒä¸æ˜¯å¢é‡æ›´æ–°ï¼Œä¸æ˜¯è¡¥ä¸ï¼Œè€Œæ˜¯å®Œæ•´çš„æœ€ç»ˆç‰ˆæœ¬ã€‚
    *   **ã€ä¸¥ç¦è§£é‡Šã€‘**: ç»å¯¹ç¦æ­¢åŒ…å«ä»»ä½•æ‰¿ä¸Šå¯ä¸‹ã€è§£é‡Šæ€§æˆ–å¯¹æ¯”æ€§çš„æ–‡å­—ã€‚ä¸¥ç¦å‡ºç°â€œåŸºäºç°æœ‰è§„åˆ’...â€ã€â€œåœ¨...åŸºç¡€ä¸Šæ–°å¢...â€æˆ–â€œæœ€ç»ˆå½¢æˆä¸€ä¸ª...â€ç­‰æè¿°æ€§è¯­å¥ã€‚
    *   **ã€æ ¼å¼çº¯ç²¹ã€‘**: ç›´æ¥ã€æ¸…æ™°åœ°å°†æ‰€æœ‰éœ€æ±‚è¦ç‚¹ï¼Œä¸€æ¡ä¸€æ¡åˆ—å‡ºæ¥ã€‚

#### **5. ç¤ºä¾‹ (å¤šè½®å¯¹è¯)**

*   **ç”¨æˆ·ç¬¬ä¸€è½®**: "æˆ‘è¦åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€è¦æœ‰å•†å“æµè§ˆå’Œè´­ç‰©è½¦ã€‚"
    *   **æ¨¡å‹è¾“å‡º**: `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š 1. å•†å“æµè§ˆä¸æœç´¢ 2. è´­ç‰©è½¦ç®¡ç†[SHORT_PLAN_ACTION_END]`

*   **ç”¨æˆ·ç¬¬äºŒè½®**: "å¾ˆå¥½ï¼Œç°åœ¨ç»™æˆ‘åŠ ä¸Šç”¨æˆ·ç™»å½•å’Œä¼˜æƒ åˆ¸åŠŸèƒ½ã€‚"
    *   **æ¨¡å‹è¾“å‡º (æ­£ç¡®)**: `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š 1. å•†å“æµè§ˆä¸æœç´¢ 2. è´­ç‰©è½¦ç®¡ç† 3. ç”¨æˆ·æ³¨å†Œä¸ç™»å½• 4. ä¼˜æƒ åˆ¸ç³»ç»Ÿ[SHORT_PLAN_ACTION_END]`

*   **ç”¨æˆ·ç¬¬ä¸‰è½®**: "å†åŠ ä¸€ä¸ªè®¢å•ç®¡ç†ã€‚"
    *   **æ¨¡å‹è¾“å‡º (æ­£ç¡®)**: `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š 1. å•†å“æµè§ˆä¸æœç´¢ 2. è´­ç‰©è½¦ç®¡ç† 3. ç”¨æˆ·æ³¨å†Œä¸ç™»å½• 4. ä¼˜æƒ åˆ¸ç³»ç»Ÿ 5. è®¢å•ç®¡ç†[SHORT_PLAN_ACTION_END]`
    *   **æ¨¡å‹è¾“å‡º (ç¾éš¾æ€§é”™è¯¯)**: `[SHORT_PLAN_ACTION_START]ä¸ºç³»ç»Ÿå¢åŠ è®¢å•ç®¡ç†åŠŸèƒ½ï¼š1. æŸ¥çœ‹è®¢å•åˆ—è¡¨ 2. è®¢å•è¯¦æƒ…[SHORT_PLAN_ACTION_END]`  **(è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºå®ƒå®Œå…¨å¿˜è®°äº†ä¹‹å‰çš„è´­ç‰©ç³»ç»Ÿã€ç™»å½•ç­‰æ‰€æœ‰éœ€æ±‚ï¼Œè¿™æ˜¯ç»å¯¹è¦é¿å…çš„)**"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
ç”¨æˆ·å¯¹è¯å†å²ï¼š
{context_str}

å½“å‰è§„åˆ’å†…å®¹ï¼š
{current_plan if current_plan else "æš‚æ— è§„åˆ’å†…å®¹"}

ç”¨æˆ·å½“å‰æ¶ˆæ¯ï¼š{message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""


        else:
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """You are the AI assistant for GTPlanner. Your core mission is to analyze user intent and output instructions according to a predefined format.

#### **1. Core Mission**
Analyze the user's intent within the conversational context and choose one of the following four types to respond with:
*   **Project Planning**: The user proposes a new project, new features, or optimization suggestions.
*   **Document Generation**: The user explicitly requests the generation of a design or technical document.
*   **Full Flow**: The user presents a complex project requirement, suitable for generating the plan and documentation in one go.
*   **Conversation**: General greetings, thanks, or standard questions.

#### **2. Output Format (Must be strictly followed)**
*   **Conversation**: `[TEXT_START]Your reply content here[TEXT_END]`
*   **Project Planning**: `[SHORT_PLAN_ACTION_START]The complete and final list of requirements[SHORT_PLAN_ACTION_END]`
*   **Document Generation**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (No content between tags)
*   **Full Flow**: `[FULL_FLOW_ACTION_START]The complete and final list of requirements[FULL_FLOW_ACTION_END]`

**Formatting Rules**:
1.  All tags must use the paired `[TAG_START]` and `[TAG_END]` format.
2.  There must be no extra characters or spaces before or after a tag.
3.  Ensure that start and end tags are strictly paired.

#### **3. Workflow and Decision-Making**
1.  **Identify the intent** and select an `ACTION` accordingly.
2.  **Handle Project Planning**: If the intent is "Project Planning," you **must** follow the rules in Section 4.
3.  **Prohibit Conversation**: When the intent is "Project Planning" or "Document Generation," you are **forbidden** from using `[TEXT_START]` to reply.

#### **4. Requirement Processing Rules (The Core Mandate)**

**Your sole operational model is to maintain the "final state" of a project. Every output you generate must be a brand-new, complete, and final plan based on the entire conversation history.**

1.  **Mandatory Merge Workflow**:
    *   **Step 1: Review History** - Look at the complete plan from the **previous turn**.
    *   **Step 2: Understand Additions** - Analyze what new requirements or modifications the user's **current message** introduces.
    *   **Step 3: Generate Anew** - **Seamlessly integrate** the new points into the old plan to produce a **brand-new, all-encompassing** list of requirements. Your output must look as if you are seeing the project for the first time and listing everything in one go.

2.  **Output Content Guidelines**:
    *   **Absolute Completeness**: The content inside the `ACTION` tag must **always be the final, complete list that merges all historical requirements**. It is not an incremental update, not a patch, but the complete final version.
    *   **ã€STRICTLY FORBIDDEN: Explanationsã€‘**: Absolutely no transitional, explanatory, or comparative text. Do not use phrases like "Based on the existing plan...", "Adding to the previous scope...", or "The final plan is now...".
    *   **ã€PURE FORMATã€‘**: Directly and clearly list all requirement points, one by one.

#### **5. Example (Multi-Turn Dialogue)**

*   **User: Turn 1**: "I want to create an online shopping system. It needs product browsing and a shopping cart."
    *   **Model Output**: `[SHORT_PLAN_ACTION_START]Create an online shopping system with the following requirements: 1. Product browsing and search 2. Shopping cart management[SHORT_PLAN_ACTION_END]`

*   **User: Turn 2**: "Great, now add user login and a coupon feature."
    *   **Model Output (Correct)**: `[SHORT_PLAN_ACTION_START]Create an online shopping system with the following requirements: 1. Product browsing and search 2. Shopping cart management 3. User registration and login 4. Coupon system[SHORT_PLAN_ACTION_END]`

*   **User: Turn 3**: "Also add order management."
    *   **Model Output (Correct)**: `[SHORT_PLAN_ACTION_START]Create an online shopping system with the following requirements: 1. Product browsing and search 2. Shopping cart management 3. User registration and login 4. Coupon system 5. Order management[SHORT_PLAN_ACTION_END]`
    *   **Model Output (Catastrophic Error)**: `[SHORT_PLAN_ACTION_START]Add order management to the system: 1. View order list 2. Order details[SHORT_PLAN_ACTION_END]`  **(This is a catastrophic error because it completely forgot all previous requirements like the shopping system, login, etc. This must be avoided at all costs.)**"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
User conversation history:
{context_str}

Current plan content:
{current_plan if current_plan else "No current plan available"}

Current user message: {message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«å’Œå“åº”ç”Ÿæˆ
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        yield stream_data_block("[STATUS_END]")

        

        # å¯¼å…¥æµå¼LLMè°ƒç”¨
        from utils.call_llm import call_llm_stream_async

        # æµå¼è¾“å‡ºLLMçš„å“åº”å¹¶ç›‘å¬ACTIONæ ‡ç­¾
        action_buffer = []
        in_action = False
        action_type = None
        content_buffer = ""
        pending_output = ""  # ç¼“å†²å¾…è¾“å‡ºçš„å†…å®¹

        async for chunk in call_llm_stream_async(prompt):
            if chunk:
                content_buffer += chunk
                pending_output += chunk

                # å¤„ç†ç¼“å†²åŒºä¸­çš„å®Œæ•´æ ‡ç­¾
                while True:
                    # æŸ¥æ‰¾æ ‡ç­¾å¼€å§‹
                    start_pos = pending_output.find('[')
                    if start_pos == -1:
                        # æ²¡æœ‰æ ‡ç­¾ï¼Œè¾“å‡ºæ‰€æœ‰å†…å®¹
                        if pending_output and not in_action:
                            yield stream_data(pending_output)
                        elif pending_output and in_action:
                            action_buffer.append(pending_output)
                        pending_output = ""
                        break

                    # è¾“å‡ºæ ‡ç­¾å‰çš„å†…å®¹
                    if start_pos > 0:
                        before_tag = pending_output[:start_pos]
                        if not in_action:
                            yield stream_data(before_tag)
                        else:
                            action_buffer.append(before_tag)

                    # æŸ¥æ‰¾æ ‡ç­¾ç»“æŸ
                    end_pos = pending_output.find(']', start_pos)
                    if end_pos == -1:
                        # æ ‡ç­¾ä¸å®Œæ•´ï¼Œä¿ç•™ä»æ ‡ç­¾å¼€å§‹çš„æ‰€æœ‰å†…å®¹
                        pending_output = pending_output[start_pos:]
                        break

                    # æå–å®Œæ•´æ ‡ç­¾
                    complete_tag = pending_output[start_pos:end_pos + 1]
                    remaining_content = pending_output[end_pos + 1:]

                    # å¤„ç†æ ‡ç­¾
                    if complete_tag == "[SHORT_PLAN_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "short_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[LONG_DOC_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "long_doc"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[FULL_FLOW_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "full_flow"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯

                    elif complete_tag in ["[SHORT_PLAN_ACTION_END]", "[LONG_DOC_ACTION_END]", "[FULL_FLOW_ACTION_END]"] and in_action:
                        # å¤„ç†ACTIONå†…å®¹
                        action_content = ''.join(action_buffer).strip()
                        if action_content:
                            # æ ¹æ®action_typeæ„é€ action_content
                            full_action_content = f"{action_type}:{action_content}"
                            async for planning_chunk in handle_action(full_action_content, message, current_plan, language, conversation_history):
                                yield planning_chunk
                        in_action = False
                        action_buffer = []
                        action_type = None
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    else:
                        # éACTIONæ ‡ç­¾ï¼Œæ­£å¸¸å‘é€
                        if not in_action:
                            yield stream_data(complete_tag)
                        else:
                            action_buffer.append(complete_tag)

                    # ç»§ç»­å¤„ç†å‰©ä½™å†…å®¹
                    pending_output = remaining_content

        # å¤„ç†å¯èƒ½å‰©ä½™çš„å†…å®¹
        if pending_output:
            if not in_action:
                yield stream_data(pending_output)
            else:
                action_buffer.append(pending_output)

        yield stream_data_block("")  # ç©ºè¡Œè¡¨ç¤ºç»“æŸ

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_stream_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while generating response. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_direct_action_response(
    action: str,
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    æ ¹æ®æ˜ç¡®çš„actionç›´æ¥ç”Ÿæˆå“åº”ï¼Œè·³è¿‡AIæ„å›¾è¯†åˆ«
    """
   

    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯ - æ ¹æ®è¯­è¨€æœ¬åœ°åŒ–
        yield stream_data("[STATUS_START]")
        if language == "zh":
            yield stream_data("ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        else:
            yield stream_data("ğŸ”„ Processing your request...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        if action == "generate_document":
            # ç›´æ¥è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆ
            if not current_plan:
                yield stream_data("[ERROR_START]")
                if language == "zh":
                    yield stream_data("âŒ æœªæ‰¾åˆ°å½“å‰è§„åˆ’å†…å®¹ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£ã€‚")
                else:
                    yield stream_data("âŒ No current plan found. Cannot generate document.")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            if language == "zh":
                yield stream_data("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯¦ç»†è®¾è®¡æ–‡æ¡£...")
            else:
                yield stream_data("ğŸ“„ Generating detailed design document...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆæµå¼æ¥å£
            from api.v1.planning import long_planning_stream, LongPlanningRequest

            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in long_planning_stream(doc_request):
                yield chunk



        else:
            yield stream_data("[ERROR_START]")
            if language == "zh":
                yield stream_data(f"âŒ ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}")
            else:
                yield stream_data(f"âŒ Unsupported action type: {action}")
            yield stream_data_block("[ERROR_END]")

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_direct_action_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        if language == "zh":
            yield stream_data("âŒ å¤„ç†æ“ä½œæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
        else:
            yield stream_data("âŒ An internal error occurred while processing the operation. Please try again later.")
        yield stream_data_block("[ERROR_END]")


@chat_router.post("/unified")
async def unified_conversation(body: ConversationRequest):
    """
    ç»Ÿä¸€å¯¹è¯æ¥å£ï¼šé›†æˆæ„å›¾è¯†åˆ«ã€å¯¹è¯å›å¤ã€è§„åˆ’ç”Ÿæˆå’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½
    å®Œå…¨ä½¿ç”¨æµå¼å“åº”
    """
    message = body.message
    conversation_history = body.conversation_history or []
    session_id = body.session_id
    language = body.language
    action = body.action
    context = body.context or {}

    
    if not message:
        async def error_stream():
            yield stream_data("[ERROR_START]")
            yield stream_data("âŒ Missing message in request body.")
            yield stream_data_block("[ERROR_END]")
        return StreamingResponse(error_stream(), media_type="text/plain")

    # ç¡®å®šè¯­è¨€ - ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ é€’çš„ç•Œé¢è¯­è¨€ï¼Œç¡®ä¿ç•Œé¢è¯­è¨€å’ŒLLMè¾“å‡ºè¯­è¨€çš„ä¸€è‡´æ€§
    language = determine_language(message, None, language)

    # æ ¹æ®actionå­—æ®µç›´æ¥è°ƒç”¨ç›¸åº”åŠŸèƒ½ï¼Œæˆ–ä½¿ç”¨AIæ„å›¾è¯†åˆ«
    if action:
        return StreamingResponse(
            generate_direct_action_response(action, message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )
    else:
        # è¿”å›æµå¼å“åº”ï¼ˆä½¿ç”¨AIæ„å›¾è¯†åˆ«ï¼‰
        return StreamingResponse(
            generate_stream_response(message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )


