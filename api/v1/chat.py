from typing import Any, Optional, List, Dict

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from utils.multilingual_utils import determine_language
# from utils.context_manager import optimize_conversation_context  # å·²ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½

# å¯¼å…¥è§„åˆ’ç›¸å…³çš„åŠŸèƒ½
from api.v1.planning import (
    short_planning, long_planning,
    short_planning_stream, long_planning_stream,
    ShortPlanningRequest, LongPlanningRequest
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])


def stream_data(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆSSEæ ¼å¼è¦æ±‚ï¼‰
    ä½¿ç”¨å ä½ç¬¦ä¿æŠ¤markdownæ¢è¡Œç¬¦ï¼Œé¿å…ä¸SSEåè®®å†²çª
    """
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…ä¸SSEæ¶ˆæ¯åˆ†éš”ç¬¦å†²çª
    protected_content = content.replace('\n', '<|newline|>')
    return f"data: {protected_content}\n".encode('utf-8')


def stream_data_block(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®å—ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œå¹¶æ·»åŠ ç»“æŸæ ‡è®°
    """
    return f"data: {content}\n\n".encode('utf-8')


class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    message_type: Optional[str] = "message"  # 'message', 'plan', 'document'
    timestamp: Optional[int] = None


class ConversationRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    session_id: Optional[str] = None
    language: Optional[str] = None
    action: Optional[str] = None  # 'generate_document'
    context: Optional[Dict[str, Any]] = None


class ConversationAction(BaseModel):
    type: str  # 'plan', 'document', 'suggestion'
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    language: Optional[str] = None
    user_id: Optional[str] = None


class IntentAnalysisResult(BaseModel):
    intent: str  # 'requirement' or 'conversation'
    confidence: float
    reasoning: str

async def process_intent_actions(intent: str, message: str, current_plan: str, actions: List[Dict], language: str) -> List[ConversationAction]:
    """
    æ ¹æ®æ„å›¾å¤„ç†ç‰¹æ®Šæ“ä½œï¼Œå¦‚ç”Ÿæˆè§„åˆ’æˆ–æ–‡æ¡£
    """
    processed_actions = []

    try:
        if intent == "requirement":
            # ç”ŸæˆçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="é¡¹ç›®è§„åˆ’" if language == "zh" else "Project Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "version": 1
                    }
                ))

        elif intent == "optimization" and current_plan:
            # ä¼˜åŒ–ç°æœ‰è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="ä¼˜åŒ–è§„åˆ’" if language == "zh" else "Optimized Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan,
                        "version": 2
                    }
                ))

        elif intent == "document_generation" and current_plan:
            # ç”Ÿæˆé•¿æ–‡æ¡£
            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            doc_result = await long_planning(doc_request)

            if "flow" in doc_result:
                processed_actions.append(ConversationAction(
                    type="document",
                    content=doc_result["flow"],
                    title="è®¾è®¡æ–‡æ¡£" if language == "zh" else "Design Document",
                    metadata={
                        "language": doc_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan
                    }
                ))

        # å¤„ç†LLMå»ºè®®çš„å…¶ä»–actions
        for action in actions:
            if isinstance(action, dict) and "type" in action and "content" in action:
                processed_actions.append(ConversationAction(
                    type=action.get("type", "suggestion"),
                    content=action.get("content", ""),
                    title=action.get("title"),
                    metadata=action.get("metadata", {})
                ))

    except Exception as e:
        print(f"Error processing intent actions: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå»ºè®®action
        processed_actions.append(ConversationAction(
            type="suggestion",
            content="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚" if language == "zh" else "There was an issue processing your request. Please try rephrasing your needs.",
            title="å»ºè®®" if language == "zh" else "Suggestion"
        ))

    return processed_actions


async def handle_action(action_content: str, original_message: str, current_plan: str, language: str, conversation_history: Optional[List[ChatMessage]] = None):
    """
    å¤„ç†ACTIONæ ‡ç­¾ï¼Œè°ƒç”¨ç›¸åº”çš„æµå¼planningæ¥å£å¹¶è¿”å›æµå¼ç»“æœ
    """
    try:
        # è§£æactionç±»å‹å’Œå†…å®¹
        if action_content.startswith("short_plan:"):
            requirement = action_content[11:].strip() or original_message

            # è°ƒç”¨æµå¼çŸ­è§„åˆ’æ¥å£
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk



        elif action_content.startswith("long_doc:"):
            # LONG_DOC_ACTIONä¸ä½¿ç”¨æ ‡ç­¾å†…å®¹ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡æ¡£ç”Ÿæˆéœ€æ±‚
            doc_requirement = "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†çš„è®¾è®¡æ–‡æ¡£" if language == "zh" else "Generate detailed design document based on current plan"

            # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨context.current_plan
            doc_request = LongPlanningRequest(
                requirement=doc_requirement,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in long_planning_stream(doc_request):
                if isinstance(chunk, str):
                    # long_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("full_flow:"):
            requirement = action_content[10:].strip() or original_message

            # å…ˆæ‰§è¡ŒçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )

            # æ”¶é›†çŸ­è§„åˆ’çš„ç»“æœ
            plan_result = ""
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    yield chunk.encode('utf-8')
                    # æå–è§„åˆ’å†…å®¹ï¼ˆå»é™¤SSEæ ¼å¼ï¼‰
                    if chunk.startswith("data: ") and not chunk.startswith("data: ["):
                        plan_content = chunk[6:].replace('<|newline|>', '\n').strip()
                        if plan_content:
                            plan_result += plan_content + "\n"
                else:
                    yield chunk

            # ç­‰å¾…çŸ­è§„åˆ’å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘é•¿æ–‡æ¡£ç”Ÿæˆ
            if plan_result.strip():
                # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨åˆšç”Ÿæˆçš„è§„åˆ’ä½œä¸ºprevious_flow
                doc_request = LongPlanningRequest(
                    requirement=requirement,
                    previous_flow=plan_result.strip(),
                    language=language
                )
                async for chunk in long_planning_stream(doc_request):
                    if isinstance(chunk, str):
                        yield chunk.encode('utf-8')
                    else:
                        yield chunk

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in handle_action: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while processing the action. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_stream_response(
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    ç”ŸæˆåŸºäºæ ‡ç­¾çš„æµå¼å“åº”
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        # ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¯¹è¯å†å²
        # TODO: åç»­ç ”ç©¶æ›´æ™ºèƒ½çš„å‹ç¼©æ–¹æ¡ˆæ—¶å¯ä»¥é‡æ–°å¯ç”¨ä»¥ä¸‹ä»£ç 
        # try:
        #     # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        #     history_dicts = []
        #     for msg in conversation_history:
        #         history_dicts.append({
        #             "role": msg.role,
        #             "content": msg.content,
        #             "message_type": msg.message_type,
        #             "timestamp": msg.timestamp
        #         })
        #
        #     # ä¼˜åŒ–ä¸Šä¸‹æ–‡
        #     context_str, context_stats = await optimize_conversation_context(
        #         history_dicts, message
        #     )
        # except Exception as e:
        #     print(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")

        # ç›´æ¥å¤„ç†åŸå§‹å¯¹è¯å†å²ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·èŠå¤©è®°å½•
        user_conversation_history = []
        for msg in conversation_history:
            # åªåŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ¶ˆæ¯ï¼Œæ’é™¤ç³»ç»Ÿæ¶ˆæ¯
            if msg.role in ["user", "assistant"] and msg.message_type == "message":
                msg_content = f"{msg.role}: {msg.content}"
                user_conversation_history.append(msg_content)

        # æ„å»ºçº¯å‡€çš„ç”¨æˆ·å¯¹è¯å†å²å­—ç¬¦ä¸²
        context_str = "\n".join(user_conversation_history) if user_conversation_history else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"

        # æ„å»ºåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡
        if language == "zh":
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """ä½ æ˜¯GTPlannerçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œé¡¹ç›®è§„åˆ’å’Œè®¾è®¡ã€‚

è¯·åˆ†æç”¨æˆ·çš„æ„å›¾å¹¶ç›¸åº”å›åº”ã€‚åœ¨åˆ†ææ—¶ï¼Œè¯·ç‰¹åˆ«æ³¨æ„å¯¹è¯ä¸Šä¸‹æ–‡å’Œå·²æœ‰è§„åˆ’å†…å®¹ã€‚å¯èƒ½çš„æ„å›¾åŒ…æ‹¬ï¼š

1. **é¡¹ç›®éœ€æ±‚(requirement)**ï¼š
   - ç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ›å»ºå…¨æ–°çš„ç³»ç»Ÿ/åº”ç”¨/é¡¹ç›®
   - ç”¨æˆ·åœ¨ç°æœ‰è§„åˆ’åŸºç¡€ä¸Šæå‡ºæ–°çš„åŠŸèƒ½éœ€æ±‚æˆ–æ”¹è¿›å»ºè®®
   - âš ï¸ **é‡è¦åŸåˆ™**ï¼šå¦‚æœå·²æœ‰è§„åˆ’å†…å®¹ï¼Œç”¨æˆ·çš„æ–°éœ€æ±‚æ˜¯å¯¹ç°æœ‰é¡¹ç›®çš„åŠŸèƒ½æ‰©å±•ï¼Œå¿…é¡»ä¿æŒåŸé¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½å’Œä¸»ä½“æ¶æ„ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šé›†æˆæ–°åŠŸèƒ½

2. **ä¼˜åŒ–æ”¹è¿›(optimization)**ï¼šç”¨æˆ·å¯¹ç°æœ‰è§„åˆ’æå‡ºä¿®æ”¹å»ºè®®æˆ–ä¼˜åŒ–æ„è§ã€‚

3. **æ–‡æ¡£ç”Ÿæˆ(document_generation)**ï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè¯¦ç»†æ–‡æ¡£ã€è®¾è®¡æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£ï¼Œæˆ–è€…è¯´"åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆæ–‡æ¡£"ã€"ç”Ÿæˆè®¾è®¡æ–‡æ¡£"ç­‰ã€‚

4. **æ™®é€šå¯¹è¯(conversation)**ï¼šç”¨æˆ·åªæ˜¯é—®å€™ã€æ„Ÿè°¢ã€è¯¢é—®æ¦‚å¿µã€å¯»æ±‚å»ºè®®æˆ–è¿›è¡Œä¸€èˆ¬æ€§è®¨è®ºã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡ç­¾æ ¼å¼è¾“å‡ºï¼Œæ³¨æ„æ ‡ç­¾æ ¼å¼å¿…é¡»å®Œå…¨æ­£ç¡®ï¼š

- æ™®é€šå¯¹è¯å›å¤ï¼š[TEXT_START]æ‚¨çš„å›å¤å†…å®¹ï¼ˆå¯åŒ…å«å»ºè®®ã€æç¤ºç­‰ï¼‰[TEXT_END]
- è§¦å‘çŸ­è§„åˆ’ï¼š[SHORT_PLAN_ACTION]å®Œæ•´æè¿°ç”¨æˆ·éœ€æ±‚ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€ç›®æ ‡ã€åŠŸèƒ½è¦æ±‚ç­‰è¯¦ç»†ä¿¡æ¯[/SHORT_PLAN_ACTION]
- è§¦å‘é•¿æ–‡æ¡£ï¼š[LONG_DOC_ACTION][/LONG_DOC_ACTION]
- è§¦å‘å®Œæ•´æµç¨‹ï¼š[FULL_FLOW_ACTION]å®Œæ•´æè¿°ç”¨æˆ·éœ€æ±‚ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€ç›®æ ‡ã€åŠŸèƒ½è¦æ±‚ç­‰è¯¦ç»†ä¿¡æ¯[/FULL_FLOW_ACTION]

æ³¨æ„ï¼šå»ºè®®å’Œæç¤ºå†…å®¹åº”è¯¥ç›´æ¥åŒ…å«åœ¨TEXTæ ‡ç­¾å†…ï¼Œä¸è¦ä½¿ç”¨å•ç‹¬çš„SUGGESTIONæ ‡ç­¾ã€‚

âš ï¸ æ ‡ç­¾æ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. TEXTæ ‡ç­¾æ ¼å¼ï¼šå¼€å§‹ç”¨[TEXT_START]ï¼Œç»“æŸç”¨[TEXT_END]ï¼ˆæ³¨æ„ï¼šTEXT_ENDæ²¡æœ‰æ–œæ ï¼ï¼‰
2. ACTIONæ ‡ç­¾æ ¼å¼ï¼šå¼€å§‹ç”¨[ACTION_NAME]ï¼Œç»“æŸç”¨[/ACTION_NAME]ï¼ˆæ³¨æ„ï¼šç»“æŸæ ‡ç­¾æœ‰æ–œæ ï¼ï¼‰
3. é”™è¯¯ç¤ºä¾‹ï¼š[/TEXT_START]ã€[/TEXT_END]ã€[SHORT_PLAN_ACTION/] éƒ½æ˜¯é”™è¯¯çš„
4. æ­£ç¡®ç¤ºä¾‹ï¼š[TEXT_START]å†…å®¹[TEXT_END]ã€[SHORT_PLAN_ACTION]å†…å®¹[/SHORT_PLAN_ACTION]
5. ä¸è¦åœ¨æ ‡ç­¾åå‰åæ·»åŠ ä»»ä½•é¢å¤–å­—ç¬¦
6. æ ‡ç­¾å¿…é¡»ç‹¬ç«‹æˆè¡Œæˆ–ç´§è´´å†…å®¹ï¼Œä¸è¦æœ‰å¤šä½™ç©ºæ ¼

æ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©åˆé€‚çš„æ ‡ç­¾è¾“å‡ºï¼š

**ä½•æ—¶ä½¿ç”¨SHORT_PLAN_ACTIONï¼š**
- ç”¨æˆ·æå‡ºæ–°çš„é¡¹ç›®éœ€æ±‚æˆ–åŠŸèƒ½éœ€æ±‚ï¼ˆå¦‚ï¼šå¼€å‘XXç³»ç»Ÿã€åˆ›å»ºXXåº”ç”¨ã€è®¾è®¡XXåŠŸèƒ½ï¼‰
- ç”¨æˆ·è¦æ±‚åˆ›å»ºã€å¼€å‘ã€è®¾è®¡å…·ä½“çš„ç³»ç»Ÿ/åº”ç”¨/æ’ä»¶/å·¥å…·
- ç”¨æˆ·æè¿°äº†å…·ä½“çš„äº§å“åŠŸèƒ½å’Œç‰¹æ€§
- ç”¨æˆ·åœ¨ç°æœ‰é¡¹ç›®åŸºç¡€ä¸Šæå‡ºæ–°åŠŸèƒ½æˆ–æ”¹è¿›å»ºè®®
- éœ€è¦ç”Ÿæˆæ­¥éª¤åŒ–çš„å®æ–½è§„åˆ’æ—¶

**æ˜ç¡®çš„é¡¹ç›®éœ€æ±‚å…³é”®è¯ï¼š**
- "å¼€å‘"ã€"åˆ›å»º"ã€"è®¾è®¡"ã€"åˆ¶ä½œ"ã€"æ„å»º"
- "ç³»ç»Ÿ"ã€"åº”ç”¨"ã€"æ’ä»¶"ã€"å·¥å…·"ã€"å¹³å°"
- "åŠŸèƒ½"ã€"ç‰¹æ€§"ã€"æ¨¡å—"ã€"ç»„ä»¶"ã€"æ·»åŠ "ã€"é›†æˆ"ã€"æ”¯æŒ"
- æè¿°å…·ä½“çš„æŠ€æœ¯å®ç°æˆ–äº§å“ç‰¹æ€§

**âš ï¸ åŠŸèƒ½æ‰©å±•è§„åˆ’çš„æ ¸å¿ƒåŸåˆ™ï¼š**
å½“ç”¨æˆ·åœ¨ç°æœ‰é¡¹ç›®åŸºç¡€ä¸Šæå‡ºæ–°åŠŸèƒ½æ—¶ï¼ˆå¦‚"æ·»åŠ XXåŠŸèƒ½"ã€"é›†æˆXX"ã€"æ”¯æŒXX"ï¼‰ï¼Œç”Ÿæˆçš„è§„åˆ’å¿…é¡»ï¼š
1. **ä¿æŒåŸé¡¹ç›®çš„å®Œæ•´æ€§**ï¼šåŒ…å«åŸæœ‰çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å’Œæ¶æ„
2. **æ˜ç¡®æ–°åŠŸèƒ½çš„é›†æˆæ–¹å¼**ï¼šè¯´æ˜æ–°åŠŸèƒ½å¦‚ä½•ä¸ç°æœ‰åŠŸèƒ½ååŒå·¥ä½œ
3. **æä¾›å®Œæ•´çš„é¡¹ç›®è§„åˆ’**ï¼šä¸æ˜¯ç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—è§„åˆ’ï¼Œè€Œæ˜¯åŒ…å«åŸæœ‰åŠŸèƒ½+æ–°åŠŸèƒ½çš„å®Œæ•´é¡¹ç›®è§„åˆ’
4. **ä¿æŒé¡¹ç›®ä¸»é¢˜ä¸€è‡´æ€§**ï¼šç¡®ä¿æ–°åŠŸèƒ½æœåŠ¡äºåŸé¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡

**ä½•æ—¶ä½¿ç”¨LONG_DOC_ACTIONï¼š**
- ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè¯¦ç»†æ–‡æ¡£ã€è®¾è®¡æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£
- ç”¨æˆ·è¯´"ç”Ÿæˆæ–‡æ¡£"ã€"åŸºäºè§„åˆ’ç”Ÿæˆæ–‡æ¡£"ã€"è¯¦ç»†è®¾è®¡"ç­‰
- å·²æœ‰è§„åˆ’å†…å®¹ï¼Œç”¨æˆ·éœ€è¦æ›´è¯¦ç»†çš„æ–‡æ¡£è¯´æ˜
- éœ€è¦å°†è§„åˆ’è½¬åŒ–ä¸ºå®Œæ•´è®¾è®¡æ–‡æ¡£æ—¶

**ä½•æ—¶ä½¿ç”¨FULL_FLOW_ACTIONï¼š**
- ç”¨æˆ·æå‡ºå®Œæ•´çš„é¡¹ç›®éœ€æ±‚ï¼Œä¸”ä½ è®¤ä¸ºéœ€è¦åŒæ—¶æä¾›è§„åˆ’å’Œè¯¦ç»†æ–‡æ¡£
- ç”¨æˆ·çš„éœ€æ±‚è¶³å¤Ÿå¤æ‚ï¼Œå€¼å¾—ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®æµç¨‹ï¼ˆè§„åˆ’+æ–‡æ¡£ï¼‰
- å½“ä½ åˆ¤æ–­ç”¨æˆ·æœ€ç»ˆéœ€è¦çš„æ˜¯å®Œæ•´çš„é¡¹ç›®æ–¹æ¡ˆæ—¶
- ä¸éœ€è¦ç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼Œä½ å¯ä»¥ä¸»åŠ¨åˆ¤æ–­æ˜¯å¦é€‚åˆä½¿ç”¨å®Œæ•´æµç¨‹

**çµæ´»ä½¿ç”¨ç­–ç•¥ï¼š**

**ç›´æ¥ä½¿ç”¨è§„åˆ™ï¼š**
- å¦‚æœç”¨æˆ·éœ€æ±‚å·²ç»è¶³å¤Ÿæ˜ç¡®å’Œå®Œæ•´ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨SHORT_PLAN_ACTIONç”Ÿæˆè§„åˆ’
- å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆæ–‡æ¡£ä¸”å·²æœ‰å®Œæ•´çš„é¡¹ç›®èƒŒæ™¯ä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨LONG_DOC_ACTION

**å¿…é¡»å…ˆç”¨SHORT_PLAN_ACTIONçš„æƒ…å†µï¼š**
- å½“ç”¨æˆ·è¦æ±‚ç”Ÿæˆæ–‡æ¡£(LONG_DOC_ACTION)ä½†å½“å‰å¯¹è¯ä¸­ç¼ºå°‘å®Œæ•´çš„é¡¹ç›®è§„åˆ’æ—¶
- å½“éœ€è¦å…ˆå»ºç«‹é¡¹ç›®çš„åŸºç¡€è§„åˆ’æ¡†æ¶ï¼Œå†ç”Ÿæˆè¯¦ç»†æ–‡æ¡£æ—¶
- å½“ç”¨æˆ·çš„éœ€æ±‚æè¿°ä¸å¤Ÿå…·ä½“ï¼Œéœ€è¦å…ˆé€šè¿‡è§„åˆ’è¿‡ç¨‹æ˜ç¡®å…·ä½“å®æ–½æ­¥éª¤æ—¶

**åˆ¤æ–­åŸåˆ™ï¼š**
- ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·éœ€æ±‚çš„å®Œæ•´æ€§å’Œå¯æ‰§è¡Œæ€§
- ç¡®ä¿ç”Ÿæˆçš„å†…å®¹æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ”¯æ’‘
- é¿å…åœ¨ç¼ºå°‘åŸºç¡€è§„åˆ’çš„æƒ…å†µä¸‹ç›´æ¥ç”Ÿæˆè¯¦ç»†æ–‡æ¡£

**åŸºæœ¬è¾“å‡ºè§„åˆ™ï¼š**
- é¡¹ç›®éœ€æ±‚ï¼šç›´æ¥ä½¿ç”¨SHORT_PLAN_ACTIONæˆ–FULL_FLOW_ACTIONï¼Œä¸éœ€è¦å…ˆç”¨TEXTå›å¤
- æ–‡æ¡£ç”Ÿæˆè¯·æ±‚ï¼šç›´æ¥ä½¿ç”¨LONG_DOC_ACTIONï¼Œä¸éœ€è¦å…ˆç”¨TEXTå›å¤
- æ™®é€šå¯¹è¯ï¼šåªç”¨TEXTæ ‡ç­¾å›å¤

**é‡è¦ï¼šå½“è¯†åˆ«åˆ°é¡¹ç›®éœ€æ±‚æ—¶ï¼Œå¿…é¡»ç›´æ¥è§¦å‘ç›¸åº”çš„ACTIONï¼Œä¸è¦åªç”¨TEXTå›å¤ï¼**

**åˆ¤æ–­ç¤ºä¾‹ï¼š**
- "å¤šè¯­è¨€è¯æ±‡é‡æå‡è®¾è®¡æ–‡æ¡£" â†’ é¡¹ç›®éœ€æ±‚ â†’ ä½¿ç”¨SHORT_PLAN_ACTION
- "æµè§ˆå™¨æ’ä»¶ï¼Œä¸ºç”¨æˆ·æä¾›åˆ’è¯å³æ—¶ç¿»è¯‘" â†’ é¡¹ç›®éœ€æ±‚ â†’ ä½¿ç”¨SHORT_PLAN_ACTION
- "å¼€å‘ä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿ" â†’ é¡¹ç›®éœ€æ±‚ â†’ ä½¿ç”¨SHORT_PLAN_ACTION
- "ä½ å¥½ï¼Œè¯·é—®ä½ èƒ½åšä»€ä¹ˆï¼Ÿ" â†’ æ™®é€šå¯¹è¯ â†’ ä½¿ç”¨TEXTæ ‡ç­¾
- "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†æ–‡æ¡£" â†’ æ–‡æ¡£ç”Ÿæˆ â†’ ä½¿ç”¨LONG_DOC_ACTION

âš ï¸ é‡è¦æç¤ºï¼š
1. **SHORT_PLAN_ACTIONæ ‡ç­¾**ï¼šè¾“å‡ºå®Œæ•´çš„ç”¨æˆ·éœ€æ±‚æè¿°ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€ç›®æ ‡ã€åŠŸèƒ½è¦æ±‚ã€æŠ€æœ¯éœ€æ±‚ç­‰
2. **LONG_DOC_ACTIONæ ‡ç­¾**ï¼šæ ‡ç­¾å†…ä¸éœ€è¦åŒ…å«ä»»ä½•å†…å®¹ï¼Œç³»ç»Ÿä¼šä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„è§„åˆ’å†…å®¹
3. **FULL_FLOW_ACTIONæ ‡ç­¾**ï¼šè¾“å‡ºå®Œæ•´çš„ç”¨æˆ·éœ€æ±‚æè¿°ï¼Œä¾›çŸ­è§„åˆ’èŠ‚ç‚¹ä½¿ç”¨ï¼Œç¡®ä¿éœ€æ±‚æè¿°è¶³å¤Ÿè¯¦ç»†å’Œå‡†ç¡®
4. åŸºäºå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾å’Œéœ€æ±‚
5. **å¦‚æœç”¨æˆ·æ˜¯åœ¨ç°æœ‰é¡¹ç›®åŸºç¡€ä¸Šæå‡ºæ–°éœ€æ±‚ï¼Œéœ€æ±‚æè¿°ä¸­å¿…é¡»åŒ…å«ï¼š**
   - åŸé¡¹ç›®çš„å®Œæ•´èƒŒæ™¯å’Œæ ¸å¿ƒåŠŸèƒ½æè¿°
   - æ–°å¢åŠŸèƒ½çš„å…·ä½“è¦æ±‚
   - æ–°åŠŸèƒ½ä¸åŸæœ‰åŠŸèƒ½çš„é›†æˆæ–¹å¼
   - ç¡®ä¿ç”Ÿæˆçš„æ˜¯å®Œæ•´é¡¹ç›®è§„åˆ’ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—è§„åˆ’

**éœ€æ±‚æè¿°æ¨¡æ¿ï¼ˆç”¨äºåŠŸèƒ½æ‰©å±•åœºæ™¯ï¼‰ï¼š**
"åŸºäºç°æœ‰çš„[åŸé¡¹ç›®æè¿°]ï¼Œåœ¨ä¿æŒå…¶[æ ¸å¿ƒåŠŸèƒ½åˆ—è¡¨]çš„åŸºç¡€ä¸Šï¼Œæ–°å¢[æ–°åŠŸèƒ½æè¿°]ã€‚æ–°åŠŸèƒ½åº”ä¸ç°æœ‰åŠŸèƒ½[é›†æˆæ–¹å¼]ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªå®Œæ•´çš„[é¡¹ç›®ç±»å‹]ï¼Œå…·å¤‡[å®Œæ•´åŠŸèƒ½åˆ—è¡¨]ã€‚"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
ç”¨æˆ·å¯¹è¯å†å²ï¼š
{context_str}

å½“å‰è§„åˆ’å†…å®¹ï¼š
{current_plan if current_plan else "æš‚æ— è§„åˆ’å†…å®¹"}

ç”¨æˆ·å½“å‰æ¶ˆæ¯ï¼š{message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""


        else:
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """You are GTPlanner's AI assistant, specialized in helping users with project planning and design.

Please analyze the user's intent and respond accordingly. When analyzing, pay special attention to conversation context and existing plan content. Possible intents include:

1. **Project Requirement (requirement)**:
   - User explicitly requests to create a completely new system/application/project
   - User proposes new functional requirements or improvements based on existing plan
   - âš ï¸ **Important Principle**: If there's existing plan content, user's new requirements are functional extensions to the existing project. Must maintain the core functionality and main architecture of the original project while integrating new features

2. **Optimization (optimization)**: User provides feedback or suggestions to improve an existing plan.

3. **Document Generation (document_generation)**: User explicitly requests to generate a detailed document or to create documentation based on an existing plan.

4. **Normal Conversation (conversation)**: User is just greeting, thanking, asking about concepts, seeking advice, or having general discussion.

Please strictly follow the tag format below, ensuring the tag format is completely correct:

- Normal conversation: [TEXT_START]Your response content (can include suggestions, tips, etc.)[TEXT_END]
- Trigger short planning: [SHORT_PLAN_ACTION]Completely describe user requirements, including project background, objectives, functional requirements and other detailed information[/SHORT_PLAN_ACTION]
- Trigger long documentation: [LONG_DOC_ACTION][/LONG_DOC_ACTION]
- Trigger full flow: [FULL_FLOW_ACTION]Completely describe user requirements, including project background, objectives, functional requirements and other detailed information[/FULL_FLOW_ACTION]

Note: Suggestions and tips should be included directly within TEXT tags, do not use separate SUGGESTION tags.

âš ï¸ Tag Format Requirements (Must Follow Strictly):
1. TEXT tag format: Start with [TEXT_START], end with [TEXT_END] (Note: TEXT_END has NO slash!)
2. ACTION tag format: Start with [ACTION_NAME], end with [/ACTION_NAME] (Note: End tag has slash!)
3. Wrong examples: [/TEXT_START], [/TEXT_END], [SHORT_PLAN_ACTION/] are all WRONG
4. Correct examples: [TEXT_START]content[TEXT_END], [SHORT_PLAN_ACTION]content[/SHORT_PLAN_ACTION]
5. Do not add any extra characters before or after tag names
6. Tags must be on separate lines or directly adjacent to content, no extra spaces

Choose the appropriate tag based on user intent:

**When to use SHORT_PLAN_ACTION:**
- User proposes new project requirements or functional requirements (e.g.: develop XX system, create XX app, design XX feature)
- User requests to create, develop, design specific systems/applications/plugins/tools
- User describes specific product features and characteristics
- User proposes new features or improvements based on existing project
- When step-by-step implementation planning is needed

**Clear Project Requirement Keywords:**
- "develop", "create", "design", "build", "construct"
- "system", "application", "plugin", "tool", "platform"
- "feature", "functionality", "module", "component", "add", "integrate", "support"
- Descriptions of specific technical implementations or product characteristics

**âš ï¸ Core Principles for Feature Extension Planning:**
When users propose new features based on existing projects (e.g., "add XX feature", "integrate XX", "support XX"), the generated plan must:
1. **Maintain original project integrity**: Include all existing core functions and architecture
2. **Clarify new feature integration**: Explain how new features work with existing functionality
3. **Provide complete project planning**: Not an independent feature module plan, but a complete project plan including original + new features
4. **Maintain project theme consistency**: Ensure new features serve the core objectives of the original project

**When to use LONG_DOC_ACTION:**
- User explicitly requests detailed documents, design documents, technical documents
- User says "generate document", "create documentation based on plan", "detailed design", etc.
- There's existing plan content and user needs more detailed documentation
- When converting plans into complete design documents

**When to use FULL_FLOW_ACTION:**
- User proposes complete project requirements and you think both planning and detailed documentation are needed
- User's requirements are complex enough to warrant a complete project flow (planning + documentation)
- When you judge that user ultimately needs a complete project solution
- No explicit user request needed, you can proactively judge if full flow is appropriate

**Flexible Usage Strategy:**

**Direct Usage Rules:**
- If user requirements are already clear and complete, can directly use SHORT_PLAN_ACTION to generate planning
- If user explicitly requests document generation and has complete project background information, can directly use LONG_DOC_ACTION

**Must Use SHORT_PLAN_ACTION First When:**
- User requests document generation (LONG_DOC_ACTION) but current conversation lacks complete project planning
- Need to establish basic project planning framework before generating detailed documents
- User's requirement description is not specific enough, need to clarify implementation steps through planning process first

**Judgment Principles:**
- Prioritize completeness and executability of user requirements
- Ensure generated content has sufficient contextual support
- Avoid generating detailed documents when lacking basic planning foundation

**Basic Output Rules:**
- Project requirements: Directly use SHORT_PLAN_ACTION or FULL_FLOW_ACTION, no need to reply with TEXT first
- Document generation requests: Directly use LONG_DOC_ACTION, no need to reply with TEXT first
- Normal conversation: Only use TEXT tags

**Important: When identifying project requirements, must directly trigger corresponding ACTION, don't just reply with TEXT!**

**Judgment Examples:**
- "Multi-language vocabulary improvement design document" â†’ Project requirement â†’ Use SHORT_PLAN_ACTION
- "Browser extension for instant translation" â†’ Project requirement â†’ Use SHORT_PLAN_ACTION
- "Develop an online shopping system" â†’ Project requirement â†’ Use SHORT_PLAN_ACTION
- "Hello, what can you do?" â†’ Normal conversation â†’ Use TEXT tags
- "Generate detailed document based on current plan" â†’ Document generation â†’ Use LONG_DOC_ACTION

âš ï¸ Important Note:
1. **SHORT_PLAN_ACTION tags**: Output complete user requirement description, including project background, objectives, functional requirements, technical needs, etc.
2. **LONG_DOC_ACTION tags**: No content needed inside tags, system will use current planning content from context
3. **FULL_FLOW_ACTION tags**: Output complete user requirement description for short planning node, ensure requirement description is detailed and accurate
4. Based on current conversation context, accurately understand user's real intent and requirements
5. **If user is proposing new requirements based on existing project, requirement description must include:**
   - Complete background and core functionality description of the original project
   - Specific requirements for new features
   - Integration approach between new and existing features
   - Ensure the generated plan is a complete project plan, not an independent feature module plan

**Requirement Description Template (for feature extension scenarios):**
"Based on the existing [original project description], while maintaining its [core functionality list], add [new feature description]. The new feature should [integration approach] with existing features, ultimately forming a complete [project type] with [complete functionality list]."""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
User conversation history:
{context_str}

Current plan content:
{current_plan if current_plan else "No current plan available"}

Current user message: {message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«å’Œå“åº”ç”Ÿæˆ
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        yield stream_data_block("[STATUS_END]")

        # å¯¼å…¥æµå¼LLMè°ƒç”¨
        from utils.call_llm import call_llm_stream_async

        # æµå¼è¾“å‡ºLLMçš„å“åº”å¹¶ç›‘å¬ACTIONæ ‡ç­¾
        action_buffer = []
        in_action = False
        action_type = None
        content_buffer = ""
        pending_output = ""  # ç¼“å†²å¾…è¾“å‡ºçš„å†…å®¹

        async for chunk in call_llm_stream_async(prompt):
            if chunk:
                content_buffer += chunk
                pending_output += chunk

                # å¤„ç†ç¼“å†²åŒºä¸­çš„å®Œæ•´æ ‡ç­¾
                while True:
                    # æŸ¥æ‰¾æ ‡ç­¾å¼€å§‹
                    start_pos = pending_output.find('[')
                    if start_pos == -1:
                        # æ²¡æœ‰æ ‡ç­¾ï¼Œè¾“å‡ºæ‰€æœ‰å†…å®¹
                        if pending_output and not in_action:
                            yield stream_data(pending_output)
                        elif pending_output and in_action:
                            action_buffer.append(pending_output)
                        pending_output = ""
                        break

                    # è¾“å‡ºæ ‡ç­¾å‰çš„å†…å®¹
                    if start_pos > 0:
                        before_tag = pending_output[:start_pos]
                        if not in_action:
                            yield stream_data(before_tag)
                        else:
                            action_buffer.append(before_tag)

                    # æŸ¥æ‰¾æ ‡ç­¾ç»“æŸ
                    end_pos = pending_output.find(']', start_pos)
                    if end_pos == -1:
                        # æ ‡ç­¾ä¸å®Œæ•´ï¼Œä¿ç•™ä»æ ‡ç­¾å¼€å§‹çš„æ‰€æœ‰å†…å®¹
                        pending_output = pending_output[start_pos:]
                        break

                    # æå–å®Œæ•´æ ‡ç­¾
                    complete_tag = pending_output[start_pos:end_pos + 1]
                    remaining_content = pending_output[end_pos + 1:]

                    # å¤„ç†æ ‡ç­¾
                    if complete_tag == "[SHORT_PLAN_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "short_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[LONG_DOC_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "long_doc"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[FULL_FLOW_ACTION]":
                        in_action = True
                        action_buffer = []
                        action_type = "full_flow"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯

                    elif complete_tag in ["[/SHORT_PLAN_ACTION]", "[/LONG_DOC_ACTION]", "[/FULL_FLOW_ACTION]"] and in_action:
                        # å¤„ç†ACTIONå†…å®¹
                        action_content = ''.join(action_buffer).strip()
                        if action_content:
                            # æ ¹æ®action_typeæ„é€ action_content
                            full_action_content = f"{action_type}:{action_content}"
                            async for planning_chunk in handle_action(full_action_content, message, current_plan, language, conversation_history):
                                yield planning_chunk
                        in_action = False
                        action_buffer = []
                        action_type = None
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    else:
                        # éACTIONæ ‡ç­¾ï¼Œæ­£å¸¸å‘é€
                        if not in_action:
                            yield stream_data(complete_tag)
                        else:
                            action_buffer.append(complete_tag)

                    # ç»§ç»­å¤„ç†å‰©ä½™å†…å®¹
                    pending_output = remaining_content

        # å¤„ç†å¯èƒ½å‰©ä½™çš„å†…å®¹
        if pending_output:
            if not in_action:
                yield stream_data(pending_output)
            else:
                action_buffer.append(pending_output)

        yield stream_data_block("")  # ç©ºè¡Œè¡¨ç¤ºç»“æŸ

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_stream_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while generating response. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_direct_action_response(
    action: str,
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    æ ¹æ®æ˜ç¡®çš„actionç›´æ¥ç”Ÿæˆå“åº”ï¼Œè·³è¿‡AIæ„å›¾è¯†åˆ«
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        if action == "generate_document":
            # ç›´æ¥è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆ
            if not current_plan:
                yield stream_data("[ERROR_START]")
                yield stream_data("âŒ No current plan found. Cannot generate document.")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            yield stream_data("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯¦ç»†è®¾è®¡æ–‡æ¡£...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆæµå¼æ¥å£
            from api.v1.planning import long_planning_stream, LongPlanningRequest

            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in long_planning_stream(doc_request):
                yield chunk



        else:
            yield stream_data("[ERROR_START]")
            yield stream_data(f"âŒ Unsupported action type: {action}")
            yield stream_data_block("[ERROR_END]")

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_direct_action_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while processing the operation. Please try again later.")
        yield stream_data_block("[ERROR_END]")


@chat_router.post("/unified")
async def unified_conversation(body: ConversationRequest):
    """
    ç»Ÿä¸€å¯¹è¯æ¥å£ï¼šé›†æˆæ„å›¾è¯†åˆ«ã€å¯¹è¯å›å¤ã€è§„åˆ’ç”Ÿæˆå’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½
    å®Œå…¨ä½¿ç”¨æµå¼å“åº”
    """
    message = body.message
    conversation_history = body.conversation_history or []
    session_id = body.session_id
    language = body.language
    action = body.action
    context = body.context or {}

    if not message:
        async def error_stream():
            yield stream_data("[ERROR_START]")
            yield stream_data("âŒ Missing message in request body.")
            yield stream_data_block("[ERROR_END]")
        return StreamingResponse(error_stream(), media_type="text/plain")

    # ç¡®å®šè¯­è¨€ - ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ é€’çš„ç•Œé¢è¯­è¨€ï¼Œç¡®ä¿ç•Œé¢è¯­è¨€å’ŒLLMè¾“å‡ºè¯­è¨€çš„ä¸€è‡´æ€§
    language = determine_language(message, None, language)

    # æ ¹æ®actionå­—æ®µç›´æ¥è°ƒç”¨ç›¸åº”åŠŸèƒ½ï¼Œæˆ–ä½¿ç”¨AIæ„å›¾è¯†åˆ«
    if action:
        return StreamingResponse(
            generate_direct_action_response(action, message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )
    else:
        # è¿”å›æµå¼å“åº”ï¼ˆä½¿ç”¨AIæ„å›¾è¯†åˆ«ï¼‰
        return StreamingResponse(
            generate_stream_response(message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )


