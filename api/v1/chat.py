from typing import Any, Optional, List, Dict

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from utils.multilingual_utils import determine_language
# from utils.context_manager import optimize_conversation_context  # å·²ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½

# å¯¼å…¥è§„åˆ’ç›¸å…³çš„åŠŸèƒ½
from api.v1.planning import (
    short_planning, long_planning,
    short_planning_stream, long_planning_stream,
    ShortPlanningRequest, LongPlanningRequest
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])


def stream_data(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆSSEæ ¼å¼è¦æ±‚ï¼‰
    ä½¿ç”¨å ä½ç¬¦ä¿æŠ¤markdownæ¢è¡Œç¬¦ï¼Œé¿å…ä¸SSEåè®®å†²çª
    """
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…ä¸SSEæ¶ˆæ¯åˆ†éš”ç¬¦å†²çª
    protected_content = content.replace('\n', '<|newline|>')
    return f"data: {protected_content}\n".encode('utf-8')


def stream_data_block(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®å—ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œå¹¶æ·»åŠ ç»“æŸæ ‡è®°
    """
    return f"data: {content}\n\n".encode('utf-8')


class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    message_type: Optional[str] = "message"  # 'message', 'plan', 'document'
    timestamp: Optional[int] = None


class ConversationRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    session_id: Optional[str] = None
    language: Optional[str] = None
    action: Optional[str] = None  # 'generate_document'
    context: Optional[Dict[str, Any]] = None


class ConversationAction(BaseModel):
    type: str  # 'plan', 'document', 'suggestion'
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    language: Optional[str] = None
    user_id: Optional[str] = None


class IntentAnalysisResult(BaseModel):
    intent: str  # 'requirement' or 'conversation'
    confidence: float
    reasoning: str

async def process_intent_actions(intent: str, message: str, current_plan: str, actions: List[Dict], language: str) -> List[ConversationAction]:
    """
    æ ¹æ®æ„å›¾å¤„ç†ç‰¹æ®Šæ“ä½œï¼Œå¦‚ç”Ÿæˆè§„åˆ’æˆ–æ–‡æ¡£
    """
    processed_actions = []

    try:
        if intent == "requirement":
            # ç”ŸæˆçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="é¡¹ç›®è§„åˆ’" if language == "zh" else "Project Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "version": 1
                    }
                ))

        elif intent == "optimization" and current_plan:
            # ä¼˜åŒ–ç°æœ‰è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="ä¼˜åŒ–è§„åˆ’" if language == "zh" else "Optimized Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan,
                        "version": 2
                    }
                ))

        elif intent == "document_generation" and current_plan:
            # ç”Ÿæˆé•¿æ–‡æ¡£
            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            doc_result = await long_planning(doc_request)

            if "flow" in doc_result:
                processed_actions.append(ConversationAction(
                    type="document",
                    content=doc_result["flow"],
                    title="è®¾è®¡æ–‡æ¡£" if language == "zh" else "Design Document",
                    metadata={
                        "language": doc_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan
                    }
                ))

        # å¤„ç†LLMå»ºè®®çš„å…¶ä»–actions
        for action in actions:
            if isinstance(action, dict) and "type" in action and "content" in action:
                processed_actions.append(ConversationAction(
                    type=action.get("type", "suggestion"),
                    content=action.get("content", ""),
                    title=action.get("title"),
                    metadata=action.get("metadata", {})
                ))

    except Exception as e:
        print(f"Error processing intent actions: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå»ºè®®action
        processed_actions.append(ConversationAction(
            type="suggestion",
            content="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚" if language == "zh" else "There was an issue processing your request. Please try rephrasing your needs.",
            title="å»ºè®®" if language == "zh" else "Suggestion"
        ))

    return processed_actions


async def handle_action(action_content: str, original_message: str, current_plan: str, language: str, conversation_history: Optional[List[ChatMessage]] = None):
    """
    å¤„ç†ACTIONæ ‡ç­¾ï¼Œè°ƒç”¨ç›¸åº”çš„æµå¼planningæ¥å£å¹¶è¿”å›æµå¼ç»“æœ
    """
    try:
        # è§£æactionç±»å‹å’Œå†…å®¹
        if action_content.startswith("short_plan:"):
            requirement = action_content[11:].strip() or original_message

            # è°ƒç”¨æµå¼çŸ­è§„åˆ’æ¥å£
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk



        elif action_content.startswith("long_doc:"):
            # LONG_DOC_ACTIONä¸ä½¿ç”¨æ ‡ç­¾å†…å®¹ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡æ¡£ç”Ÿæˆéœ€æ±‚
            doc_requirement = "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†çš„è®¾è®¡æ–‡æ¡£" if language == "zh" else "Generate detailed design document based on current plan"

            # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨context.current_plan
            doc_request = LongPlanningRequest(
                requirement=doc_requirement,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in long_planning_stream(doc_request):
                if isinstance(chunk, str):
                    # long_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("full_flow:"):
            requirement = action_content[10:].strip() or original_message

            # å…ˆæ‰§è¡ŒçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )

            # æ”¶é›†çŸ­è§„åˆ’çš„ç»“æœ
            plan_result = ""
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    yield chunk.encode('utf-8')
                    # æå–è§„åˆ’å†…å®¹ï¼ˆå»é™¤SSEæ ¼å¼ï¼‰
                    if chunk.startswith("data: ") and not chunk.startswith("data: ["):
                        plan_content = chunk[6:].replace('<|newline|>', '\n').strip()
                        if plan_content:
                            plan_result += plan_content + "\n"
                else:
                    yield chunk

            # ç­‰å¾…çŸ­è§„åˆ’å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘é•¿æ–‡æ¡£ç”Ÿæˆ
            if plan_result.strip():
                # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨åˆšç”Ÿæˆçš„è§„åˆ’ä½œä¸ºprevious_flow
                doc_request = LongPlanningRequest(
                    requirement=requirement,
                    previous_flow=plan_result.strip(),
                    language=language
                )
                async for chunk in long_planning_stream(doc_request):
                    if isinstance(chunk, str):
                        yield chunk.encode('utf-8')
                    else:
                        yield chunk

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in handle_action: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while processing the action. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_stream_response(
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    ç”ŸæˆåŸºäºæ ‡ç­¾çš„æµå¼å“åº”
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        # ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¯¹è¯å†å²
        # TODO: åç»­ç ”ç©¶æ›´æ™ºèƒ½çš„å‹ç¼©æ–¹æ¡ˆæ—¶å¯ä»¥é‡æ–°å¯ç”¨ä»¥ä¸‹ä»£ç 
        # try:
        #     # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        #     history_dicts = []
        #     for msg in conversation_history:
        #         history_dicts.append({
        #             "role": msg.role,
        #             "content": msg.content,
        #             "message_type": msg.message_type,
        #             "timestamp": msg.timestamp
        #         })
        #
        #     # ä¼˜åŒ–ä¸Šä¸‹æ–‡
        #     context_str, context_stats = await optimize_conversation_context(
        #         history_dicts, message
        #     )
        # except Exception as e:
        #     print(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")

        # ç›´æ¥å¤„ç†åŸå§‹å¯¹è¯å†å²ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·èŠå¤©è®°å½•
        user_conversation_history = []
        for msg in conversation_history:
            # åªåŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ¶ˆæ¯ï¼Œæ’é™¤ç³»ç»Ÿæ¶ˆæ¯
            if msg.role in ["user", "assistant"] and msg.message_type == "message":
                msg_content = f"{msg.role}: {msg.content}"
                user_conversation_history.append(msg_content)

        # æ„å»ºçº¯å‡€çš„ç”¨æˆ·å¯¹è¯å†å²å­—ç¬¦ä¸²
        context_str = "\n".join(user_conversation_history) if user_conversation_history else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"

        # æ„å»ºåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡
        if language == "zh":
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """ä½ æ˜¯GTPlannerçš„AIåŠ©æ‰‹ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æ„å›¾ï¼Œå¹¶æ ¹æ®é¢„è®¾æ ¼å¼è¾“å‡ºæŒ‡ä»¤ã€‚

#### **1. æ ¸å¿ƒä»»åŠ¡**
åˆ†æç”¨æˆ·åœ¨å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„æ„å›¾ï¼Œä»ä»¥ä¸‹å››ç§ç±»å‹ä¸­é€‰æ‹©ä¸€ç§å¹¶æŒ‰è¦æ±‚å›åº”ï¼š
*   **é¡¹ç›®è§„åˆ’ (Project Planning)**: ç”¨æˆ·æå‡ºæ–°é¡¹ç›®ã€æ–°åŠŸèƒ½æˆ–ä¼˜åŒ–å»ºè®®ã€‚
*   **æ–‡æ¡£ç”Ÿæˆ (Document Generation)**: ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè®¾è®¡æˆ–æŠ€æœ¯æ–‡æ¡£ã€‚
*   **å®Œæ•´æµç¨‹ (Full Flow)**: ç”¨æˆ·æå‡ºå¤æ‚é¡¹ç›®éœ€æ±‚ï¼Œé€‚åˆä¸€æ¬¡æ€§ç”Ÿæˆè§„åˆ’å’Œæ–‡æ¡£ã€‚
*   **æ™®é€šå¯¹è¯ (Conversation)**: æ—¥å¸¸é—®å€™ã€æ„Ÿè°¢ã€æˆ–ä¸€èˆ¬æ€§æé—®ã€‚

#### **2. è¾“å‡ºæ ¼å¼ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)**
*   **æ™®é€šå¯¹è¯**: `[TEXT_START]ä½ çš„å›å¤å†…å®¹[TEXT_END]`
*   **é¡¹ç›®è§„åˆ’**: `[SHORT_PLAN_ACTION_START]å®Œæ•´çš„æœ€ç»ˆéœ€æ±‚åˆ—è¡¨[SHORT_PLAN_ACTION_END]`
*   **æ–‡æ¡£ç”Ÿæˆ**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (æ ‡ç­¾å†…æ— å†…å®¹)
*   **å®Œæ•´æµç¨‹**: `[FULL_FLOW_ACTION_START]å®Œæ•´çš„æœ€ç»ˆéœ€æ±‚åˆ—è¡¨[FULL_FLOW_ACTION_END]`

**æ ¼å¼è§„åˆ™**:
1.  æ‰€æœ‰æ ‡ç­¾éƒ½ä½¿ç”¨ `[TAG_START]` å’Œ `[TAG_END]` çš„é…å¯¹æ ¼å¼ã€‚
2.  æ ‡ç­¾å‰åä¸è¦æœ‰ä»»ä½•å¤šä½™å­—ç¬¦æˆ–ç©ºæ ¼ã€‚
3.  ç¡®ä¿å¼€å§‹å’Œç»“æŸæ ‡ç­¾ä¸¥æ ¼é…å¯¹ã€‚

#### **3. å·¥ä½œæµç¨‹ä¸å†³ç­–**
1.  **è¯†åˆ«æ„å›¾**ï¼Œå¹¶æ ¹æ®æ„å›¾é€‰æ‹©`ACTION`ã€‚
2.  **å¤„ç†é¡¹ç›®è§„åˆ’**: å¦‚æœæ„å›¾æ˜¯â€œé¡¹ç›®è§„åˆ’â€ï¼Œåˆ™**å¿…é¡»**éµå¾ªç¬¬4èŠ‚çš„è§„åˆ™ã€‚
3.  **ç¦æ­¢å¯¹è¯**: å½“æ„å›¾ä¸ºâ€œé¡¹ç›®è§„åˆ’â€æˆ–â€œæ–‡æ¡£ç”Ÿæˆâ€æ—¶ï¼Œ**ç¦æ­¢**ä½¿ç”¨ `[TEXT_START]` è¿›è¡Œå›å¤ã€‚

#### **4. éœ€æ±‚å¤„ç†è§„åˆ™ (æ ¸å¿ƒ)**

æ­¤è§„åˆ™ç”¨äºç”Ÿæˆ `[SHORT_PLAN_ACTION_START]` å’Œ `[FULL_FLOW_ACTION_START]` æ ‡ç­¾å†…çš„å†…å®¹ã€‚

1.  **åˆå¹¶éœ€æ±‚**:
    *   **æ–°é¡¹ç›®**: å¦‚æœæ˜¯å…¨æ–°çš„é¡¹ç›®ï¼Œç›´æ¥æ•´ç†ç”¨æˆ·çš„éœ€æ±‚ã€‚
    *   **åŠŸèƒ½æ‰©å±•/ä¿®æ”¹**: å¦‚æœç”¨æˆ·åœ¨å·²æœ‰è§„åˆ’ä¸Šæå‡ºæ–°è¦æ±‚ï¼Œä½ **å¿…é¡»**åœ¨åå°é»˜é»˜åœ°å°†**å†å²éœ€æ±‚**ä¸ç”¨æˆ·çš„**æœ€æ–°éœ€æ±‚**è¿›è¡Œåˆå¹¶ã€‚

2.  **è¾“å‡ºæœ€ç»ˆçš„å®Œæ•´æ¸…å•**:
    *   `ACTION`æ ‡ç­¾å†…çš„å†…å®¹ï¼Œ**å¿…é¡»æ˜¯åˆå¹¶åå…¨æ–°çš„ã€å®Œæ•´çš„ã€è¦†ç›–æ‰€æœ‰åŠŸèƒ½ç‚¹çš„éœ€æ±‚æ¸…å•**ã€‚
    *   **ã€å…³é”®ç¦æ­¢é¡¹ã€‘**: ç¦æ­¢åŒ…å«ä»»ä½•æ‰¿ä¸Šå¯ä¸‹ã€è§£é‡Šæ€§æˆ–å¯¹æ¯”æ€§çš„æ–‡å­—ã€‚ä¸¥ç¦å‡ºç°â€œåŸºäºç°æœ‰è§„åˆ’...â€ã€â€œåœ¨...åŸºç¡€ä¸Šæ–°å¢...â€æˆ–â€œæœ€ç»ˆå½¢æˆä¸€ä¸ª...â€ç­‰æè¿°æ€§è¯­å¥ã€‚
    *   **ã€å…³é”®è¦æ±‚ã€‘**: ç›´æ¥äº†å½“åœ°å°†æ‰€æœ‰éœ€æ±‚è¦ç‚¹ï¼Œä¸€æ¡ä¸€æ¡åˆ—å‡ºæ¥ï¼Œå°±å¥½åƒè¿™æ˜¯ç¬¬ä¸€æ¬¡æå‡ºçš„å®Œæ•´è§„åˆ’ä¸€æ ·ã€‚

#### **5. ç¤ºä¾‹**

*   **ç”¨æˆ·ç¬¬ä¸€è½®**: "æˆ‘è¦åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€è¦æœ‰å•†å“æµè§ˆå’Œè´­ç‰©è½¦ã€‚"
    *   **æ¨¡å‹è¾“å‡º**: `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š 1. å•†å“æµè§ˆä¸æœç´¢ 2. è´­ç‰©è½¦ç®¡ç†[SHORT_PLAN_ACTION_END]`

*   **ç”¨æˆ·ç¬¬äºŒè½®**: "å¾ˆå¥½ï¼Œç°åœ¨ç»™æˆ‘åŠ ä¸Šç”¨æˆ·ç™»å½•å’Œä¼˜æƒ åˆ¸åŠŸèƒ½ã€‚"
    *   **æ¨¡å‹è¾“å‡º (æ­£ç¡®ç¤ºä¾‹)**: `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿï¼Œéœ€æ±‚å¦‚ä¸‹ï¼š 1. å•†å“æµè§ˆä¸æœç´¢ 2. è´­ç‰©è½¦ç®¡ç† 3. ç”¨æˆ·æ³¨å†Œä¸ç™»å½• 4. ä¼˜æƒ åˆ¸ç³»ç»Ÿ[SHORT_PLAN_ACTION_END]`
    *   **æ¨¡å‹è¾“å‡º (é”™è¯¯ç¤ºä¾‹)**: `[SHORT_PLAN_ACTION_START]åŸºäºè´­ç‰©ç³»ç»Ÿï¼Œæ–°å¢ç™»å½•å’Œä¼˜æƒ åˆ¸...` (è¿™æ˜¯é”™è¯¯çš„ï¼ŒåŒ…å«äº†è¿‡ç¨‹æè¿°)"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
ç”¨æˆ·å¯¹è¯å†å²ï¼š
{context_str}

å½“å‰è§„åˆ’å†…å®¹ï¼š
{current_plan if current_plan else "æš‚æ— è§„åˆ’å†…å®¹"}

ç”¨æˆ·å½“å‰æ¶ˆæ¯ï¼š{message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""


        else:
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """You are GTPlanner, an AI assistant. Your core task is to analyze user intent from the conversation context and respond with instructions in a predefined format.

#### **1. Core Task**
Analyze the user's intent and choose one of the four following types to formulate your response:
*   **Project Planning**: The user proposes a new project, new features, or suggests optimizations.
*   **Document Generation**: The user explicitly asks to generate design or technical documents.
*   **Full Flow**: The user presents a complex project requirement, suitable for generating a plan and documents in one go.
*   **General Conversation**: The user offers a greeting, thanks, or asks a general question.

#### **2. Output Format (Must Be Strictly Followed)**
*   **General Conversation**: `[TEXT_START]Your response here[TEXT_END]`
*   **Project Planning**: `[SHORT_PLAN_ACTION_START]The complete and final list of requirements[SHORT_PLAN_ACTION_END]`
*   **Document Generation**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (The tag contents must be empty)
*   **Full Flow**: `[FULL_FLOW_ACTION_START]The complete and final list of requirements[FULL_FLOW_ACTION_END]`

**Formatting Rules**:
1.  All tags must use the paired `[TAG_START]` and `[TAG_END]` format.
2.  There must be no extra characters or spaces before or after a tag.
3.  Ensure all opening and closing tags are strictly matched.

#### **3. Workflow and Decision Logic**
1.  **Identify Intent** and select the appropriate `ACTION` based on the user's input.
2.  **Process Project Planning**: If the intent is "Project Planning", you **MUST** follow the rules in Section 4.
3.  **No Conversation for Actions**: When the intent is "Project Planning" or "Document Generation", you are **forbidden** from using `[TEXT_START]` to reply.

#### **4. Requirement Processing Rules (Core Logic)**

This section defines how to generate the content inside the `[SHORT_PLAN_ACTION_START]` and `[FULL_FLOW_ACTION_START]` tags.

1.  **Merge Requirements**:
    *   **New Project**: If the user is proposing a project for the first time, simply organize their requirements.
    *   **Feature Addition/Modification**: If the user adds new requirements to an existing plan from the conversation history, you **MUST** silently merge the **historical requirements** with the **latest user input** in the background.

2.  **Output the Final, Complete List**:
    *   The content inside the `ACTION` tag **MUST** be the new, complete, and consolidated list of requirements covering all features.
    *   **[CRITICAL PROHIBITION]**: Strictly avoid any transitional, explanatory, or comparative text. You are forbidden from using phrases like "Based on the existing plan...", "Adding the following features to...", or "The final system will include...".
    *   **[CRITICAL REQUIREMENT]**: Directly and plainly list all requirement points, one by one, as if this is the first time the complete plan has been specified.

#### **5. Examples**

*   **User, Turn 1**: "I want to create an online shopping system with product browsing and a shopping cart."
    *   **Model Output**: `[SHORT_PLAN_ACTION_START]Create an online shopping system with the following requirements: 1. Product browsing and search 2. Shopping cart management[SHORT_PLAN_ACTION_END]`

*   **User, Turn 2**: "Great, now add user login and a coupon feature."
    *   **Model Output (Correct)**: `[SHORT_PLAN_ACTION_START]Create an online shopping system with the following requirements: 1. Product browsing and search 2. Shopping cart management 3. User registration and login 4. Coupon system[SHORT_PLAN_ACTION_END]`
    *   **Model Output (Incorrect)**: `[SHORT_PLAN_ACTION_START]Based on the shopping system, we will now add user login and a coupon feature...` (This is wrong because it includes explanatory text)."""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
User conversation history:
{context_str}

Current plan content:
{current_plan if current_plan else "No current plan available"}

Current user message: {message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«å’Œå“åº”ç”Ÿæˆ
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        yield stream_data_block("[STATUS_END]")

        # å¯¼å…¥æµå¼LLMè°ƒç”¨
        from utils.call_llm import call_llm_stream_async

        # æµå¼è¾“å‡ºLLMçš„å“åº”å¹¶ç›‘å¬ACTIONæ ‡ç­¾
        action_buffer = []
        in_action = False
        action_type = None
        content_buffer = ""
        pending_output = ""  # ç¼“å†²å¾…è¾“å‡ºçš„å†…å®¹

        async for chunk in call_llm_stream_async(prompt):
            if chunk:
                content_buffer += chunk
                pending_output += chunk

                # å¤„ç†ç¼“å†²åŒºä¸­çš„å®Œæ•´æ ‡ç­¾
                while True:
                    # æŸ¥æ‰¾æ ‡ç­¾å¼€å§‹
                    start_pos = pending_output.find('[')
                    if start_pos == -1:
                        # æ²¡æœ‰æ ‡ç­¾ï¼Œè¾“å‡ºæ‰€æœ‰å†…å®¹
                        if pending_output and not in_action:
                            yield stream_data(pending_output)
                        elif pending_output and in_action:
                            action_buffer.append(pending_output)
                        pending_output = ""
                        break

                    # è¾“å‡ºæ ‡ç­¾å‰çš„å†…å®¹
                    if start_pos > 0:
                        before_tag = pending_output[:start_pos]
                        if not in_action:
                            yield stream_data(before_tag)
                        else:
                            action_buffer.append(before_tag)

                    # æŸ¥æ‰¾æ ‡ç­¾ç»“æŸ
                    end_pos = pending_output.find(']', start_pos)
                    if end_pos == -1:
                        # æ ‡ç­¾ä¸å®Œæ•´ï¼Œä¿ç•™ä»æ ‡ç­¾å¼€å§‹çš„æ‰€æœ‰å†…å®¹
                        pending_output = pending_output[start_pos:]
                        break

                    # æå–å®Œæ•´æ ‡ç­¾
                    complete_tag = pending_output[start_pos:end_pos + 1]
                    remaining_content = pending_output[end_pos + 1:]

                    # å¤„ç†æ ‡ç­¾
                    if complete_tag == "[SHORT_PLAN_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "short_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[LONG_DOC_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "long_doc"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[FULL_FLOW_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "full_flow"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯

                    elif complete_tag in ["[SHORT_PLAN_ACTION_END]", "[LONG_DOC_ACTION_END]", "[FULL_FLOW_ACTION_END]"] and in_action:
                        # å¤„ç†ACTIONå†…å®¹
                        action_content = ''.join(action_buffer).strip()
                        if action_content:
                            # æ ¹æ®action_typeæ„é€ action_content
                            full_action_content = f"{action_type}:{action_content}"
                            async for planning_chunk in handle_action(full_action_content, message, current_plan, language, conversation_history):
                                yield planning_chunk
                        in_action = False
                        action_buffer = []
                        action_type = None
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    else:
                        # éACTIONæ ‡ç­¾ï¼Œæ­£å¸¸å‘é€
                        if not in_action:
                            yield stream_data(complete_tag)
                        else:
                            action_buffer.append(complete_tag)

                    # ç»§ç»­å¤„ç†å‰©ä½™å†…å®¹
                    pending_output = remaining_content

        # å¤„ç†å¯èƒ½å‰©ä½™çš„å†…å®¹
        if pending_output:
            if not in_action:
                yield stream_data(pending_output)
            else:
                action_buffer.append(pending_output)

        yield stream_data_block("")  # ç©ºè¡Œè¡¨ç¤ºç»“æŸ

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_stream_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while generating response. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_direct_action_response(
    action: str,
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    æ ¹æ®æ˜ç¡®çš„actionç›´æ¥ç”Ÿæˆå“åº”ï¼Œè·³è¿‡AIæ„å›¾è¯†åˆ«
    """
   

    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯ - æ ¹æ®è¯­è¨€æœ¬åœ°åŒ–
        yield stream_data("[STATUS_START]")
        if language == "zh":
            yield stream_data("ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        else:
            yield stream_data("ğŸ”„ Processing your request...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        if action == "generate_document":
            # ç›´æ¥è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆ
            if not current_plan:
                yield stream_data("[ERROR_START]")
                if language == "zh":
                    yield stream_data("âŒ æœªæ‰¾åˆ°å½“å‰è§„åˆ’å†…å®¹ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£ã€‚")
                else:
                    yield stream_data("âŒ No current plan found. Cannot generate document.")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            if language == "zh":
                yield stream_data("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯¦ç»†è®¾è®¡æ–‡æ¡£...")
            else:
                yield stream_data("ğŸ“„ Generating detailed design document...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆæµå¼æ¥å£
            from api.v1.planning import long_planning_stream, LongPlanningRequest

            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in long_planning_stream(doc_request):
                yield chunk



        else:
            yield stream_data("[ERROR_START]")
            if language == "zh":
                yield stream_data(f"âŒ ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}")
            else:
                yield stream_data(f"âŒ Unsupported action type: {action}")
            yield stream_data_block("[ERROR_END]")

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_direct_action_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        if language == "zh":
            yield stream_data("âŒ å¤„ç†æ“ä½œæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
        else:
            yield stream_data("âŒ An internal error occurred while processing the operation. Please try again later.")
        yield stream_data_block("[ERROR_END]")


@chat_router.post("/unified")
async def unified_conversation(body: ConversationRequest):
    """
    ç»Ÿä¸€å¯¹è¯æ¥å£ï¼šé›†æˆæ„å›¾è¯†åˆ«ã€å¯¹è¯å›å¤ã€è§„åˆ’ç”Ÿæˆå’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½
    å®Œå…¨ä½¿ç”¨æµå¼å“åº”
    """
    message = body.message
    conversation_history = body.conversation_history or []
    session_id = body.session_id
    language = body.language
    action = body.action
    context = body.context or {}

    
    if not message:
        async def error_stream():
            yield stream_data("[ERROR_START]")
            yield stream_data("âŒ Missing message in request body.")
            yield stream_data_block("[ERROR_END]")
        return StreamingResponse(error_stream(), media_type="text/plain")

    # ç¡®å®šè¯­è¨€ - ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ é€’çš„ç•Œé¢è¯­è¨€ï¼Œç¡®ä¿ç•Œé¢è¯­è¨€å’ŒLLMè¾“å‡ºè¯­è¨€çš„ä¸€è‡´æ€§
    language = determine_language(message, None, language)

    # æ ¹æ®actionå­—æ®µç›´æ¥è°ƒç”¨ç›¸åº”åŠŸèƒ½ï¼Œæˆ–ä½¿ç”¨AIæ„å›¾è¯†åˆ«
    if action:
        return StreamingResponse(
            generate_direct_action_response(action, message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )
    else:
        # è¿”å›æµå¼å“åº”ï¼ˆä½¿ç”¨AIæ„å›¾è¯†åˆ«ï¼‰
        return StreamingResponse(
            generate_stream_response(message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )


