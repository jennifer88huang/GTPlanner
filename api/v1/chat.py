from typing import Any, Optional, List, Dict

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from utils.multilingual_utils import determine_language
# from utils.context_manager import optimize_conversation_context  # å·²ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½

# å¯¼å…¥è§„åˆ’ç›¸å…³çš„åŠŸèƒ½
from api.v1.planning import (
    short_planning, long_planning,
    short_planning_stream, long_planning_stream,
    ShortPlanningRequest, LongPlanningRequest
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])


def stream_data(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆSSEæ ¼å¼è¦æ±‚ï¼‰
    ä½¿ç”¨å ä½ç¬¦ä¿æŠ¤markdownæ¢è¡Œç¬¦ï¼Œé¿å…ä¸SSEåè®®å†²çª
    """
    # å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œé¿å…ä¸SSEæ¶ˆæ¯åˆ†éš”ç¬¦å†²çª
    protected_content = content.replace('\n', '<|newline|>')
    return f"data: {protected_content}\n".encode('utf-8')


def stream_data_block(content: str) -> bytes:
    """
    å°è£…æµå¼è¾“å‡ºæ•°æ®å—ï¼Œç¡®ä¿UTF-8ç¼–ç ï¼Œå¹¶æ·»åŠ ç»“æŸæ ‡è®°
    """
    return f"data: {content}\n\n".encode('utf-8')


class ChatMessage(BaseModel):
    role: str  # 'user' or 'assistant'
    content: str
    message_type: Optional[str] = "message"  # 'message', 'plan', 'document'
    timestamp: Optional[int] = None


class ConversationRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    session_id: Optional[str] = None
    language: Optional[str] = None
    action: Optional[str] = None  # 'generate_document'
    context: Optional[Dict[str, Any]] = None


class ConversationAction(BaseModel):
    type: str  # 'plan', 'document', 'suggestion'
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatRequest(BaseModel):
    message: str
    conversation_history: Optional[List[ChatMessage]] = []
    language: Optional[str] = None
    user_id: Optional[str] = None


class IntentAnalysisResult(BaseModel):
    intent: str  # 'requirement' or 'conversation'
    confidence: float
    reasoning: str

async def process_intent_actions(intent: str, message: str, current_plan: str, actions: List[Dict], language: str) -> List[ConversationAction]:
    """
    æ ¹æ®æ„å›¾å¤„ç†ç‰¹æ®Šæ“ä½œï¼Œå¦‚ç”Ÿæˆè§„åˆ’æˆ–æ–‡æ¡£
    """
    processed_actions = []

    try:
        if intent == "requirement":
            # ç”ŸæˆçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="é¡¹ç›®è§„åˆ’" if language == "zh" else "Project Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "version": 1
                    }
                ))

        elif intent == "optimization" and current_plan:
            # ä¼˜åŒ–ç°æœ‰è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            planning_result = await short_planning(planning_request)

            if "flow" in planning_result:
                processed_actions.append(ConversationAction(
                    type="plan",
                    content=planning_result["flow"],
                    title="ä¼˜åŒ–è§„åˆ’" if language == "zh" else "Optimized Plan",
                    metadata={
                        "language": planning_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan,
                        "version": 2
                    }
                ))

        elif intent == "document_generation" and current_plan:
            # ç”Ÿæˆé•¿æ–‡æ¡£
            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )
            doc_result = await long_planning(doc_request)

            if "flow" in doc_result:
                processed_actions.append(ConversationAction(
                    type="document",
                    content=doc_result["flow"],
                    title="è®¾è®¡æ–‡æ¡£" if language == "zh" else "Design Document",
                    metadata={
                        "language": doc_result.get("language", language),
                        "based_on": current_plan[:100] + "..." if len(current_plan) > 100 else current_plan
                    }
                ))

        # å¤„ç†LLMå»ºè®®çš„å…¶ä»–actions
        for action in actions:
            if isinstance(action, dict) and "type" in action and "content" in action:
                processed_actions.append(ConversationAction(
                    type=action.get("type", "suggestion"),
                    content=action.get("content", ""),
                    title=action.get("title"),
                    metadata=action.get("metadata", {})
                ))

    except Exception as e:
        print(f"Error processing intent actions: {e}")
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªå»ºè®®action
        processed_actions.append(ConversationAction(
            type="suggestion",
            content="å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚ã€‚" if language == "zh" else "There was an issue processing your request. Please try rephrasing your needs.",
            title="å»ºè®®" if language == "zh" else "Suggestion"
        ))

    return processed_actions


async def handle_action(action_content: str, original_message: str, current_plan: str, language: str, conversation_history: Optional[List[ChatMessage]] = None):
    """
    å¤„ç†ACTIONæ ‡ç­¾ï¼Œè°ƒç”¨ç›¸åº”çš„æµå¼planningæ¥å£å¹¶è¿”å›æµå¼ç»“æœ
    """
    try:
        # è§£æactionç±»å‹å’Œå†…å®¹
        if action_content.startswith("short_plan:"):
            requirement = action_content[11:].strip() or original_message

            # è°ƒç”¨æµå¼çŸ­è§„åˆ’æ¥å£
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    # short_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk



        elif action_content.startswith("long_doc:"):
            # LONG_DOC_ACTIONä¸ä½¿ç”¨æ ‡ç­¾å†…å®¹ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡æ¡£ç”Ÿæˆéœ€æ±‚
            doc_requirement = "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†çš„è®¾è®¡æ–‡æ¡£" if language == "zh" else "Generate detailed design document based on current plan"

            # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨context.current_plan
            doc_request = LongPlanningRequest(
                requirement=doc_requirement,
                previous_flow=current_plan,
                language=language
            )
            async for chunk in long_planning_stream(doc_request):
                if isinstance(chunk, str):
                    # long_planning_streamå·²ç»è¿”å›æ ¼å¼åŒ–çš„SSEæ•°æ®ï¼Œç›´æ¥è¾“å‡º
                    yield chunk.encode('utf-8')
                else:
                    yield chunk

        elif action_content.startswith("full_flow:"):
            requirement = action_content[10:].strip() or original_message

            # å…ˆæ‰§è¡ŒçŸ­è§„åˆ’
            planning_request = ShortPlanningRequest(
                requirement=requirement,
                language=language
            )

            # æ”¶é›†çŸ­è§„åˆ’çš„ç»“æœ
            plan_result = ""
            async for chunk in short_planning_stream(planning_request):
                if isinstance(chunk, str):
                    yield chunk.encode('utf-8')
                    # æå–è§„åˆ’å†…å®¹ï¼ˆå»é™¤SSEæ ¼å¼ï¼‰
                    if chunk.startswith("data: ") and not chunk.startswith("data: ["):
                        plan_content = chunk[6:].replace('<|newline|>', '\n').strip()
                        if plan_content:
                            plan_result += plan_content + "\n"
                else:
                    yield chunk

            # ç­‰å¾…çŸ­è§„åˆ’å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘é•¿æ–‡æ¡£ç”Ÿæˆ
            if plan_result.strip():
                # è°ƒç”¨æµå¼é•¿æ–‡æ¡£ç”Ÿæˆï¼Œä½¿ç”¨åˆšç”Ÿæˆçš„è§„åˆ’ä½œä¸ºprevious_flow
                doc_request = LongPlanningRequest(
                    requirement=requirement,
                    previous_flow=plan_result.strip(),
                    language=language
                )
                async for chunk in long_planning_stream(doc_request):
                    if isinstance(chunk, str):
                        yield chunk.encode('utf-8')
                    else:
                        yield chunk

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in handle_action: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while processing the action. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_stream_response(
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    ç”ŸæˆåŸºäºæ ‡ç­¾çš„æµå¼å“åº”
    """
    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        # ç¦ç”¨ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¯¹è¯å†å²
        # TODO: åç»­ç ”ç©¶æ›´æ™ºèƒ½çš„å‹ç¼©æ–¹æ¡ˆæ—¶å¯ä»¥é‡æ–°å¯ç”¨ä»¥ä¸‹ä»£ç 
        # try:
        #     # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        #     history_dicts = []
        #     for msg in conversation_history:
        #         history_dicts.append({
        #             "role": msg.role,
        #             "content": msg.content,
        #             "message_type": msg.message_type,
        #             "timestamp": msg.timestamp
        #         })
        #
        #     # ä¼˜åŒ–ä¸Šä¸‹æ–‡
        #     context_str, context_stats = await optimize_conversation_context(
        #         history_dicts, message
        #     )
        # except Exception as e:
        #     print(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {e}")

        # ç›´æ¥å¤„ç†åŸå§‹å¯¹è¯å†å²ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·èŠå¤©è®°å½•
        user_conversation_history = []
        for msg in conversation_history:
            # åªåŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯æ¶ˆæ¯ï¼Œæ’é™¤ç³»ç»Ÿæ¶ˆæ¯
            if msg.role in ["user", "assistant"] and msg.message_type == "message":
                msg_content = f"{msg.role}: {msg.content}"
                user_conversation_history.append(msg_content)

        # æ„å»ºçº¯å‡€çš„ç”¨æˆ·å¯¹è¯å†å²å­—ç¬¦ä¸²
        context_str = "\n".join(user_conversation_history) if user_conversation_history else "è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚"

        # æ„å»ºåˆ†ç¦»çš„ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡
        if language == "zh":
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """ä½ æ˜¯GTPlannerçš„AIåŠ©æ‰‹ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·æ„å›¾ï¼Œå¹¶æ ¹æ®é¢„è®¾æ ¼å¼è¾“å‡ºæŒ‡ä»¤ã€‚

#### **1. æ ¸å¿ƒä»»åŠ¡**
åˆ†æç”¨æˆ·åœ¨å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„æ„å›¾ï¼Œä»ä»¥ä¸‹å››ç§ç±»å‹ä¸­é€‰æ‹©ä¸€ç§å¹¶æŒ‰è¦æ±‚å›åº”ï¼š
*   **é¡¹ç›®è§„åˆ’ (Project Planning)**: ç”¨æˆ·æå‡ºæ–°é¡¹ç›®ã€æ–°åŠŸèƒ½æˆ–ä¼˜åŒ–å»ºè®®ã€‚
*   **æ–‡æ¡£ç”Ÿæˆ (Document Generation)**: ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆè®¾è®¡æˆ–æŠ€æœ¯æ–‡æ¡£ã€‚
*   **å®Œæ•´æµç¨‹ (Full Flow)**: ç”¨æˆ·æå‡ºå¤æ‚é¡¹ç›®éœ€æ±‚ï¼Œé€‚åˆä¸€æ¬¡æ€§ç”Ÿæˆè§„åˆ’å’Œæ–‡æ¡£ã€‚
*   **æ™®é€šå¯¹è¯ (Conversation)**: æ—¥å¸¸é—®å€™ã€æ„Ÿè°¢ã€æˆ–ä¸€èˆ¬æ€§æé—®ã€‚

#### **2. è¾“å‡ºæ ¼å¼ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)**
*   **æ™®é€šå¯¹è¯**: `[TEXT_START]ä½ çš„å›å¤å†…å®¹[TEXT_END]`
*   **é¡¹ç›®è§„åˆ’**: `[SHORT_PLAN_ACTION_START]è¯¦ç»†çš„éœ€æ±‚æè¿°[SHORT_PLAN_ACTION_END]`
*   **æ–‡æ¡£ç”Ÿæˆ**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (æ ‡ç­¾å†…æ— å†…å®¹)
*   **å®Œæ•´æµç¨‹**: `[FULL_FLOW_ACTION_START]è¯¦ç»†çš„éœ€æ±‚æè¿°[FULL_FLOW_ACTION_END]`

**æ ¼å¼è§„åˆ™**:
1.  æ‰€æœ‰æ ‡ç­¾éƒ½ä½¿ç”¨ `[TAG_START]` å’Œ `[TAG_END]` çš„é…å¯¹æ ¼å¼ã€‚
2.  æ ‡ç­¾å‰åä¸è¦æœ‰ä»»ä½•å¤šä½™å­—ç¬¦æˆ–ç©ºæ ¼ã€‚
3.  ç¡®ä¿å¼€å§‹å’Œç»“æŸæ ‡ç­¾ä¸¥æ ¼é…å¯¹ã€‚

#### **3. å·¥ä½œæµç¨‹ä¸å†³ç­–**

1.  **è¯†åˆ«æ„å›¾**:
    *   **é¡¹ç›®è§„åˆ’**: é‡åˆ° "å¼€å‘"ã€"åˆ›å»º"ã€"è®¾è®¡"ã€"æ·»åŠ åŠŸèƒ½"ã€"é›†æˆ"ã€"ç³»ç»Ÿ"ã€"åº”ç”¨" ç­‰å…³é”®è¯ï¼Œæˆ–ç”¨æˆ·æè¿°äº†å…·ä½“çš„äº§å“åŠŸèƒ½ã€‚
    *   **æ–‡æ¡£ç”Ÿæˆ**: é‡åˆ° "ç”Ÿæˆæ–‡æ¡£"ã€"è®¾è®¡æ–‡æ¡£"ã€"æŠ€æœ¯æ–‡æ¡£" ç­‰æ˜ç¡®æŒ‡ä»¤ã€‚
    *   **æ™®é€šå¯¹è¯**: å…¶ä»–æ‰€æœ‰æƒ…å†µã€‚

2.  **å†³ç­–ä¼˜å…ˆçº§**:
    *   **ç›´æ¥è§¦å‘**: å½“æ„å›¾ä¸ºâ€œé¡¹ç›®è§„åˆ’â€æˆ–â€œæ–‡æ¡£ç”Ÿæˆâ€æ—¶ï¼Œ**å¿…é¡»ç›´æ¥è¾“å‡ºç›¸åº”ACTION**ï¼Œç¦æ­¢ä½¿ç”¨ `[TEXT_START]` è¿›è¡Œå›å¤ã€‚
    *   **ä¿¡æ¯ä¸è¶³**: å¦‚æœç”¨æˆ·è¦æ±‚ç”Ÿæˆæ–‡æ¡£ (`LONG_DOC_ACTION`) ä½†å½“å‰ç¼ºå°‘é¡¹ç›®è§„åˆ’ï¼Œåº”ä¼˜å…ˆä½¿ç”¨ `[SHORT_PLAN_ACTION]` æ¥æ„å»ºè§„åˆ’ã€‚
    *   **ä¸»åŠ¨æ¨è**: å¯¹äºå¤æ‚çš„æ–°é¡¹ç›®ï¼Œå¯ä¸»åŠ¨åˆ¤æ–­å¹¶ä½¿ç”¨ `[FULL_FLOW_ACTION]`ã€‚

#### **4. ACTIONæ ‡ç­¾å†…å®¹è¦æ±‚**

*   **`[SHORT_PLAN_ACTION_START]` å’Œ `[FULL_FLOW_ACTION_START]`**:
    *   æ ‡ç­¾å†…å¿…é¡»åŒ…å«**å®Œæ•´ã€è¯¦ç»†çš„éœ€æ±‚æè¿°**ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€ç›®æ ‡å’Œæ‰€æœ‰åŠŸèƒ½ç‚¹ã€‚
    *   **âš ï¸ åŠŸèƒ½æ‰©å±•æ ¸å¿ƒåŸåˆ™**: å¦‚æœç”¨æˆ·æ˜¯åœ¨ç°æœ‰é¡¹ç›®åŸºç¡€ä¸Šå¢åŠ åŠŸèƒ½ï¼ˆå¦‚â€œæ·»åŠ ç™»å½•åŠŸèƒ½â€ï¼‰ï¼Œéœ€æ±‚æè¿°**å¿…é¡»**éµå¾ªä»¥ä¸‹ç»“æ„ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®è§„åˆ’ï¼Œè€Œéç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—ï¼š
        > "åŸºäºç°æœ‰çš„ **[åŸé¡¹ç›®æè¿°]**ï¼Œåœ¨ä¿æŒå…¶ **[åŸæ ¸å¿ƒåŠŸèƒ½åˆ—è¡¨]** çš„åŸºç¡€ä¸Šï¼Œæ–°å¢ **[æ–°åŠŸèƒ½å…·ä½“æè¿°]**ã€‚æ–°åŠŸèƒ½éœ€è¦ä¸åŸæœ‰åŠŸèƒ½ **[æè¿°é›†æˆæ–¹å¼]**ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªåŒ…å« **[æ‰€æœ‰æ–°æ—§åŠŸèƒ½åˆ—è¡¨]** çš„å®Œæ•´é¡¹ç›®ã€‚"

*   **`[LONG_DOC_ACTION_START]`**:
    *   æ ‡ç­¾å†…æ°¸è¿œä¿æŒä¸ºç©ºã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å·²æœ‰è§„åˆ’ã€‚

#### **5. åˆ¤æ–­ç¤ºä¾‹**
*   "å¼€å‘ä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿ" â†’ `[SHORT_PLAN_ACTION_START]...[SHORT_PLAN_ACTION_END]`
*   "åœ¨è´­ç‰©ç³»ç»Ÿé‡ŒåŠ ä¸Šä¼˜æƒ åˆ¸åŠŸèƒ½" â†’ `[SHORT_PLAN_ACTION_START]åˆ›å»ºä¸€ä¸ªåœ¨çº¿è´­ç‰©ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéœ€è¦æ”¯æŒä»¥ä¸‹å®Œæ•´åŠŸèƒ½ï¼š1. ç”¨æˆ·æ³¨å†Œä¸ç™»å½•ï¼›2. å•†å“æµè§ˆä¸æœç´¢ï¼›3. è´­ç‰©è½¦ç®¡ç†ï¼›4. ä¼˜æƒ åˆ¸ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·åœ¨ç»“ç®—æ—¶è¾“å…¥ä¼˜æƒ ç è¿›è¡ŒæŠµæ‰£ã€‚[SHORT_PLAN_ACTION_END]` ##æ•´ç†æ›´æ–°åçš„å®Œæ•´çš„ç”¨æˆ·éœ€æ±‚
*   "åŸºäºå½“å‰è§„åˆ’ç”Ÿæˆè¯¦ç»†æ–‡æ¡£" â†’ `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]`
*   "ä½ å¥½" â†’ `[TEXT_START]ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ[TEXT_END]`"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
ç”¨æˆ·å¯¹è¯å†å²ï¼š
{context_str}

å½“å‰è§„åˆ’å†…å®¹ï¼š
{current_plan if current_plan else "æš‚æ— è§„åˆ’å†…å®¹"}

ç”¨æˆ·å½“å‰æ¶ˆæ¯ï¼š{message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""


        else:
            # ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†ï¼ˆåŒ…å«æ‰€æœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œç‹¬ç«‹äºç”¨æˆ·å¯¹è¯å†å²ï¼‰
            system_prompt = """You are an AI assistant for GTPlanner. Your core task is to analyze user intent and respond by generating commands in a predefined format.

#### **1. Core Task**
Analyze the user's intent within the conversational context and choose one of the following four types to respond with:
*   **Project Planning**: The user proposes a new project, new features, or suggests improvements.
*   **Document Generation**: The user explicitly requests the generation of a design or technical document.
*   **Full Flow**: The user presents a complex project requirement suitable for generating both a plan and a document at once.
*   **General Conversation**: Casual greetings, thanks, or general questions.

#### **2. Output Format (Strictly Enforced)**
*   **General Conversation**: `[TEXT_START]Your reply content here[TEXT_END]`
*   **Project Planning**: `[SHORT_PLAN_ACTION_START]Detailed requirement description[SHORT_PLAN_ACTION_END]`
*   **Document Generation**: `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]` (The tag is empty)
*   **Full Flow**: `[FULL_FLOW_ACTION_START]Detailed requirement description[FULL_FLOW_ACTION_END]`

**Formatting Rules**:
1.  All tags use the `[TAG_START]` and `[TAG_END]` paired format.
2.  Do not add any extra characters or spaces before or after the tags.
3.  Ensure start and end tags are strictly paired.

#### **3. Workflow and Decision Logic**

1.  **Identify Intent**:
    *   **Project Planning**: Triggered by keywords like "develop," "create," "design," "add feature," "integrate," "system," "app," or when the user describes specific product functionality.
    *   **Document Generation**: Triggered by explicit commands like "generate document," "design doc," or "technical specification."
    *   **General Conversation**: All other cases.

2.  **Decision Priority**:
    *   **Direct Trigger**: If the intent is "Project Planning" or "Document Generation," you **must directly output the corresponding ACTION tag**. Do not reply with `[TEXT_START]`.
    *   **Insufficient Information**: If the user requests document generation (`LONG_DOC_ACTION`) but a project plan is missing from the context, prioritize using `[SHORT_PLAN_ACTION]` first to establish the plan.
    *   **Proactive Recommendation**: For complex new projects, you can proactively decide to use `[FULL_FLOW_ACTION]`.

#### **4. Content Requirements for ACTION Tags**

*   **For `[SHORT_PLAN_ACTION_START]` and `[FULL_FLOW_ACTION_START]`**:
    *   The content inside the tag must be a **complete and detailed requirement description**, including the project background, goals, and all functional points.
    *   **âš ï¸ Core Principle for Feature Extension**: If the user is adding a feature to an existing project (e.g., "add a login feature"), the requirement description **must** follow the template below. This ensures a complete project plan is generated, not just an isolated functional module.
        > "Based on the existing **[original project description]**, while maintaining its core features of **[list of original core features]**, add a new feature: **[description of the new feature]**. The new feature should integrate with the existing functions by **[describe the integration method]**, resulting in a complete project that includes **[list of all new and old features]**."

*   **For `[LONG_DOC_ACTION_START]`**:
    *   This tag's content should always be empty. The system will automatically use the project plan from the current context.

#### **5. Examples**
*   "Develop an online shopping system" â†’ `[SHORT_PLAN_ACTION_START]...[SHORT_PLAN_ACTION_END]`
*   "Add a coupon feature to the shopping system" â†’ `[SHORT_PLAN_ACTION_START]Create an online shopping system. The system must support the following complete features: 1. User registration and login; 2. Product browsing and search; 3. Shopping cart management; 4. A coupon system that allows users to apply discount codes at checkout.[SHORT_PLAN_ACTION_END]` ##Organize the updated complete user requirements.
*   "Generate a detailed document based on the current plan" â†’ `[LONG_DOC_ACTION_START][LONG_DOC_ACTION_END]`
*   "Hello" â†’ `[TEXT_START]Hello! How can I help you?[TEXT_END]`"""

            # ç”¨æˆ·å¯¹è¯ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆçº¯å‡€çš„ç”¨æˆ·æ•°æ®ï¼Œä¸åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼‰
            user_context = f"""
User conversation history:
{context_str}

Current plan content:
{current_plan if current_plan else "No current plan available"}

Current user message: {message}"""

            # ç»„åˆå®Œæ•´æç¤ºè¯
            prompt = f"""{system_prompt}

{user_context}"""

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾è¯†åˆ«å’Œå“åº”ç”Ÿæˆ
        yield stream_data("[STATUS_START]")
        yield stream_data("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
        yield stream_data_block("[STATUS_END]")

        # å¯¼å…¥æµå¼LLMè°ƒç”¨
        from utils.call_llm import call_llm_stream_async

        # æµå¼è¾“å‡ºLLMçš„å“åº”å¹¶ç›‘å¬ACTIONæ ‡ç­¾
        action_buffer = []
        in_action = False
        action_type = None
        content_buffer = ""
        pending_output = ""  # ç¼“å†²å¾…è¾“å‡ºçš„å†…å®¹

        async for chunk in call_llm_stream_async(prompt):
            if chunk:
                content_buffer += chunk
                pending_output += chunk

                # å¤„ç†ç¼“å†²åŒºä¸­çš„å®Œæ•´æ ‡ç­¾
                while True:
                    # æŸ¥æ‰¾æ ‡ç­¾å¼€å§‹
                    start_pos = pending_output.find('[')
                    if start_pos == -1:
                        # æ²¡æœ‰æ ‡ç­¾ï¼Œè¾“å‡ºæ‰€æœ‰å†…å®¹
                        if pending_output and not in_action:
                            yield stream_data(pending_output)
                        elif pending_output and in_action:
                            action_buffer.append(pending_output)
                        pending_output = ""
                        break

                    # è¾“å‡ºæ ‡ç­¾å‰çš„å†…å®¹
                    if start_pos > 0:
                        before_tag = pending_output[:start_pos]
                        if not in_action:
                            yield stream_data(before_tag)
                        else:
                            action_buffer.append(before_tag)

                    # æŸ¥æ‰¾æ ‡ç­¾ç»“æŸ
                    end_pos = pending_output.find(']', start_pos)
                    if end_pos == -1:
                        # æ ‡ç­¾ä¸å®Œæ•´ï¼Œä¿ç•™ä»æ ‡ç­¾å¼€å§‹çš„æ‰€æœ‰å†…å®¹
                        pending_output = pending_output[start_pos:]
                        break

                    # æå–å®Œæ•´æ ‡ç­¾
                    complete_tag = pending_output[start_pos:end_pos + 1]
                    remaining_content = pending_output[end_pos + 1:]

                    # å¤„ç†æ ‡ç­¾
                    if complete_tag == "[SHORT_PLAN_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "short_plan"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[LONG_DOC_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "long_doc"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    elif complete_tag == "[FULL_FLOW_ACTION_START]":
                        in_action = True
                        action_buffer = []
                        action_type = "full_flow"
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯

                    elif complete_tag in ["[SHORT_PLAN_ACTION_END]", "[LONG_DOC_ACTION_END]", "[FULL_FLOW_ACTION_END]"] and in_action:
                        # å¤„ç†ACTIONå†…å®¹
                        action_content = ''.join(action_buffer).strip()
                        if action_content:
                            # æ ¹æ®action_typeæ„é€ action_content
                            full_action_content = f"{action_type}:{action_content}"
                            async for planning_chunk in handle_action(full_action_content, message, current_plan, language, conversation_history):
                                yield planning_chunk
                        in_action = False
                        action_buffer = []
                        action_type = None
                        # ACTIONæ ‡ç­¾ä¸å‘é€åˆ°å‰ç«¯
                    else:
                        # éACTIONæ ‡ç­¾ï¼Œæ­£å¸¸å‘é€
                        if not in_action:
                            yield stream_data(complete_tag)
                        else:
                            action_buffer.append(complete_tag)

                    # ç»§ç»­å¤„ç†å‰©ä½™å†…å®¹
                    pending_output = remaining_content

        # å¤„ç†å¯èƒ½å‰©ä½™çš„å†…å®¹
        if pending_output:
            if not in_action:
                yield stream_data(pending_output)
            else:
                action_buffer.append(pending_output)

        yield stream_data_block("")  # ç©ºè¡Œè¡¨ç¤ºç»“æŸ

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_stream_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        yield stream_data("âŒ An internal error occurred while generating response. Please try again later.")
        yield stream_data_block("[ERROR_END]")


async def generate_direct_action_response(
    action: str,
    message: str,
    conversation_history: List[ChatMessage],
    session_id: Optional[str],
    language: str,
    context: Dict[str, Any]
):
    """
    æ ¹æ®æ˜ç¡®çš„actionç›´æ¥ç”Ÿæˆå“åº”ï¼Œè·³è¿‡AIæ„å›¾è¯†åˆ«
    """
    # è°ƒè¯•ä¿¡æ¯
    print(f"[DEBUG] generate_direct_action_response called with:")
    print(f"  action: {action}")
    print(f"  message: {message[:100] if message else 'None'}...")
    print(f"  language: {language}")
    print(f"  session_id: {session_id}")

    try:
        # å‘é€çŠ¶æ€ä¿¡æ¯ - æ ¹æ®è¯­è¨€æœ¬åœ°åŒ–
        yield stream_data("[STATUS_START]")
        if language == "zh":
            yield stream_data("ğŸ”„ æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
        else:
            yield stream_data("ğŸ”„ Processing your request...")
        yield stream_data_block("[STATUS_END]")

        # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_plan = context.get("current_plan", "")

        if action == "generate_document":
            # ç›´æ¥è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆ
            if not current_plan:
                yield stream_data("[ERROR_START]")
                if language == "zh":
                    yield stream_data("âŒ æœªæ‰¾åˆ°å½“å‰è§„åˆ’å†…å®¹ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£ã€‚")
                else:
                    yield stream_data("âŒ No current plan found. Cannot generate document.")
                yield stream_data_block("[ERROR_END]")
                return

            yield stream_data("[STATUS_START]")
            if language == "zh":
                yield stream_data("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯¦ç»†è®¾è®¡æ–‡æ¡£...")
            else:
                yield stream_data("ğŸ“„ Generating detailed design document...")
            yield stream_data_block("[STATUS_END]")

            # è°ƒç”¨é•¿æ–‡æ¡£ç”Ÿæˆæµå¼æ¥å£
            from api.v1.planning import long_planning_stream, LongPlanningRequest

            doc_request = LongPlanningRequest(
                requirement=message,
                previous_flow=current_plan,
                language=language
            )

            async for chunk in long_planning_stream(doc_request):
                yield chunk



        else:
            yield stream_data("[ERROR_START]")
            if language == "zh":
                yield stream_data(f"âŒ ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}")
            else:
                yield stream_data(f"âŒ Unsupported action type: {action}")
            yield stream_data_block("[ERROR_END]")

    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import logging
        logging.error(f"Error in generate_direct_action_response: {str(e)}", exc_info=True)

        # å‘ç”¨æˆ·è¿”å›é€šç”¨é”™è¯¯ä¿¡æ¯ï¼Œä¸æš´éœ²å†…éƒ¨ç»†èŠ‚
        yield stream_data("[ERROR_START]")
        if language == "zh":
            yield stream_data("âŒ å¤„ç†æ“ä½œæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚")
        else:
            yield stream_data("âŒ An internal error occurred while processing the operation. Please try again later.")
        yield stream_data_block("[ERROR_END]")


@chat_router.post("/unified")
async def unified_conversation(body: ConversationRequest):
    """
    ç»Ÿä¸€å¯¹è¯æ¥å£ï¼šé›†æˆæ„å›¾è¯†åˆ«ã€å¯¹è¯å›å¤ã€è§„åˆ’ç”Ÿæˆå’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½
    å®Œå…¨ä½¿ç”¨æµå¼å“åº”
    """
    message = body.message
    conversation_history = body.conversation_history or []
    session_id = body.session_id
    language = body.language
    action = body.action
    context = body.context or {}

    # è°ƒè¯•ä¿¡æ¯
    print(f"[DEBUG] unified_conversation called with:")
    print(f"  message: {message[:100] if message else 'None'}...")
    print(f"  language: {language}")
    print(f"  action: {action}")
    print(f"  session_id: {session_id}")

    if not message:
        async def error_stream():
            yield stream_data("[ERROR_START]")
            yield stream_data("âŒ Missing message in request body.")
            yield stream_data_block("[ERROR_END]")
        return StreamingResponse(error_stream(), media_type="text/plain")

    # ç¡®å®šè¯­è¨€ - ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ é€’çš„ç•Œé¢è¯­è¨€ï¼Œç¡®ä¿ç•Œé¢è¯­è¨€å’ŒLLMè¾“å‡ºè¯­è¨€çš„ä¸€è‡´æ€§
    language = determine_language(message, None, language)

    # æ ¹æ®actionå­—æ®µç›´æ¥è°ƒç”¨ç›¸åº”åŠŸèƒ½ï¼Œæˆ–ä½¿ç”¨AIæ„å›¾è¯†åˆ«
    if action:
        return StreamingResponse(
            generate_direct_action_response(action, message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )
    else:
        # è¿”å›æµå¼å“åº”ï¼ˆä½¿ç”¨AIæ„å›¾è¯†åˆ«ï¼‰
        return StreamingResponse(
            generate_stream_response(message, conversation_history, session_id, language, context),
            media_type="text/plain; charset=utf-8",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/plain; charset=utf-8"
            }
        )


