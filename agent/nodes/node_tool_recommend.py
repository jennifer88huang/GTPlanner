"""
å·¥å…·æ¨èèŠ‚ç‚¹ (Node_Tool_Recommend)

åŸºäºæŸ¥è¯¢æ–‡æœ¬ä»å‘é‡æ•°æ®åº“æ£€ç´¢æœ€ç›¸å…³çš„å·¥å…·ï¼Œå¹¶è¿”å›æ¨èåˆ—è¡¨ã€‚
åŸºäºæ¶æ„æ–‡æ¡£ä¸­å®šä¹‰çš„è¾“å…¥è¾“å‡ºè§„æ ¼å®ç°ã€‚

åŠŸèƒ½æè¿°ï¼š
- æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
- è°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢
- è¿”å›æœ€ç›¸å…³çš„å·¥å…·åˆ—è¡¨
- æ”¯æŒç»“æœè¿‡æ»¤å’Œæ’åº
- å¯é€‰çš„å¤§æ¨¡å‹é‡æ’åº
"""

import time
import requests
import asyncio
import json
from typing import Dict, List, Any, Optional
from pocketflow import Node
from utils.call_llm import call_llm_async
from utils.config_manager import get_vector_service_config


class NodeToolRecommend(Node):
    """å·¥å…·æ¨èèŠ‚ç‚¹"""
    
    def __init__(self, max_retries: int = 3, wait: float = 2.0):
        """
        åˆå§‹åŒ–å·¥å…·æ¨èèŠ‚ç‚¹

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait: é‡è¯•ç­‰å¾…æ—¶é—´
        """
        super().__init__(max_retries=max_retries, wait=wait)

        # ä»é…ç½®æ–‡ä»¶åŠ è½½å‘é‡æœåŠ¡é…ç½®
        vector_config = get_vector_service_config()
        self.vector_service_url = vector_config.get("base_url", "http://nodeport.sensedeal.vip:32421")
        self.timeout = vector_config.get("timeout", 30)

        # è¿™äº›å‚æ•°ä¿æŒç¡¬ç¼–ç ï¼Œä¸ä»é…ç½®æ–‡ä»¶è¯»å–
        self.index_name = "default"  # ä½¿ç”¨é»˜è®¤ç´¢å¼•å
        self.vector_field = "combined_text"

        # æ¨èé…ç½®
        self.default_top_k = 5
        self.min_score_threshold = 0.1  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        self.use_llm_filter = True  # æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰
        self.llm_candidate_count = 10  # ä¼ ç»™å¤§æ¨¡å‹çš„å€™é€‰å·¥å…·æ•°é‡

        # æ£€æŸ¥å‘é‡æœåŠ¡å¯ç”¨æ€§
        try:
            response = requests.get(f"{self.vector_service_url}/health", timeout=5)
            self.vector_service_available = response.status_code == 200
        except Exception:
            self.vector_service_available = False
            print("âš ï¸ å‘é‡æœåŠ¡ä¸å¯ç”¨")

    def prep(self, shared) -> Dict[str, Any]:
        """
        å‡†å¤‡é˜¶æ®µï¼šä»å…±äº«å˜é‡è·å–æŸ¥è¯¢å‚æ•°

        Args:
            shared: pocketflowå­—å…¸å…±äº«å˜é‡

        Returns:
            å‡†å¤‡ç»“æœå­—å…¸
        """
        try:
            # ä»å…±äº«å˜é‡è·å–æŸ¥è¯¢å‚æ•°
            query = shared.get("query", "")
            top_k = shared.get("top_k", self.default_top_k)
            index_name = shared.get("index_name", self.index_name)
            tool_types = shared.get("tool_types", [])  # å¯é€‰çš„å·¥å…·ç±»å‹è¿‡æ»¤
            min_score = shared.get("min_score", self.min_score_threshold)
            use_llm_filter = shared.get("use_llm_filter", self.use_llm_filter)  # æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰

            # å¦‚æœæ²¡æœ‰æä¾›æŸ¥è¯¢ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæå–
            if not query:
                query = self._extract_query_from_shared_state(shared)

            # éªŒè¯è¾“å…¥
            if not query or not query.strip():
                return {
                    "error": "No query provided for tool recommendation",
                    "query": "",
                    "top_k": top_k,
                    "index_name": index_name
                }

            # é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
            processed_query = self._preprocess_query(query.strip())

            return {
                "query": processed_query,
                "original_query": query,
                "top_k": top_k,
                "index_name": index_name,
                "tool_types": tool_types,
                "min_score": min_score,
                "use_llm_filter": use_llm_filter
            }

        except Exception as e:
            return {
                "error": f"Tool recommendation preparation failed: {str(e)}",
                "query": "",
                "top_k": self.default_top_k,
                "index_name": self.index_name
            }

    def exec(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œé˜¶æ®µï¼šè°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œå·¥å…·æ£€ç´¢

        Args:
            prep_res: å‡†å¤‡é˜¶æ®µçš„ç»“æœ

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        if "error" in prep_res:
            raise ValueError(prep_res["error"])

        query = prep_res["query"]
        top_k = prep_res["top_k"]
        index_name = prep_res["index_name"]
        tool_types = prep_res["tool_types"]
        min_score = prep_res["min_score"]
        use_llm_filter = prep_res["use_llm_filter"]

        if not query:
            raise ValueError("Empty query for tool recommendation")

        if not self.vector_service_available:
            raise RuntimeError("Vector service is not available")

        try:
            start_time = time.time()

            # è°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œæ£€ç´¢ï¼ˆè·å–æ›´å¤šå€™é€‰ï¼‰
            search_top_k = max(top_k, self.llm_candidate_count) if use_llm_filter else top_k
            search_results = self._search_tools(query, index_name, search_top_k)

            # è¿‡æ»¤å’Œå¤„ç†ç»“æœ
            filtered_results = self._filter_results(
                search_results,
                tool_types=tool_types,
                min_score=min_score
            )

            # åå¤„ç†ç»“æœ
            processed_results = self._process_results(filtered_results)

            # ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_llm_filter and len(processed_results) > 1:
                try:
                    llm_selected_results = self._llm_filter_tools_sync(query, processed_results, top_k)
                    processed_results = llm_selected_results
                    print(f"âœ… å¤§æ¨¡å‹ç­›é€‰å®Œæˆï¼Œè¿”å› {len(processed_results)} ä¸ªå·¥å…·")
                except Exception as e:
                    print(f"âš ï¸ å¤§æ¨¡å‹ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {str(e)}")
                    processed_results = processed_results[:top_k]
            else:
                processed_results = processed_results[:top_k]

            search_time = time.time() - start_time

            return {
                "recommended_tools": processed_results,
                "total_found": len(processed_results),
                "search_time": round(search_time * 1000),  # è½¬æ¢ä¸ºæ¯«ç§’
                "query_used": query,
                "original_query": prep_res["original_query"],
                "search_metadata": {
                    "index_name": index_name,
                    "top_k": top_k,
                    "min_score": min_score,
                    "tool_types_filter": tool_types
                }
            }

        except Exception as e:
            raise RuntimeError(f"Tool recommendation execution failed: {str(e)}")

    def post(self, shared, prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """
        åå¤„ç†é˜¶æ®µï¼šå°†æ¨èç»“æœå­˜å‚¨åˆ°å…±äº«çŠ¶æ€

        Args:
            shared: å…±äº«çŠ¶æ€å¯¹è±¡
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exec_res: æ‰§è¡Œé˜¶æ®µç»“æœ

        Returns:
            ä¸‹ä¸€æ­¥åŠ¨ä½œ
        """
        try:
            if "error" in exec_res:
                if hasattr(shared, 'record_error'):
                    shared.record_error(Exception(exec_res["error"]), "NodeToolRecommend.exec")
                return "error"

            recommended_tools = exec_res["recommended_tools"]
            total_found = exec_res["total_found"]

            # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
            if isinstance(shared, dict):
                # å­æµç¨‹æ¨¡å¼ï¼šä¿å­˜æ¨èç»“æœåˆ°å…±äº«å˜é‡
                shared["recommended_tools"] = recommended_tools
                shared["tool_recommendation_result"] = {
                    "tools": recommended_tools,
                    "total_found": total_found,
                    "query": exec_res["query_used"],
                    "search_time": exec_res["search_time"]
                }
                return "success"

            # ä¸»æµç¨‹æ¨¡å¼ï¼šä¿å­˜åˆ°ç ”ç©¶å‘ç°æˆ–ç›¸åº”çš„çŠ¶æ€
            if not hasattr(shared, 'tool_recommendations'):
                shared.tool_recommendations = []

            # è½¬æ¢æ¨èç»“æœä¸ºæ ‡å‡†æ ¼å¼
            for tool in recommended_tools:
                recommendation = {
                    "tool_id": tool["id"],
                    "tool_type": tool["type"],
                    "tool_name": tool.get("summary", ""),
                    "description": tool.get("description", ""),
                    "relevance_score": tool.get("score", 0.0),
                    "examples": tool.get("examples", ""),
                    "metadata": {
                        "category": tool.get("category", ""),
                        "file_path": tool.get("file_path", ""),
                        "requirement": tool.get("requirement", ""),
                        "base_url": tool.get("base_url", ""),
                        "endpoints": tool.get("endpoints", "")
                    },
                    "search_metadata": {
                        "query": exec_res["query_used"],
                        "search_time": exec_res["search_time"]
                    }
                }
                shared.tool_recommendations.append(recommendation)

            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯è®°å½•æ¨èç»“æœ
            shared.add_system_message(
                f"å·¥å…·æ¨èå®Œæˆï¼Œæ‰¾åˆ° {total_found} ä¸ªç›¸å…³å·¥å…·",
                agent_source="NodeToolRecommend",
                query=exec_res["query_used"],
                tools_count=total_found,
                search_time_ms=exec_res["search_time"]
            )

            return "success"

        except Exception as e:
            if hasattr(shared, 'record_error'):
                shared.record_error(e, "NodeToolRecommend.post")
            return "error"

    def exec_fallback(self, prep_res: Dict[str, Any], exc: Exception) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤±è´¥æ—¶çš„é™çº§å¤„ç†

        Args:
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exc: å¼‚å¸¸å¯¹è±¡

        Returns:
            é”™è¯¯ä¿¡æ¯
        """
        return {
            "error": f"Tool recommendation execution failed: {str(exc)}",
            "recommended_tools": [],
            "total_found": 0,
            "query_used": prep_res.get("query", ""),
            "original_query": prep_res.get("original_query", "")
        }

    def _extract_query_from_shared_state(self, shared) -> str:
        """ä»å…±äº«çŠ¶æ€ä¸­æå–æŸ¥è¯¢æ–‡æœ¬"""
        query_candidates = []

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
        if isinstance(shared, dict):
            # å­æµç¨‹æ¨¡å¼ï¼šç›´æ¥ä»å­—å…¸ä¸­è·å–
            if "user_query" in shared:
                query_candidates.append(shared["user_query"])
            if "search_query" in shared:
                query_candidates.append(shared["search_query"])
            if "task_description" in shared:
                query_candidates.append(shared["task_description"])
        else:
            # ä¸»æµç¨‹æ¨¡å¼ï¼šä»å…±äº«çŠ¶æ€å¯¹è±¡ä¸­æå–
            if hasattr(shared, 'user_intent') and hasattr(shared.user_intent, 'original_query'):
                query_candidates.append(shared.user_intent.original_query)

            if hasattr(shared, 'structured_requirements'):
                if hasattr(shared.structured_requirements, 'project_overview'):
                    if hasattr(shared.structured_requirements.project_overview, 'title'):
                        query_candidates.append(shared.structured_requirements.project_overview.title)
                    if hasattr(shared.structured_requirements.project_overview, 'description'):
                        query_candidates.append(shared.structured_requirements.project_overview.description)

        # è¿”å›ç¬¬ä¸€ä¸ªéç©ºçš„æŸ¥è¯¢
        for candidate in query_candidates:
            if candidate and candidate.strip():
                return candidate.strip()

        return ""

    def _preprocess_query(self, query: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬"""
        # åŸºç¡€æ¸…ç†
        processed = query.strip()

        # å¯ä»¥æ·»åŠ æ›´å¤šé¢„å¤„ç†é€»è¾‘ï¼Œå¦‚ï¼š
        # - å…³é”®è¯æå–
        # - åŒä¹‰è¯æ‰©å±•
        # - åœç”¨è¯è¿‡æ»¤

        return processed

    def _search_tools(self, query: str, index_name: str, top_k: int) -> Dict[str, Any]:
        """è°ƒç”¨å‘é‡æœåŠ¡è¿›è¡Œå·¥å…·æ£€ç´¢"""
        try:
            # æ„å»ºæœç´¢è¯·æ±‚
            search_request = {
                "query": query,
                "vector_field": self.vector_field,
                "index": index_name,
                "top_k": top_k
            }

            # è°ƒç”¨å‘é‡æœåŠ¡
            response = requests.post(
                f"{self.vector_service_url}/search",
                json=search_request,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )

            if response.status_code == 200:
                result = response.json()
                print(f"âœ… æ£€ç´¢åˆ° {result.get('total', 0)} ä¸ªç›¸å…³å·¥å…·")
                return result
            else:
                error_msg = f"å‘é‡æœåŠ¡è¿”å›é”™è¯¯: {response.status_code}, {response.text}"
                print(f"âŒ {error_msg}")
                raise RuntimeError(error_msg)

        except requests.exceptions.RequestException as e:
            error_msg = f"è°ƒç”¨å‘é‡æœåŠ¡å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            raise RuntimeError(error_msg)

    def _filter_results(self, search_results: Dict[str, Any],
                       tool_types: List[str] = None,
                       min_score: float = 0.0) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æœç´¢ç»“æœ"""
        if "results" not in search_results:
            return []

        filtered = []
        for result in search_results["results"]:
            # æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
            score = result.get("score", 0.0)
            if score < min_score:
                continue

            # æ£€æŸ¥å·¥å…·ç±»å‹è¿‡æ»¤
            document = result.get("document", {})
            if tool_types and document.get("type") not in tool_types:
                continue

            # æ·»åŠ åˆ†æ•°åˆ°æ–‡æ¡£ä¸­
            document["score"] = score
            filtered.append(document)

        return filtered

    def _process_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åå¤„ç†æœç´¢ç»“æœ"""
        processed = []

        for result in results:
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            processed_result = {
                "id": result.get("id", ""),
                "type": result.get("type", ""),
                "summary": result.get("summary", ""),
                "description": result.get("description", ""),
                "examples": result.get("examples", ""),
                "score": result.get("score", 0.0),
                "category": result.get("category", ""),
                "file_path": result.get("file_path", ""),
                "created_at": result.get("created_at", ""),
                "updated_at": result.get("updated_at", "")
            }

            # æ·»åŠ ç±»å‹ç‰¹å®šå­—æ®µ
            if result.get("type") == "PYTHON_PACKAGE":
                processed_result["requirement"] = result.get("requirement", "")
            elif result.get("type") == "APIS":
                processed_result["base_url"] = result.get("base_url", "")
                processed_result["endpoints"] = result.get("endpoints", "")

            processed.append(processed_result)

        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
        processed.sort(key=lambda x: x["score"], reverse=True)

        return processed

    def _llm_filter_tools_sync(self, query: str, tools: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰å·¥å…·ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨asyncio.runæ¥è¿è¡Œå¼‚æ­¥å‡½æ•°
            return asyncio.run(self._llm_filter_tools(query, tools, top_k))
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹ç­›é€‰å¤±è´¥: {str(e)}")
            return tools[:top_k]

    async def _llm_filter_tools(self, query: str, tools: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤§æ¨¡å‹ç­›é€‰æœ€åˆé€‚çš„å·¥å…·"""
        if not tools:
            return []

        # æ„å»ºæç¤ºè¯
        prompt = self._build_filter_prompt(query, tools, top_k)

        try:
            # è°ƒç”¨å¤§æ¨¡å‹
            response = await call_llm_async(prompt, is_json=True)

            # è§£æå¤§æ¨¡å‹å“åº”
            selected_tools = self._parse_llm_filter_response(response, tools)

            return selected_tools

        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
            return tools[:top_k]

    def _build_filter_prompt(self, query: str, tools: List[Dict[str, Any]], top_k: int) -> str:
        """æ„å»ºå¤§æ¨¡å‹ç­›é€‰çš„æç¤ºè¯"""
        tools_info = []
        for i, tool in enumerate(tools):
            tool_info = {
                "index": i,
                "id": tool["id"],
                "type": tool["type"],
                "summary": tool["summary"],
                "description": tool["description"][:500] + "..." if len(tool["description"]) > 500 else tool["description"]
            }
            tools_info.append(tool_info)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·æ¨èä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œæˆ‘å·²ç»é€šè¿‡å‘é‡æ£€ç´¢æ‰¾åˆ°äº†ä¸€äº›å€™é€‰å·¥å…·ã€‚è¯·ä½ æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼Œä»è¿™äº›å€™é€‰å·¥å…·ä¸­**ç­›é€‰å‡º**æœ€åˆé€‚çš„å‰{top_k}ä¸ªå·¥å…·ã€‚

**é‡è¦è¯´æ˜ï¼šä½ çš„ä»»åŠ¡æ˜¯ç­›é€‰å†³ç­–ï¼Œä¸æ˜¯æ’åºã€‚åªè¿”å›ä½ è®¤ä¸ºçœŸæ­£é€‚åˆç”¨æˆ·éœ€æ±‚çš„å·¥å…·ï¼Œå¦‚æœå€™é€‰å·¥å…·éƒ½ä¸åˆé€‚ï¼Œå¯ä»¥è¿”å›ç©ºåˆ—è¡¨ã€‚**

ç”¨æˆ·æŸ¥è¯¢: {query}

å€™é€‰å·¥å…·åˆ—è¡¨:
{json.dumps(tools_info, ensure_ascii=False, indent=2)}

è¯·ä»”ç»†åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ï¼Œè€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
1. å·¥å…·åŠŸèƒ½ä¸æŸ¥è¯¢éœ€æ±‚çš„**ç›´æ¥åŒ¹é…åº¦**
2. å·¥å…·ç±»å‹æ˜¯å¦**çœŸæ­£é€‚åˆ**è§£å†³ç”¨æˆ·é—®é¢˜
3. å·¥å…·çš„å®ç”¨æ€§å’Œå¯æ“ä½œæ€§
4. å·¥å…·æè¿°ä¸­æ˜¯å¦åŒ…å«ç”¨æˆ·éœ€è¦çš„**æ ¸å¿ƒåŠŸèƒ½**

ç­›é€‰æ ‡å‡†ï¼š
- åªé€‰æ‹©ä¸ç”¨æˆ·æŸ¥è¯¢**é«˜åº¦ç›¸å…³**çš„å·¥å…·
- ä¼˜å…ˆé€‰æ‹©åŠŸèƒ½**ç›´æ¥åŒ¹é…**çš„å·¥å…·
- å¦‚æœæŸä¸ªå·¥å…·ä¸æŸ¥è¯¢éœ€æ±‚ä¸åŒ¹é…ï¼Œ**ä¸è¦é€‰æ‹©å®ƒ**
- æœ€å¤šè¿”å›{top_k}ä¸ªå·¥å…·ï¼Œä½†å¦‚æœåˆé€‚çš„å·¥å…·å°‘äº{top_k}ä¸ªï¼Œåªè¿”å›åˆé€‚çš„

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼š

{{
    "selected_tools": [
        {{
            "index": å·¥å…·åœ¨åŸåˆ—è¡¨ä¸­çš„ç´¢å¼•,
            "reason": "é€‰æ‹©è¿™ä¸ªå·¥å…·çš„å…·ä½“ç†ç”±ï¼Œè¯´æ˜å®ƒå¦‚ä½•æ»¡è¶³ç”¨æˆ·éœ€æ±‚"
        }}
    ],
    "analysis": "æ•´ä½“åˆ†æè¯´æ˜ï¼Œè§£é‡Šç­›é€‰é€»è¾‘"
}}

æ³¨æ„ï¼š
- åªè¿”å›çœŸæ­£åˆé€‚çš„å·¥å…·ï¼Œä¸è¦ä¸ºäº†å‡‘æ•°è€Œé€‰æ‹©ä¸ç›¸å…³çš„å·¥å…·
- ç´¢å¼•å¿…é¡»æ˜¯æœ‰æ•ˆçš„ï¼ˆ0åˆ°{len(tools)-1}ï¼‰
- æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åº
- å¦‚æœæ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œselected_toolså¯ä»¥ä¸ºç©ºæ•°ç»„"""

        return prompt

    def _parse_llm_filter_response(self, response: Dict[str, Any], original_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è§£æå¤§æ¨¡å‹ç­›é€‰å“åº”"""
        try:
            if "selected_tools" not in response:
                print("âš ï¸ å¤§æ¨¡å‹å“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘selected_toolså­—æ®µ")
                return original_tools[:3]  # è¿”å›å‰3ä¸ªä½œä¸ºé»˜è®¤

            selected_tools = response["selected_tools"]
            if not isinstance(selected_tools, list):
                print("âš ï¸ å¤§æ¨¡å‹å“åº”æ ¼å¼é”™è¯¯ï¼šselected_toolsä¸æ˜¯åˆ—è¡¨")
                return original_tools[:3]

            # å¦‚æœå¤§æ¨¡å‹æ²¡æœ‰é€‰æ‹©ä»»ä½•å·¥å…·ï¼Œè¿”å›ç©ºåˆ—è¡¨æˆ–å‰å‡ ä¸ª
            if not selected_tools:
                print("âš ï¸ å¤§æ¨¡å‹æ²¡æœ‰é€‰æ‹©ä»»ä½•å·¥å…·")
                return original_tools[:1]  # è‡³å°‘è¿”å›æœ€ç›¸å…³çš„ä¸€ä¸ª

            filtered_tools = []
            used_indices = set()

            for item in selected_tools:
                if not isinstance(item, dict) or "index" not in item:
                    continue

                index = item["index"]
                if not isinstance(index, int) or index < 0 or index >= len(original_tools):
                    continue

                if index in used_indices:
                    continue

                used_indices.add(index)
                tool = original_tools[index].copy()
                tool["llm_reason"] = item.get("reason", "")
                tool["llm_selected"] = True  # æ ‡è®°ä¸ºå¤§æ¨¡å‹é€‰ä¸­
                filtered_tools.append(tool)

            # æ³¨æ„ï¼šè¿™é‡Œä¸è¡¥å……æœªé€‰ä¸­çš„å·¥å…·ï¼Œåªè¿”å›å¤§æ¨¡å‹ç­›é€‰å‡ºçš„å·¥å…·
            print(f"âœ… å¤§æ¨¡å‹ç­›é€‰è§£ææˆåŠŸï¼Œç­›é€‰å‡º {len(filtered_tools)} ä¸ªå·¥å…·")
            if "analysis" in response:
                print(f"ğŸ“ å¤§æ¨¡å‹åˆ†æ: {response['analysis']}")

            return filtered_tools

        except Exception as e:
            print(f"âŒ è§£æå¤§æ¨¡å‹å“åº”å¤±è´¥: {str(e)}")
            return original_tools[:3]  # è¿”å›å‰3ä¸ªä½œä¸ºé»˜è®¤
        
if __name__ == '__main__':
    from utils.config_manager import get_all_config
    from node_tool_index import NodeToolIndex
    init_node = NodeToolIndex()
    recommend_node = NodeToolRecommend()
    shared_with_init = {
        "tools_dir":"/home/tang/pyprojects/OpenSQZ-GTPlanner/GTPlanner/tools",
        "index_name":"",
        "force_reindex":True
    }
    prep_init_result = init_node.prep(shared_with_init)
    exec_init_result = init_node.exec(prep_init_result)
    print(exec_init_result)
    time.sleep(1) #ç°åœ¨éœ€è¦ä¼‘çœ 1ç§’ï¼Œç­‰å¾…ç´¢å¼•åˆ·æ–°ã€‚ä¹‹åä¼šä¿®å¤è¿™ä¸ªé—®é¢˜
    shared_with_llm = {
        "query": "æˆ‘æƒ³è§£æè§†é¢‘å­—å¹•",
        "top_k": 10,
        "index_name": exec_init_result.get("index_name", "default"),  # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„ç´¢å¼•
        "use_llm_filter": True  
    }
    prep_result = recommend_node.prep(shared=shared_with_llm)
    exec_result = recommend_node.exec(prep_result)
    print("---------")
    print(exec_result)

