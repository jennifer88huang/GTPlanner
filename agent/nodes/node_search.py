"""
æœç´¢å¼•æ“èŠ‚ç‚¹ (Node_Search)

åŸºäºå…³é”®è¯è¿›è¡Œç½‘ç»œæœç´¢ï¼Œè¿”å›ç›¸å…³ç»“æœã€‚
åŸºäºæ¶æ„æ–‡æ¡£ä¸­å®šä¹‰çš„è¾“å…¥è¾“å‡ºè§„æ ¼å®ç°ã€‚

åŠŸèƒ½æè¿°ï¼š
- å…³é”®è¯ä¼˜åŒ–å’Œç»„åˆ
- å¤šæœç´¢å¼•æ“APIè°ƒç”¨
- ç»“æœå»é‡å’Œæ’åº
- ç›¸å…³æ€§è¯„åˆ†è®¡ç®—
- ç»“æœæ ¼å¼æ ‡å‡†åŒ–
"""

import time
import hashlib
from typing import Dict, List, Any, Optional
from pocketflow import AsyncNode
from ..utils.search import JinaSearchClient


class NodeSearch(AsyncNode):
    """æœç´¢å¼•æ“èŠ‚ç‚¹"""
    
    def __init__(self, max_retries: int = 3, wait: float = 2.0):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“èŠ‚ç‚¹
        
        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            wait: é‡è¯•ç­‰å¾…æ—¶é—´
        """
        super().__init__(max_retries=max_retries, wait=wait)
        self.name = "NodeSearch"
        
        # åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯
        try:
            self.search_client = JinaSearchClient()
            self.search_available = True
        except ValueError:
            self.search_client = None
            self.search_available = False
            print("âš ï¸ æœç´¢APIæœªé…ç½®")

        # æœç´¢é…ç½®
        self.default_max_results = 10
        self.default_language = "zh-CN"
        self.timeout = 30

        # ç›¸å…³æ€§è¯„åˆ†æƒé‡
        self.title_weight = 0.4
        self.snippet_weight = 0.3
        self.url_weight = 0.2
        self.source_weight = 0.1
    
    async def prep_async(self, shared) -> Dict[str, Any]:
        """
        å‡†å¤‡é˜¶æ®µï¼šä»pocketflowå­—å…¸å…±äº«å˜é‡è·å–æœç´¢å…³é”®è¯

        Args:
            shared: pocketflowå­—å…¸å…±äº«å˜é‡

        Returns:
            å‡†å¤‡ç»“æœå­—å…¸
        """
        try:
            # ğŸ”§ æ”¯æŒå•ä¸ªå…³é”®è¯è¾“å…¥
            current_keyword = shared.get("current_keyword")
            search_keywords = shared.get("search_keywords", [])
            search_type = shared.get("search_type", "web")
            max_results = shared.get("max_results", self.default_max_results)
            language = shared.get("language", self.default_language)

            # ä¼˜å…ˆä½¿ç”¨å•ä¸ªå…³é”®è¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…³é”®è¯åˆ—è¡¨
            if current_keyword:
                search_keywords = [current_keyword]
            elif not search_keywords:
                search_keywords = self._extract_keywords_from_shared_state(shared)

            # éªŒè¯è¾“å…¥
            if not search_keywords:
                return self._create_error_result("No search keywords provided", search_type)
            
            # ä¼˜åŒ–å…³é”®è¯
            optimized_keywords = self._optimize_keywords(search_keywords)
            
            return {
                "search_keywords": optimized_keywords,
                "search_type": search_type,
                "max_results": max_results,
                "language": language,
                "original_keywords": search_keywords,
                "keyword_count": len(optimized_keywords)
            }
            
        except Exception as e:
            return self._create_error_result(f"Search preparation failed: {str(e)}")
    
    async def exec_async(self, prep_res: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œé˜¶æ®µï¼šæ‰§è¡Œæœç´¢æ“ä½œ
        
        Args:
            prep_res: å‡†å¤‡é˜¶æ®µçš„ç»“æœ
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        if "error" in prep_res:
            raise ValueError(prep_res["error"])
        
        search_keywords = prep_res["search_keywords"]
        search_type = prep_res["search_type"]
        max_results = prep_res["max_results"]
        language = prep_res["language"]
        
        if not search_keywords:
            raise ValueError("Empty search keywords")
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œæœç´¢
            all_results = []
            
            for keyword in search_keywords:
                try:
                    if self.search_available and self.search_client:
                        # ä½¿ç”¨çœŸå®æœç´¢API - å¼‚æ­¥è°ƒç”¨
                        results = await self.search_client.search_simple(keyword, count=max_results)

                        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                        formatted_results = []
                        for i, result in enumerate(results):
                            formatted_result = {
                                "title": result.get("title", ""),
                                "url": result.get("url", ""),
                                "snippet": result.get("description", ""),
                                "search_keyword": keyword,
                                "rank": i + 1,
                                "source_type": self._classify_source_type(result.get("url", "")),
                                "content": result.get("content", "")
                            }
                            formatted_result["relevance_score"] = self._calculate_relevance_score(
                                formatted_result, keyword
                            )
                            formatted_results.append(formatted_result)

                        all_results.extend(formatted_results)
                    else:
                        # æœç´¢APIä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤å…³é”®è¯
                        print(f"âš ï¸ æœç´¢APIä¸å¯ç”¨ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                        continue

                    # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    time.sleep(0.5)

                except Exception as e:
                    # å•ä¸ªå…³é”®è¯æœç´¢å¤±è´¥ä¸å½±å“å…¶ä»–å…³é”®è¯
                    print(f"âŒ æœç´¢å¤±è´¥ï¼Œå…³é”®è¯ '{keyword}': {str(e)}")
                    continue
            
            # å»é‡å’Œæ’åº
            deduplicated_results = self._deduplicate_results(all_results)
            sorted_results = self._sort_results(deduplicated_results)
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = sorted_results[:max_results]
            
            search_time = time.time() - start_time
            
            return {
                "search_results": final_results,
                "total_found": len(final_results),
                "search_time": round(search_time * 1000),  # è½¬æ¢ä¸ºæ¯«ç§’
                "keywords_processed": len(search_keywords),
                "deduplication_stats": {
                    "original_count": len(all_results),
                    "deduplicated_count": len(deduplicated_results),
                    "final_count": len(final_results)
                }
            }
            
        except Exception as e:
            raise RuntimeError(f"Search execution failed: {str(e)}")
    
    async def post_async(self, shared, prep_res: Dict[str, Any], exec_res: Dict[str, Any]) -> str:
        """
        åå¤„ç†é˜¶æ®µï¼šå°†æœç´¢ç»“æœå­˜å‚¨åˆ°å…±äº«çŠ¶æ€
        
        Args:
            shared: å…±äº«çŠ¶æ€å¯¹è±¡
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exec_res: æ‰§è¡Œé˜¶æ®µç»“æœ
            
        Returns:
            ä¸‹ä¸€æ­¥åŠ¨ä½œ
        """
        try:
            if "error" in exec_res:
                if hasattr(shared, 'record_error'):
                    shared.record_error(Exception(exec_res["error"]), "NodeSearch.exec")
                return "error"

            search_results = exec_res["search_results"]

            # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
            if isinstance(shared, dict):
                # å­æµç¨‹æ¨¡å¼ï¼šä¿å­˜é¦–æ¡æœç´¢ç»“æœåˆ°å…±äº«å˜é‡
                if search_results:
                    shared["first_search_result"] = search_results[0]
                    shared["all_search_results"] = search_results
                return "search_complete"

            # ä¸»æµç¨‹æ¨¡å¼ï¼šä¿å­˜åˆ°ç ”ç©¶å‘ç°
            if not hasattr(shared.research_findings, 'search_results'):
                shared.research_findings.search_results = []

            # è½¬æ¢æœç´¢ç»“æœä¸ºç ”ç©¶æºæ ¼å¼
            for result in search_results:
                research_source = {
                    "source_id": self._generate_source_id(result["url"]),
                    "title": result["title"],
                    "url": result["url"],
                    "content_summary": result["snippet"],
                    "relevance_score": result["relevance_score"],
                    "credibility_score": self._assess_credibility(result),
                    "extracted_insights": [],
                    "key_data_points": [],
                    "search_metadata": {
                        "search_keyword": result.get("search_keyword", ""),
                        "source_type": result.get("source_type", "unknown"),
                        "search_rank": result.get("rank", 0)
                    }
                }
                shared.research_findings.search_results.append(research_source)
            
            # æ›´æ–°ç ”ç©¶å…ƒæ•°æ®
            shared.research_findings.research_metadata.update({
                "last_search_time": time.time(),
                "search_keywords": prep_res["search_keywords"],
                "total_search_results": exec_res["total_found"],
                "search_duration_ms": exec_res["search_time"]
            })

            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯è®°å½•æœç´¢ç»“æœ
            metadata = {
                "agent_source": "NodeSearch",
                "keywords_count": prep_res["keyword_count"],
                "results_count": exec_res["total_found"],
                "search_time_ms": exec_res["search_time"]
            }
            shared.add_system_message(
                f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {exec_res['total_found']} ä¸ªç›¸å…³ç»“æœ",
                metadata=metadata
            )

            return "search_complete"

        except Exception as e:
            if hasattr(shared, 'record_error'):
                shared.record_error(e, "NodeSearch.post")
            return "error"
    
    def exec_fallback(self, prep_res: Dict[str, Any], exc: Exception) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤±è´¥æ—¶çš„é™çº§å¤„ç† - ç›´æ¥è¿”å›é”™è¯¯

        Args:
            prep_res: å‡†å¤‡é˜¶æ®µç»“æœ
            exc: å¼‚å¸¸å¯¹è±¡

        Returns:
            é”™è¯¯ä¿¡æ¯
        """
        return {
            "error": f"Search execution failed: {str(exc)}",
            "search_results": [],
            "total_found": 0
        }

    def _create_error_result(self, error_message: str, search_type: str = "web") -> Dict[str, Any]:
        """åˆ›å»ºæ ‡å‡†é”™è¯¯ç»“æœå­—å…¸"""
        return {
            "error": error_message,
            "search_keywords": [],
            "search_type": search_type,
            "max_results": self.default_max_results,
            "language": self.default_language
        }

    def _classify_source_type(self, url: str) -> str:
        """åˆ†ç±»ä¿¡æ¯æºç±»å‹"""
        if not url:
            return "unknown"

        url_lower = url.lower()

        if any(domain in url_lower for domain in [".gov", ".edu"]):
            return "official"
        elif any(domain in url_lower for domain in ["docs.", "documentation", "wiki"]):
            return "docs"
        elif any(domain in url_lower for domain in ["github.com", "stackoverflow.com"]):
            return "technical"
        elif any(domain in url_lower for domain in ["blog", "medium.com", "csdn.net"]):
            return "blog"
        elif any(domain in url_lower for domain in ["forum", "bbs", "zhihu.com"]):
            return "forum"
        else:
            return "unknown"
    
    def _extract_keywords_from_shared_state(self, shared) -> List[str]:
        """ä»å…±äº«çŠ¶æ€ä¸­æå–æœç´¢å…³é”®è¯"""
        keywords = []

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­æµç¨‹çš„å…±äº«å˜é‡ï¼ˆå­—å…¸ç±»å‹ï¼‰
        if isinstance(shared, dict):
            # å­æµç¨‹æ¨¡å¼ï¼šç›´æ¥ä»å­—å…¸ä¸­è·å–å…³é”®è¯
            search_keywords = shared.get("search_keywords", [])
            if search_keywords:
                keywords.extend(search_keywords)
            return keywords

        # ä¸»æµç¨‹æ¨¡å¼ï¼šä»å…±äº«çŠ¶æ€å¯¹è±¡ä¸­æå–å…³é”®è¯
        # ä»ç”¨æˆ·æ„å›¾ä¸­æå–å…³é”®è¯
        if hasattr(shared, 'user_intent') and shared.user_intent.extracted_keywords:
            keywords.extend(shared.user_intent.extracted_keywords)

        # æ³¨æ„ï¼šç»“æ„åŒ–éœ€æ±‚ç›¸å…³ä»£ç å·²åˆ é™¤ï¼Œå› ä¸ºéœ€æ±‚åˆ†æå­å·¥ä½œæµå·²å–æ¶ˆ

        # å»é‡å¹¶è¿”å›
        return list(set(keywords))[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯
    
    def _optimize_keywords(self, keywords: List[str]) -> List[str]:
        """ä¼˜åŒ–æœç´¢å…³é”®è¯"""
        optimized = []
        
        for keyword in keywords:
            # æ¸…ç†å…³é”®è¯
            cleaned = keyword.strip()
            if not cleaned:
                continue
            
            # æ·»åŠ ç›¸å…³æœ¯è¯­ç»„åˆ
            if "ç®¡ç†ç³»ç»Ÿ" in cleaned:
                optimized.extend([cleaned, cleaned.replace("ç®¡ç†ç³»ç»Ÿ", "ç³»ç»Ÿè®¾è®¡")])
            elif "å¹³å°" in cleaned:
                optimized.extend([cleaned, cleaned + " æ¶æ„"])
            else:
                optimized.append(cleaned)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(set(optimized))[:10]
    
    def _calculate_relevance_score(self, result: Dict[str, Any], keyword: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§è¯„åˆ†"""
        score = 0.0
        
        title = result.get("title", "").lower()
        snippet = result.get("snippet", "").lower()
        url = result.get("url", "").lower()
        keyword_lower = keyword.lower()
        
        # æ ‡é¢˜åŒ¹é…
        if keyword_lower in title:
            score += self.title_weight
        
        # æ‘˜è¦åŒ¹é…
        if keyword_lower in snippet:
            score += self.snippet_weight
        
        # URLåŒ¹é…
        if keyword_lower in url:
            score += self.url_weight
        
        # æ¥æºç±»å‹è¯„åˆ†
        source_type = result.get("source_type", "unknown")
        if source_type in ["official", "docs"]:
            score += self.source_weight
        elif source_type == "blog":
            score += self.source_weight * 0.7
        elif source_type == "forum":
            score += self.source_weight * 0.5
        
        return min(score, 1.0)  # é™åˆ¶åœ¨0-1ä¹‹é—´
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æœç´¢ç»“æœ"""
        seen_urls = set()
        deduplicated = []
        
        for result in results:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                deduplicated.append(result)
        
        return deduplicated
    
    def _sort_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºç»“æœ"""
        return sorted(results, key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    def _assess_credibility(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
        url = result.get("url", "").lower()
        source_type = result.get("source_type", "unknown")
        
        # åŸºäºåŸŸåçš„å¯ä¿¡åº¦è¯„åˆ†
        if any(domain in url for domain in [".edu", ".gov", ".org"]):
            return 0.9
        elif any(domain in url for domain in ["github.com", "stackoverflow.com"]):
            return 0.8
        elif source_type == "official":
            return 0.8
        elif source_type == "docs":
            return 0.7
        elif source_type == "blog":
            return 0.6
        else:
            return 0.5
    
    def _generate_source_id(self, url: str) -> str:
        """ç”Ÿæˆæ¥æºID"""
        return hashlib.md5(url.encode()).hexdigest()[:12]
