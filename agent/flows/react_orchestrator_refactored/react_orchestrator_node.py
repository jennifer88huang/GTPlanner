"""
ReAct Orchestrator Node

åŸºäºFunction Callingçš„ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹ï¼Œé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ã€‚
è´Ÿè´£å¤„ç†å•æ¬¡ReActæ¨ç†å’Œå†³ç­–é€»è¾‘ã€‚
"""


from typing import Dict, List, Any
from pocketflow import AsyncNode

# å¯¼å…¥OpenAI SDKå’ŒFunction Callingå·¥å…·
from utils.openai_client import get_openai_client
from agent.function_calling import get_agent_function_definitions

# å¯¼å…¥æµå¼å“åº”ç±»å‹
from agent.streaming.stream_types import StreamCallbackType

# å¯¼å…¥é‡æ„åçš„ç»„ä»¶
from .constants import (
    ErrorMessages,
    DefaultValues, 
    SystemPrompts
)

from .tool_executor import ToolExecutor




class ReActOrchestratorNode(AsyncNode):
    """ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹ - æ¨¡å—åŒ–è®¾è®¡"""

    def __init__(self):
        super().__init__()
        self.name = "ReActOrchestratorNode"
        self.description = "åŸºäºFunction Callingçš„æ¨¡å—åŒ–ReActä¸»æ§åˆ¶å™¨èŠ‚ç‚¹"

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.openai_client = get_openai_client()

        # è·å–å¯ç”¨çš„Function Callingå·¥å…·
        self.available_tools = get_agent_function_definitions()

        # åˆå§‹åŒ–ç»„ä»¶
        self.tool_executor = ToolExecutor()

    async def prep_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥å‡†å¤‡ReActæ‰§è¡Œç¯å¢ƒï¼ˆæ— çŠ¶æ€ç‰ˆæœ¬ï¼‰"""
        try:
            return {
                "success": True,
                "shared_data": shared
            }

        except Exception as e:
            return {"error": f"{ErrorMessages.REACT_PREP_FAILED}: {str(e)}"}

    async def exec_async(self, prep_result: Dict[str, Any]) -> Dict[str, Any]:
        """å¼‚æ­¥ReActæ¨ç†å’Œå†³ç­–é€»è¾‘ - åŸºäºFunction Callingï¼ˆæ”¯æŒæµå¼å“åº”ï¼‰"""

        try:
            if "error" in prep_result:
                raise ValueError(prep_result["error"])

            shared_data = prep_result.get("shared_data", {})

            # ç›´æ¥ä½¿ç”¨dialogue_historyä¸­çš„messagesï¼ˆå®Œæ•´æˆ–å‹ç¼©è¿‡çš„èŠå¤©è®°å½•ï¼‰
            dialogue_history = shared_data.get("dialogue_history", {})
            messages = dialogue_history.get("messages", [])

            # ä½¿ç”¨Function Callingæ‰§è¡Œï¼ˆä»shared_dataä¸­è·å–æµå¼å“åº”å‚æ•°ï¼‰
            result = await self._execute_with_function_calling(messages, shared_data)

            return result

        except Exception as e:
            print(f"âŒ ReActæ‰§è¡Œå¤±è´¥: {e}")
            
            return {
                "error": f"{ErrorMessages.REACT_EXEC_FAILED}: {str(e)}",
                "user_message": ErrorMessages.GENERIC_ERROR,
                "decision_success": False
            }

    async def post_async(
        self,
        shared: Dict[str, Any],
        prep_res: Dict[str, Any],
        exec_res: Dict[str, Any]
    ) -> str:
        """å¼‚æ­¥æ›´æ–°å…±äº«çŠ¶æ€ï¼ˆæ— çŠ¶æ€ç‰ˆæœ¬ï¼‰"""
        try:
            if "error" in exec_res:
                shared["react_error"] = exec_res["error"]
                return "error"

            # æ›´æ–°ReActå¾ªç¯è®¡æ•°
            self._increment_react_cycle(shared)

            # è·å–æ‰§è¡Œç»“æœ
            assistant_message = exec_res.get("user_message", "")
            tool_calls = exec_res.get("tool_calls", [])

            # å¦‚æœæœ‰åŠ©æ‰‹æ¶ˆæ¯ï¼Œæ·»åŠ åˆ°é¢„ç•™å­—æ®µ
            if assistant_message:
                self._add_assistant_message(shared, assistant_message, tool_calls)

            # å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ
            if tool_calls:
                self._process_tool_calls(shared, tool_calls)

            # ç®€åŒ–è·¯ç”±ï¼šæ€»æ˜¯ç­‰å¾…ç”¨æˆ·ï¼Œè®©LLMåœ¨å›å¤ä¸­è‡ªç„¶å¼•å¯¼ä¸‹ä¸€æ­¥
            return "wait_for_user"

        except Exception as e:
            print(f"âŒ ReAct postå¤„ç†å¤±è´¥: {e}")
            shared["react_post_error"] = str(e)
            return "error"



    def _add_assistant_message(self, shared: Dict[str, Any], message: str, tool_calls: List[Dict[str, Any]]) -> None:
        """æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°é¢„ç•™å­—æ®µ"""
        from datetime import datetime

        assistant_message = {
            "role": "assistant",
            "content": message,
            "timestamp": datetime.now().isoformat(),
            "metadata": {}
        }

        if tool_calls:
            assistant_message["metadata"]["tool_calls"] = tool_calls

        # æ·»åŠ åˆ°é¢„ç•™å­—æ®µ
        if "new_assistant_messages" not in shared:
            shared["new_assistant_messages"] = []

        shared["new_assistant_messages"].append(assistant_message)

    def _process_tool_calls(self, shared: Dict[str, Any], tool_calls: List[Dict[str, Any]]) -> None:
        """å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ"""
        import uuid
        from datetime import datetime

        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name")
            tool_result = tool_call.get("result")
            tool_args = tool_call.get("arguments", {})
            execution_time = tool_call.get("execution_time")

            if tool_name and tool_result:
                # åˆ›å»ºå·¥å…·æ‰§è¡Œè®°å½•
                tool_execution = {
                    "id": str(uuid.uuid4()),
                    "tool_name": tool_name,
                    "arguments": tool_args,
                    "result": tool_result,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat(),
                    "success": tool_result.get("success", True),
                    "error_message": tool_result.get("error")
                }

                # æ·»åŠ åˆ°é¢„ç•™å­—æ®µ
                if "new_tool_executions" not in shared:
                    shared["new_tool_executions"] = []

                shared["new_tool_executions"].append(tool_execution)

    def _increment_react_cycle(self, shared: Dict[str, Any]) -> int:
        """å¢åŠ ReActå¾ªç¯è®¡æ•°"""
        current_count = shared.get("react_cycle_count", 0)
        new_count = current_count + 1
        shared["react_cycle_count"] = new_count
        return new_count

    async def _execute_with_function_calling(
        self,
        messages: List[Dict[str, str]],
        shared: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Function Callingæ‰§è¡ŒReActé€»è¾‘ï¼ˆæ”¯æŒæµå¼å“åº”ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼å“åº”
            streaming_session = shared.get("streaming_session") if shared else None
            streaming_callbacks = shared.get("streaming_callbacks", {}) if shared else {}

            if streaming_session and streaming_callbacks:
                # æµå¼å“åº”æ¨¡å¼
                return await self._execute_with_streaming(
                    messages, streaming_session, streaming_callbacks
                )
            else:
                # éæµå¼å“åº”æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                return await self._execute_without_streaming(messages)

        except Exception as e:
            return {
                "user_message": "",
                "tool_calls": [],
                "reasoning": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "confidence": 0.0,
                "decision_success": False,
                "execution_mode": "error"
            }

    async def _execute_without_streaming(
        self,
        messages: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """éæµå¼æ‰§è¡Œï¼ˆå‘åå…¼å®¹ï¼‰"""
        # ä½¿ç”¨æ–°çš„APIæ¥å£ï¼šåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œæ¶ˆæ¯
        response = await self.openai_client.chat_completion_async(
            system_prompt=SystemPrompts.FUNCTION_CALLING_SYSTEM_PROMPT,
            messages=messages,
            tools=self.available_tools,
            tool_choice="auto",
            parallel_tool_calls=True
        )

        # å¤„ç†å“åº”
        choice = response.choices[0]
        message = choice.message

        # æå–åŠ©æ‰‹å›å¤
        assistant_message = message.content or ""

        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒå¤šä¸ªå¹¶è¡Œè°ƒç”¨ï¼‰
        tool_calls = []

        # æ£€æŸ¥æ ‡å‡†çš„OpenAI Function Callingæ ¼å¼
        if message.tool_calls:
            tool_calls = await self.tool_executor.execute_tools_parallel(message.tool_calls)

        # å¦‚æœæ²¡æœ‰æ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥è‡ªå®šä¹‰æ ¼å¼
        elif assistant_message and "<tool_call>" in assistant_message:
            print("ğŸ”§ æ£€æµ‹åˆ°è‡ªå®šä¹‰æ ¼å¼çš„å·¥å…·è°ƒç”¨")
            custom_tool_calls = self.tool_executor.parse_custom_tool_calls(assistant_message)

            if custom_tool_calls:
                tool_calls = await self.tool_executor.execute_custom_tool_calls(custom_tool_calls)
                # æ¸…ç†assistant_messageä¸­çš„å·¥å…·è°ƒç”¨æ ‡è®°
                assistant_message = self.tool_executor.clean_tool_call_markers(assistant_message)

        # LLMå·²ç»é€šè¿‡Function Callingåšå‡ºäº†å†³ç­–ï¼Œæˆ‘ä»¬åªéœ€è¦å¤„ç†ç»“æœ
        return {
            "user_message": assistant_message,
            "tool_calls": tool_calls,
            "reasoning": f"LLMæ‰§è¡Œäº†{len(tool_calls)}ä¸ªå·¥å…·è°ƒç”¨" if tool_calls else "LLMè¿›è¡Œäº†å¯¹è¯å›å¤",
            "confidence": DefaultValues.DEFAULT_CONFIDENCE,
            "decision_success": True,
            "execution_mode": "parallel" if len(tool_calls) > 1 else "single"
        }

    async def _execute_with_streaming(
        self,
        messages: List[Dict[str, str]],
        streaming_session,
        streaming_callbacks: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æµå¼æ‰§è¡Œï¼ˆå®æ—¶å“åº”ï¼‰"""
        try:
            # è§¦å‘LLMå¼€å§‹å›è°ƒ
            if StreamCallbackType.ON_LLM_START in streaming_callbacks:
                await streaming_callbacks[StreamCallbackType.ON_LLM_START](streaming_session)

            # ä½¿ç”¨æµå¼APIï¼ˆæ”¯æŒsystem_promptå‚æ•°ï¼‰
            stream = self.openai_client.chat_completion_stream_async(
                system_prompt=SystemPrompts.FUNCTION_CALLING_SYSTEM_PROMPT,
                messages=messages,
                tools=self.available_tools,
                tool_choice="auto",
                parallel_tool_calls=True
            )

            # æ”¶é›†æµå¼å“åº”
            assistant_message = ""
            tool_calls_data = []
            chunk_index = 0

            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    delta = choice.delta

                    # å¤„ç†å†…å®¹ç‰‡æ®µ
                    if delta.content:
                        assistant_message += delta.content

                        # è§¦å‘æµå¼å†…å®¹å›è°ƒ
                        if StreamCallbackType.ON_LLM_CHUNK in streaming_callbacks:
                            await streaming_callbacks[StreamCallbackType.ON_LLM_CHUNK](
                                streaming_session,
                                chunk_content=delta.content,
                                chunk_index=chunk_index
                            )
                        chunk_index += 1

                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if delta.tool_calls:
                        for tool_call in delta.tool_calls:
                            # æ”¶é›†å·¥å…·è°ƒç”¨æ•°æ®
                            if tool_call.function:
                                tool_calls_data.append(tool_call)

            # è§¦å‘LLMç»“æŸå›è°ƒ
            if StreamCallbackType.ON_LLM_END in streaming_callbacks:
                await streaming_callbacks[StreamCallbackType.ON_LLM_END](
                    streaming_session,
                    complete_message=assistant_message
                )

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_calls = []
            if tool_calls_data:
                # è§¦å‘å·¥å…·è°ƒç”¨å¼€å§‹å›è°ƒ
                for tool_call in tool_calls_data:
                    if StreamCallbackType.ON_TOOL_START in streaming_callbacks and tool_call.function:
                        # è§£æargumentsï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
                        import json
                        try:
                            arguments = json.loads(tool_call.function.arguments) if isinstance(tool_call.function.arguments, str) else tool_call.function.arguments
                        except:
                            arguments = tool_call.function.arguments

                        await streaming_callbacks[StreamCallbackType.ON_TOOL_START](
                            streaming_session,
                            tool_name=tool_call.function.name,
                            arguments=arguments
                        )

                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_calls = await self.tool_executor.execute_tools_parallel(tool_calls_data)

                # è§¦å‘å·¥å…·è°ƒç”¨ç»“æŸå›è°ƒ
                for tool_call in tool_calls:
                    if StreamCallbackType.ON_TOOL_END in streaming_callbacks:
                        await streaming_callbacks[StreamCallbackType.ON_TOOL_END](
                            streaming_session,
                            tool_name=tool_call.get("tool_name", "unknown"),
                            result=tool_call.get("result", {}),
                            execution_time=tool_call.get("execution_time", 0),
                            success=tool_call.get("success", True),
                            error_message=tool_call.get("error")
                        )

            return {
                "user_message": assistant_message,
                "tool_calls": tool_calls,
                "reasoning": f"LLMæ‰§è¡Œäº†{len(tool_calls)}ä¸ªå·¥å…·è°ƒç”¨" if tool_calls else "LLMè¿›è¡Œäº†å¯¹è¯å›å¤",
                "confidence": DefaultValues.DEFAULT_CONFIDENCE,
                "decision_success": True,
                "execution_mode": "parallel" if len(tool_calls) > 1 else "single"
            }

        except Exception as e:
            print(f"âŒ æµå¼Function Callingæ‰§è¡Œå¤±è´¥: {e}")
            return {
                "error": f"{ErrorMessages.FUNCTION_CALLING_FAILED}: {str(e)}",
                "user_message": ErrorMessages.GENERIC_ERROR,
                "decision_success": False
            }


