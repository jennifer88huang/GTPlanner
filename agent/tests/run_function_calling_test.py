#!/usr/bin/env python3
"""
Function Callingæµ‹è¯•è¿è¡Œå™¨

å¿«é€Ÿæµ‹è¯•Function CallingåŠŸèƒ½çš„è„šæœ¬ã€‚
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from agent.function_calling import (
    get_agent_function_definitions,
    execute_agent_tool,
    call_requirements_analysis
)
from utils.openai_client import get_openai_client


async def test_tool_definitions():
    """æµ‹è¯•å·¥å…·å®šä¹‰"""
    print("ğŸ”§ æµ‹è¯•å·¥å…·å®šä¹‰...")
    
    tools = get_agent_function_definitions()
    print(f"å‘ç° {len(tools)} ä¸ªå·¥å…·:")
    
    for tool in tools:
        name = tool["function"]["name"]
        desc = tool["function"]["description"]
        params = list(tool["function"]["parameters"]["properties"].keys())
        print(f"  - {name}: {desc}")
        print(f"    å‚æ•°: {params}")
    
    print("âœ… å·¥å…·å®šä¹‰æµ‹è¯•å®Œæˆ\n")


async def test_simple_llm_call():
    """æµ‹è¯•ç®€å•çš„LLM Function Calling"""
    print("ğŸ¤– æµ‹è¯•LLM Function Calling...")
    
    try:
        client = get_openai_client()
        tools = get_agent_function_definitions()
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯GTPlanneråŠ©æ‰‹ã€‚å½“ç”¨æˆ·æå‡ºé¡¹ç›®éœ€æ±‚æ—¶ï¼Œä½¿ç”¨requirements_analysiså·¥å…·æ¥åˆ†æéœ€æ±‚ã€‚"
            },
            {
                "role": "user",
                "content": "æˆ‘æƒ³å¼€å‘ä¸€ä¸ªç®€å•çš„å¾…åŠäº‹é¡¹åº”ç”¨"
            }
        ]
        
        print("å‘é€è¯·æ±‚åˆ°OpenAI...")
        response = await client.chat_completion_async(
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        
        choice = response.choices[0]
        message = choice.message
        
        print(f"LLMå›å¤: {message.content}")
        
        if hasattr(message, 'tool_calls') and message.tool_calls:
            print(f"ğŸ”§ LLMè°ƒç”¨äº† {len(message.tool_calls)} ä¸ªå·¥å…·:")
            
            for i, tool_call in enumerate(message.tool_calls, 1):
                print(f"  {i}. å·¥å…·: {tool_call.function.name}")
                print(f"     å‚æ•°: {tool_call.function.arguments}")
                
                # å°è¯•æ‰§è¡Œå·¥å…·
                try:
                    import json
                    arguments = json.loads(tool_call.function.arguments)
                    
                    print(f"     æ‰§è¡Œå·¥å…·...")
                    result = await execute_agent_tool(
                        tool_call.function.name,
                        arguments
                    )
                    
                    if result["success"]:
                        print(f"     âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ")
                        print(f"     ç»“æœç±»å‹: {type(result['result'])}")
                    else:
                        print(f"     âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {result['error']}")
                        
                except Exception as e:
                    print(f"     âŒ å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}")
        else:
            print("â„¹ï¸ LLMæ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›å¤äº†æ–‡æœ¬")
        
        print("âœ… LLM Function Callingæµ‹è¯•å®Œæˆ\n")
        
    except Exception as e:
        print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}\n")


async def test_direct_tool_call():
    """æµ‹è¯•ç›´æ¥å·¥å…·è°ƒç”¨"""
    print("ğŸ› ï¸ æµ‹è¯•ç›´æ¥å·¥å…·è°ƒç”¨...")
    
    try:
        result = await call_requirements_analysis(
            "æˆ‘æƒ³å¼€å‘ä¸€ä¸ªåœ¨çº¿å›¾ä¹¦ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬å›¾ä¹¦å€Ÿé˜…ã€å½’è¿˜ã€æœç´¢ç­‰åŠŸèƒ½"
        )
        
        print(f"å·¥å…·è°ƒç”¨ç»“æœ:")
        print(f"  æˆåŠŸ: {result['success']}")
        
        if result["success"]:
            print(f"  å·¥å…·å: {result['tool_name']}")
            print(f"  ç»“æœç±»å‹: {type(result['result'])}")
            
            # å¦‚æœç»“æœæ˜¯å­—å…¸ï¼Œæ˜¾ç¤ºä¸€äº›å…³é”®ä¿¡æ¯
            if isinstance(result['result'], dict):
                keys = list(result['result'].keys())[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé”®
                print(f"  ç»“æœé”®: {keys}")
        else:
            print(f"  é”™è¯¯: {result['error']}")
        
        print("âœ… ç›´æ¥å·¥å…·è°ƒç”¨æµ‹è¯•å®Œæˆ\n")
        
    except Exception as e:
        print(f"âŒ ç›´æ¥å·¥å…·è°ƒç”¨æµ‹è¯•å¤±è´¥: {e}\n")


async def test_config():
    """æµ‹è¯•é…ç½®"""
    print("âš™ï¸ æµ‹è¯•é…ç½®...")
    
    try:
        from config.openai_config import get_openai_config
        
        config = get_openai_config()
        print(f"OpenAIé…ç½®:")
        print(f"  æ¨¡å‹: {config.model}")
        print(f"  Function Callingå¯ç”¨: {config.function_calling_enabled}")
        print(f"  å·¥å…·é€‰æ‹©: {config.tool_choice}")
        print(f"  å¹¶è¡Œå·¥å…·è°ƒç”¨: {config.parallel_tool_calls}")
        print(f"  æœ€å¤§é‡è¯•: {config.max_retries}")
        print(f"  è¶…æ—¶: {config.timeout}")
        
        print("âœ… é…ç½®æµ‹è¯•å®Œæˆ\n")
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}\n")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ GTPlanner Function Calling æµ‹è¯•\n")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    await test_config()
    await test_tool_definitions()
    await test_simple_llm_call()
    await test_direct_tool_call()
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        test_type = sys.argv[1]
        
        if test_type == "config":
            asyncio.run(test_config())
        elif test_type == "tools":
            asyncio.run(test_tool_definitions())
        elif test_type == "llm":
            asyncio.run(test_simple_llm_call())
        elif test_type == "direct":
            asyncio.run(test_direct_tool_call())
        else:
            print("å¯ç”¨çš„æµ‹è¯•ç±»å‹: config, tools, llm, direct")
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        asyncio.run(main())
