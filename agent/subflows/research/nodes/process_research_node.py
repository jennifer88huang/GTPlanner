"""
ç ”ç©¶å¤„ç†èŠ‚ç‚¹
è´Ÿè´£ç®¡ç†ç ”ç©¶å¤„ç†çŠ¶æ€ï¼Œåè°ƒResearch Agentçš„æ‰§è¡Œ
"""

import time
import json
from pocketflow import AsyncNode
from ..flows.research_flow import ResearchFlow
from agent.llm_utils import call_llm_async


class ProcessResearch(AsyncNode):
    """ç ”ç©¶å¤„ç†èŠ‚ç‚¹ - ç®¡ç†ç ”ç©¶å¤„ç†çŠ¶æ€"""

    def __init__(self):
        super().__init__()
        self.subflow = ResearchFlow()


    async def prep_async(self, shared):
        """å‡†å¤‡ç ”ç©¶å¤„ç† - ç›´æ¥ä»å…±äº«å˜é‡æå–ä¸»agentæ³¨å…¥çš„å…³é”®è¯"""
        # è·å–ä¸»agenté€šè¿‡Function Callingæ³¨å…¥çš„ç ”ç©¶å…³é”®è¯
        research_keywords = shared.get("research_keywords", [])
        focus_areas = shared.get("focus_areas", [])
        project_context = shared.get("project_context", "")

        # è·å–ç»“æ„åŒ–éœ€æ±‚
        structured_requirements = shared.get("structured_requirements", {})

        # è·å–ç”¨æˆ·éœ€æ±‚ä¿¡æ¯
        requirements = {}
        if structured_requirements:
            project_overview = structured_requirements.get("project_overview", {})
            requirements = {
                "project_title": project_overview.get("title", ""),
                "project_description": project_overview.get("description", ""),
                "objectives": project_overview.get("objectives", [])
            }

        # å¦‚æœä¸»agentæ²¡æœ‰æä¾›å…³é”®è¯ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
        if not research_keywords:
            print("âš ï¸ ä¸»agentæœªæä¾›ç ”ç©¶å…³é”®è¯ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            research_keywords = self._extract_keywords_from_requirements(structured_requirements)

        print(f"ğŸ” è·å–åˆ°ç ”ç©¶å…³é”®è¯: {research_keywords}")
        print(f"ğŸ¯ å…³æ³¨ç‚¹: {focus_areas}")
        print(f"ğŸ“ é¡¹ç›®èƒŒæ™¯: {project_context}")

        return {
            "research_keywords": research_keywords,
            "focus_areas": focus_areas,
            "project_context": project_context,
            "requirements": requirements,
            "total_keywords": len(research_keywords)
        }
    async def exec_async(self, data):
        """æ‰§è¡Œç ”ç©¶å¤„ç† - ä½¿ç”¨LLMç›´æ¥åˆ†æå…³é”®è¯"""
        research_keywords = data["research_keywords"]
        focus_areas = data["focus_areas"]
        project_context = data["project_context"]
        requirements = data["requirements"]
        total_keywords = data["total_keywords"]

        print(f"ğŸ”„ å¼€å§‹ç ”ç©¶å¤„ç†ï¼Œå…³é”®è¯: {research_keywords}")

        try:
            # ä½¿ç”¨LLMç›´æ¥è¿›è¡Œå…³é”®è¯è°ƒç ”åˆ†æ
            research_result = await self._execute_keyword_research_with_llm(
                research_keywords, focus_areas, project_context
            )

            if not research_result.get("success"):
                raise Exception(f"LLMç ”ç©¶åˆ†æå¤±è´¥: {research_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # æå–å¤„ç†ç»“æœ
            research_findings = research_result.get("result", {})
            research_report = research_findings.get("results", [])
            aggregated_summary = research_findings.get("summary", {})
            successful_keywords = len(research_report)

            print(f"âœ… ç ”ç©¶å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_keywords}/{total_keywords} ä¸ªå…³é”®è¯")

            return {
                "keywords": research_keywords,
                "result": research_findings,
                "processing_success": True,
                "research_report": research_report,
                "aggregated_summary": aggregated_summary,
                "successful_keywords": successful_keywords,
                "total_keywords": total_keywords
            }

        except Exception as e:
            error_msg = f"ç ”ç©¶å¤„ç†å¤±è´¥: {str(e)}"
            print(f"[ERROR] {error_msg}")

            return {
                "keywords": research_keywords,
                "result": None,
                "processing_success": False,
                "error": error_msg,
                "research_report": [],
                "aggregated_summary": {},
                "successful_keywords": 0,
                "total_keywords": total_keywords
            }

    async def _execute_keyword_research_with_llm(self, keywords: list, focus_areas: list, project_context: str) -> dict:
        """ä½¿ç”¨LLMæ‰§è¡ŒåŸºäºå…³é”®è¯çš„æŠ€æœ¯è°ƒç ”"""
        try:
            # æ„å»ºè°ƒç ”æç¤ºè¯
            focus_description = "ã€".join(focus_areas)
            keywords_text = "ã€".join(keywords)

            prompt = f"""
è¯·åŸºäºä»¥ä¸‹å…³é”®è¯è¿›è¡ŒæŠ€æœ¯è°ƒç ”åˆ†æï¼š

å…³é”®è¯ï¼š{keywords_text}
å…³æ³¨ç‚¹ï¼š{focus_description}
é¡¹ç›®èƒŒæ™¯ï¼š{project_context}

è¯·é’ˆå¯¹æ¯ä¸ªå…³é”®è¯ï¼Œä»æŒ‡å®šçš„å…³æ³¨ç‚¹è§’åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š

{{
    "research_summary": {{
        "total_keywords": {len(keywords)},
        "focus_areas": {focus_areas},
        "analysis_depth": "detailed"
    }},
    "keyword_analysis": [
        {{
            "keyword": "å…³é”®è¯1",
            "analysis": {{
                "æŠ€æœ¯é€‰å‹": "ç›¸å…³æŠ€æœ¯é€‰å‹å»ºè®®",
                "æ€§èƒ½ä¼˜åŒ–": "æ€§èƒ½ä¼˜åŒ–è¦ç‚¹",
                "æœ€ä½³å®è·µ": "è¡Œä¸šæœ€ä½³å®è·µ",
                "æ¶æ„è®¾è®¡": "æ¶æ„è®¾è®¡è€ƒè™‘"
            }},
            "recommendations": ["å»ºè®®1", "å»ºè®®2"],
            "relevance_score": 0.9
        }}
    ],
    "cross_keyword_insights": [
        "è·¨å…³é”®è¯çš„ç»¼åˆæ´å¯Ÿ1",
        "è·¨å…³é”®è¯çš„ç»¼åˆæ´å¯Ÿ2"
    ],
    "implementation_roadmap": [
        "å®æ–½æ­¥éª¤1",
        "å®æ–½æ­¥éª¤2"
    ]
}}

è¦æ±‚ï¼š
1. é’ˆå¯¹æ¯ä¸ªå…³é”®è¯ï¼Œä»æ‰€æœ‰å…³æ³¨ç‚¹è§’åº¦è¿›è¡Œåˆ†æ
2. æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®
3. è¯†åˆ«å…³é”®è¯ä¹‹é—´çš„å…³è”å’ŒååŒæ•ˆåº”
4. ç»™å‡ºå®æ–½è·¯çº¿å›¾å»ºè®®
"""

            print("ğŸ”§ ä½¿ç”¨LLMè¿›è¡Œå…³é”®è¯è°ƒç ”åˆ†æ...")
            result = await call_llm_async(prompt, is_json=True)

            if isinstance(result, str):
                result = json.loads(result)

            print("âœ… å…³é”®è¯è°ƒç ”åˆ†æå®Œæˆ")

            # æ ¼å¼åŒ–ä¸ºæ ‡å‡†çš„research_findingsæ ¼å¼
            research_findings = {
                "topics": keywords,
                "results": result.get("keyword_analysis", []),
                "summary": {
                    "total_keywords": len(keywords),
                    "focus_areas": focus_areas,
                    "cross_insights": result.get("cross_keyword_insights", []),
                    "roadmap": result.get("implementation_roadmap", [])
                }
            }

            return {
                "success": True,
                "result": research_findings
            }

        except Exception as e:
            print(f"âŒ å…³é”®è¯è°ƒç ”å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"Keyword research failed: {str(e)}"
            }

    def _extract_keywords_from_requirements(self, structured_requirements: dict) -> list:
        """åŸºäºè§„åˆ™ä»ç»“æ„åŒ–éœ€æ±‚ä¸­æå–å…³é”®è¯ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        research_keywords = []

        if not structured_requirements:
            return ["é¡¹ç›®å¼€å‘", "æŠ€æœ¯é€‰å‹", "æœ€ä½³å®è·µ"]

        # ä»é¡¹ç›®æ¦‚è§ˆè·å–æ ‡é¢˜
        project_overview = structured_requirements.get("project_overview", {})
        project_title = project_overview.get("title", "")
        if project_title:
            research_keywords.append(project_title)

        # ä»æ ¸å¿ƒåŠŸèƒ½è·å–å…³é”®è¯
        functional_requirements = structured_requirements.get("functional_requirements", {})
        core_features = functional_requirements.get("core_features", [])
        for feature in core_features[:2]:  # é™åˆ¶æ•°é‡
            if isinstance(feature, dict):
                feature_name = feature.get("name", "")
            elif isinstance(feature, str):
                feature_name = feature
            else:
                feature_name = str(feature) if feature else ""

            if feature_name and feature_name not in research_keywords:
                research_keywords.append(feature_name)

        # æ·»åŠ é€šç”¨æŠ€æœ¯å…³é”®è¯
        if project_title:
            research_keywords.extend([
                f"{project_title} æŠ€æœ¯æ–¹æ¡ˆ",
                f"{project_title} æ¶æ„è®¾è®¡",
                "æœ€ä½³å®è·µ",
                "æŠ€æœ¯é€‰å‹"
            ])

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        research_keywords = list(set(research_keywords))[:5]

        if not research_keywords:
            research_keywords = ["é¡¹ç›®å¼€å‘", "æŠ€æœ¯é€‰å‹", "æœ€ä½³å®è·µ"]

        print(f"ğŸ”„ ä½¿ç”¨è§„åˆ™æå–å…³é”®è¯: {research_keywords}")
        return research_keywords

    async def post_async(self, shared, prep_res, exec_res):
        """ä¿å­˜ç ”ç©¶ç»“æœå¹¶æ›´æ–°çŠ¶æ€ - ä½¿ç”¨pocketflowå­—å…¸å…±äº«å˜é‡"""
        # æ£€æŸ¥æ‰§è¡Œç»“æœ
        if "error" in exec_res:
            shared["research_error"] = exec_res["error"]
            shared["current_stage"] = "research_failed"
            return "error"

        keywords = exec_res.get("keywords", [])
        result = exec_res.get("result")
        success = exec_res.get("processing_success", False)

        # ä»å­æµç¨‹ç»“æœä¸­æå–æ•°æ®
        research_report = exec_res.get("research_report", [])
        aggregated_summary = exec_res.get("aggregated_summary", {})
        successful_keywords = exec_res.get("successful_keywords", 0)
        total_keywords = exec_res.get("total_keywords", 0)

        # ä¿å­˜ç ”ç©¶ç»“æœåˆ°å…±äº«å˜é‡
        if success and result:
            # åˆ›å»ºç ”ç©¶å‘ç°æ•°æ®ç»“æ„
            research_findings = {
                "research_report": research_report,
                "aggregated_summary": aggregated_summary,
                "research_metadata": {
                    "research_keywords": keywords,
                    "total_keywords": total_keywords,
                    "successful_keywords": successful_keywords,
                    "success_rate": successful_keywords / total_keywords if total_keywords > 0 else 0,
                    "research_completed_at": time.time(),
                    "research_success": True
                }
            }

            # ä¿å­˜åˆ°å…±äº«å˜é‡
            shared["research_findings"] = research_findings

        # æ›´æ–°å¤„ç†é˜¶æ®µ
        shared["current_stage"] = "research_completed"

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        system_messages = shared.get("system_messages", [])
        system_messages.append({
            "message": f"ç ”ç©¶è°ƒç ”å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_keywords}/{total_keywords} ä¸ªå…³é”®è¯",
            "agent_source": "ProcessResearch",
            "timestamp": time.time(),
            "keywords": keywords,
            "successful_keywords": successful_keywords,
            "total_keywords": total_keywords,
            "success": success
        })
        shared["system_messages"] = system_messages

        # æ›´æ–°å…ƒæ•°æ®
        metadata = shared.get("metadata", {})
        processing_stages = metadata.get("processing_stages", [])
        if "research" not in processing_stages:
            processing_stages.append("research")
        metadata.update({
            "processing_stages": processing_stages,
            "last_updated": time.time(),
            "total_processing_time": metadata.get("total_processing_time", 0) + 50.0  # ä¼°ç®—å¤„ç†æ—¶é—´
        })
        shared["metadata"] = metadata

        print(f"âœ… ç ”ç©¶å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(research_report)} ä¸ªå…³é”®è¯æŠ¥å‘Š")

        # è¿”å›ä¸‹ä¸€æ­¥åŠ¨ä½œ - åŸºäºpocketflowæœ€ä½³å®è·µ
        if success and successful_keywords > 0:
            return "success"  # æˆåŠŸå®Œæˆï¼Œå¯ä»¥ç»§ç»­ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        else:
            return "failed"   # å¤„ç†å¤±è´¥ï¼Œéœ€è¦é”™è¯¯å¤„ç†
    
    def _build_analysis_requirements(self, requirements):
        """æ„å»ºåˆ†æéœ€æ±‚æè¿°"""
        analysis_parts = []
        
        if requirements.get("project_title"):
            analysis_parts.append(f"é¡¹ç›®èƒŒæ™¯ï¼š{requirements['project_title']}")
        
        if requirements.get("project_description"):
            analysis_parts.append(f"é¡¹ç›®æè¿°ï¼š{requirements['project_description']}")
        
        if requirements.get("objectives"):
            objectives_text = "ã€".join(requirements["objectives"])
            analysis_parts.append(f"é¡¹ç›®ç›®æ ‡ï¼š{objectives_text}")
        
        # é»˜è®¤åˆ†æéœ€æ±‚
        analysis_parts.append("è¯·é‡ç‚¹å…³æ³¨ï¼šæŠ€æœ¯å®ç°æ–¹æ¡ˆã€æœ€ä½³å®è·µã€ç›¸å…³å·¥å…·å’Œæ¡†æ¶ã€æ½œåœ¨é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ")
        
        return "\n".join(analysis_parts)
