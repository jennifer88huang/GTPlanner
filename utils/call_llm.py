import asyncio
import json
import os
from typing import Any

import aiohttp
from dynaconf import Dynaconf
from json_repair import repair_json
from openai import AsyncOpenAI

# Initialize Dynaconf with environment variable support
settings = Dynaconf(
    settings_files=["settings.toml", "settings.local.toml", ".secrets.toml"],
    environments=True,
    env_switcher="ENV_FOR_DYNACONF",
    load_dotenv=True,  # è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶
)


async def _request_llm_async(
    prompt,
    model,
    is_json=False,
):
    import time

    # è°ƒè¯•ä¿¡æ¯ï¼šè¯·æ±‚å¼€å§‹
    print(f"ğŸ¤– LLMè°ƒç”¨å¼€å§‹")
    print(f"   ğŸ“¡ URL: {settings.llm.base_url}/chat/completions")
    print(f"   ğŸ¯ æ¨¡å‹: {model}")
    print(f"   ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print(f"   ğŸ“‹ JSONæ¨¡å¼: {'æ˜¯' if is_json else 'å¦'}")

    url = f"{settings.llm.base_url}/chat/completions"
    payload = json.dumps(
        {
            "messages": [{"role": "user", "content": prompt}],
            "model": model,
            "stream": False,
            "temperature": 0,
            "top_p": 1,
        }
    )
    headers = {
        "Authorization": f"Bearer {settings.llm.api_key}",
        "Content-Type": "application/json",
    }

    request_start_time = time.time()
    print(f"   ğŸš€ å‘é€HTTPè¯·æ±‚...")

    try:
        async with aiohttp.ClientSession() as session:
            # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œé€‚åº”å¤æ‚æç¤ºè¯
            timeout = aiohttp.ClientTimeout(total=120, connect=15, sock_read=90)
            print(f"   â° è¶…æ—¶è®¾ç½®: æ€»è®¡60ç§’, è¿æ¥10ç§’, è¯»å–30ç§’")

            async with session.post(
                url, headers=headers, data=payload, timeout=timeout
            ) as response:
                print(f"   ğŸ“¨ æ”¶åˆ°HTTPå“åº”: {response.status}")

                if response.status != 200:
                    error_text = await response.text()
                    print(f"   âŒ HTTPé”™è¯¯: {response.status}")
                    print(f"   ğŸ“„ é”™è¯¯å†…å®¹: {error_text}")
                    raise Exception(f"HTTP {response.status}: {error_text}")

                response_json = await response.json()
                request_duration = time.time() - request_start_time
                print(f"   âœ… HTTPè¯·æ±‚å®Œæˆ (è€—æ—¶: {request_duration:.2f}ç§’)")

                # æ£€æŸ¥å“åº”ç»“æ„
                if "choices" not in response_json:
                    print(f"   âŒ å“åº”æ ¼å¼é”™è¯¯: ç¼ºå°‘choiceså­—æ®µ")
                    print(f"   ğŸ“„ å“åº”å†…å®¹: {response_json}")
                    raise Exception("LLMå“åº”æ ¼å¼é”™è¯¯")

                if not response_json["choices"]:
                    print(f"   âŒ å“åº”æ ¼å¼é”™è¯¯: choicesä¸ºç©º")
                    raise Exception("LLMå“åº”choicesä¸ºç©º")

                response_text = response_json["choices"][0]["message"]["content"]
                print(f"   ğŸ“Š LLMè¿”å›å†…å®¹é•¿åº¦: {len(response_text)} å­—ç¬¦")

                if is_json:
                    print(f"   ğŸ”§ è§£æJSONå“åº”...")
                    print(f"   ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {response_text[:200]}...")

                    # å…ˆå°è¯•ç›´æ¥è§£æJSON
                    try:
                        json_start_time = time.time()
                        json_data = json.loads(response_text)
                        json_duration = time.time() - json_start_time
                        print(f"   âœ… ç›´æ¥JSONè§£ææˆåŠŸ (è€—æ—¶: {json_duration:.2f}ç§’)")
                        return json_data
                    except json.JSONDecodeError as e:
                        print(f"   âš ï¸ ç›´æ¥JSONè§£æå¤±è´¥: {e}")
                        print(f"   ğŸ”§ å°è¯•ä½¿ç”¨repair_jsonä¿®å¤...")

                        repair_start_time = time.time()
                        json_data = repair_json(response_text, return_objects=True)
                        repair_duration = time.time() - repair_start_time
                        print(f"   âœ… JSONä¿®å¤è§£ææˆåŠŸ (è€—æ—¶: {repair_duration:.2f}ç§’)")
                        return json_data
                else:
                    print(f"   âœ… æ–‡æœ¬å“åº”è¿”å›")
                    return response_text

    except Exception as e:
        request_duration = time.time() - request_start_time
        print(f"   âŒ LLMè°ƒç”¨å¤±è´¥ (è€—æ—¶: {request_duration:.2f}ç§’)")
        print(f"   ğŸ“„ é”™è¯¯ä¿¡æ¯: {str(e)}")
        raise e


async def call_llm_async(prompt, is_json=False, max_retries=3, retry_delay=2):
    import time
    import asyncio

    print(f"ğŸ¯ å¼€å§‹LLMè°ƒç”¨ (æœ€å¤§é‡è¯•: {max_retries}æ¬¡)")
    print(f"   ğŸ”§ é…ç½®æ¨¡å‹: {settings.llm.model}")
    print(f"   ğŸŒ APIåœ°å€: {settings.llm.base_url}")

    start_time = time.time()
    last_error = None

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                print(f"ğŸ”„ ç¬¬{attempt}æ¬¡é‡è¯•...")
                await asyncio.sleep(retry_delay * attempt)  # é€’å¢å»¶è¿Ÿ

            result = await _request_llm_async(prompt, model=settings.llm.model, is_json=is_json)
            total_time = time.time() - start_time

            if attempt > 0:
                print(f"ğŸ‰ LLMè°ƒç”¨é‡è¯•æˆåŠŸ (ç¬¬{attempt}æ¬¡é‡è¯•, æ€»è€—æ—¶: {total_time:.2f}ç§’)")
            else:
                print(f"ğŸ‰ LLMè°ƒç”¨æˆåŠŸ (æ€»è€—æ—¶: {total_time:.2f}ç§’)")

            return result

        except Exception as e:
            last_error = e
            current_time = time.time() - start_time

            # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
            should_retry = attempt < max_retries and _should_retry_error(e)

            if should_retry:
                print(f"âš ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ (è€—æ—¶: {current_time:.2f}ç§’): {str(e)}")
                print(f"   ğŸ”„ å°†åœ¨{retry_delay * (attempt + 1)}ç§’åé‡è¯•...")
            else:
                total_time = time.time() - start_time
                print(f"ğŸ’¥ LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥ (æ€»è€—æ—¶: {total_time:.2f}ç§’)")
                print(f"   ğŸ“„ å¤±è´¥åŸå› : {str(e)}")
                if attempt >= max_retries:
                    print(f"   ğŸš« å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})")
                else:
                    print(f"   ğŸš« é”™è¯¯ç±»å‹ä¸é€‚åˆé‡è¯•")
                break

    raise last_error


def _should_retry_error(error):
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦åº”è¯¥é‡è¯•"""
    error_str = str(error).lower()

    # ç½‘ç»œç›¸å…³é”™è¯¯åº”è¯¥é‡è¯•
    retry_keywords = [
        'timeout',
        'server disconnected',
        'connection',
        'network',
        'socket',
        'read timeout',
        'connect timeout'
    ]

    # JSONè§£æé”™è¯¯é€šå¸¸ä¸åº”è¯¥é‡è¯•ï¼ˆé™¤éæ˜¯ç½‘ç»œå¯¼è‡´çš„ä¸å®Œæ•´å“åº”ï¼‰
    no_retry_keywords = [
        'json decode',
        'invalid json',
        'expecting',
        'unterminated'
    ]

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸é‡è¯•çš„å…³é”®è¯
    for keyword in no_retry_keywords:
        if keyword in error_str:
            return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¯•çš„å…³é”®è¯
    for keyword in retry_keywords:
        if keyword in error_str:
            return True

    # é»˜è®¤ä¸é‡è¯•æœªçŸ¥é”™è¯¯
    return False


async def _request_llm_stream_async(
    prompt,
    model,
):
    """æµå¼LLMè¯·æ±‚"""
    url = f"{settings.llm.base_url}/chat/completions"
    payload = json.dumps(
        {
            "messages": [{"role": "user", "content": prompt}],
            "model": model,
            "stream": True,
            "temperature": 0,
            "top_p": 1,
        },
        ensure_ascii=False
    )
    headers = {
        "Authorization": f"Bearer {settings.llm.api_key}",
        "Content-Type": "application/json; charset=utf-8",
    }

    async with aiohttp.ClientSession() as session:
        timeout = aiohttp.ClientTimeout(total=100000)
        async with session.post(
            url, headers=headers, data=payload.encode('utf-8'), timeout=timeout
        ) as response:
            buffer = b""
            async for chunk in response.content.iter_chunked(1024):
                buffer += chunk
                while b'\n' in buffer:
                    line_bytes, buffer = buffer.split(b'\n', 1)
                    try:
                        line_str = line_bytes.decode('utf-8').strip()
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]  # ç§»é™¤ "data: " å‰ç¼€
                            if data_str == '[DONE]':
                                return
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        content = delta['content']
                                        # ç¡®ä¿æ¢è¡Œç¬¦è¢«æ­£ç¡®ä¼ é€’
                                        yield content
                            except json.JSONDecodeError:
                                continue
                    except UnicodeDecodeError:
                        continue


async def call_llm_stream_async(prompt):
    """æµå¼è°ƒç”¨LLM"""
    async for chunk in _request_llm_stream_async(prompt, model=settings.llm.model):
        yield chunk


# Synchronous version for backward compatibility
def call_llm(prompt, conversation_history=None, is_json=False):
    """Synchronous wrapper for call_llm_async"""
    return asyncio.run(call_llm_async(prompt, conversation_history, is_json))


# Example usage
if __name__ == "__main__":

    async def test():
        response = await call_llm_async("What is the purpose of async programming?")
        print(response)

    asyncio.run(test())
