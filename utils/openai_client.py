"""
OpenAI SDKå°è£…å±‚

æä¾›ç»Ÿä¸€çš„OpenAI SDKå¼‚æ­¥æ¥å£ï¼Œé›†æˆé…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’ŒFunction CallingåŠŸèƒ½ã€‚
"""

import asyncio
import os
import time
from typing import Dict, List, Any, Optional, AsyncIterator, Callable, TypedDict
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk

from utils.logger_config import get_openai_logger

try:
    from dynaconf import Dynaconf
    DYNACONF_AVAILABLE = True
except ImportError:
    DYNACONF_AVAILABLE = False


class Message(TypedDict):
    """æ¶ˆæ¯ç±»å‹å®šä¹‰"""
    role: str
    content: str


class SimpleOpenAIConfig:
    """ç®€åŒ–çš„OpenAIé…ç½®ç±»"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-4",
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        timeout: float = 120.0,
        max_retries: int = 3,
        retry_delay: float = 2.0,
        log_requests: bool = True,
        log_responses: bool = True,
        function_calling_enabled: bool = True,
        tool_choice: str = "auto",
    ):
        # å°è¯•ä» settings.toml åŠ è½½é…ç½®
        settings = self._load_settings()

        self.api_key = api_key or self._get_setting(settings, "llm.api_key") or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or self._get_setting(settings, "llm.base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = self._get_setting(settings, "llm.model") or os.getenv("OPENAI_MODEL", model)
        self.temperature = self._get_setting(settings, "llm.temperature", temperature)
        self.max_tokens = self._get_setting(settings, "llm.max_tokens", max_tokens)
        self.timeout = self._get_setting(settings, "llm.timeout", timeout)
        self.max_retries = self._get_setting(settings, "llm.max_retries", max_retries)
        self.retry_delay = self._get_setting(settings, "llm.retry_delay", retry_delay)
        self.log_requests = self._get_setting(settings, "llm.log_requests", log_requests)
        self.log_responses = self._get_setting(settings, "llm.log_responses", log_responses)
        self.function_calling_enabled = self._get_setting(settings, "llm.function_calling_enabled", function_calling_enabled)
        self.tool_choice = self._get_setting(settings, "llm.tool_choice", tool_choice)

        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or configure llm.api_key in settings.toml.")

    def _load_settings(self):
        """åŠ è½½ Dynaconf è®¾ç½®"""
        if not DYNACONF_AVAILABLE:
            return None

        try:
            return Dynaconf(
                settings_files=["settings.toml", "settings.local.toml", ".secrets.toml"],
                environments=True,
                env_switcher="ENV_FOR_DYNACONF",
                load_dotenv=True,
            )
        except Exception:
            return None

    def _get_setting(self, settings, key: str, default: Any = None) -> Any:
        """ä»è®¾ç½®ä¸­è·å–å€¼"""
        if settings is None:
            return default

        try:
            return settings.get(key, default)
        except Exception:
            return default

    def to_openai_client_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å‚æ•°"""
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "timeout": self.timeout,
            "max_retries": self.max_retries,
        }

    def to_chat_completion_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºchat completionè°ƒç”¨å‚æ•°"""
        kwargs = {
            "model": self.model,
            "temperature": self.temperature,
        }

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        return kwargs


class OpenAIClientError(Exception):
    """OpenAIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»"""
    pass


class OpenAIRateLimitError(OpenAIClientError):
    """APIé€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass


class OpenAITimeoutError(OpenAIClientError):
    """APIè¶…æ—¶é”™è¯¯"""
    pass


class OpenAIRetryableError(OpenAIClientError):
    """å¯é‡è¯•çš„APIé”™è¯¯"""
    pass


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå‡½æ•°å¹¶åœ¨å¤±è´¥æ—¶é‡è¯•

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

            except Exception as e:
                last_error = e

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    break

                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt)

                # ä½¿ç”¨æ—¥å¿—è®°å½•é‡è¯•ä¿¡æ¯
                from utils.logger_config import get_logger
                logger = get_logger("retry_manager")
                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                logger.info(f"ğŸ”„ ç­‰å¾… {delay:.1f}ç§’åé‡è¯•...")

                await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise last_error

    def _should_retry(self, error: Exception, attempt: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        Args:
            error: é”™è¯¯å¯¹è±¡
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        if attempt >= self.max_retries:
            return False

        error_str = str(error).lower()

        # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
        retryable_errors = [
            "rate_limit",
            "timeout",
            "connection",
            "network",
            "server_error",
            "503",
            "502",
            "500",
            "429"
        ]

        return any(err in error_str for err in retryable_errors)

    def _calculate_delay(self, attempt: int) -> float:
        """
        è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰

        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        import random

        # æŒ‡æ•°é€€é¿
        delay = self.base_delay * (2 ** attempt)

        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆÂ±25%ï¼‰
        jitter = delay * 0.25 * (random.random() * 2 - 1)

        return max(0.1, delay + jitter)


class OpenAIClient:
    """OpenAI SDKå°è£…å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[SimpleOpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: OpenAIé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or SimpleOpenAIConfig()

        # è·å–æ—¥å¿—å™¨ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼‰
        self.logger = get_openai_logger()

        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        client_kwargs = self.config.to_openai_client_kwargs()
        self.async_client = AsyncOpenAI(**client_kwargs)

        # åˆ›å»ºé‡è¯•ç®¡ç†å™¨
        self.retry_manager = RetryManager(
            max_retries=self.config.max_retries,
            base_delay=self.config.retry_delay
        )

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }

        # å…¨å±€ç³»ç»Ÿæç¤ºè¯
        self.global_system_prompt = "å¦‚æœæ˜¯JSONè¾“å‡ºï¼Œæœ€ç»ˆè¾“å‡ºåªåŒ…å«JSONæ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—åŒ…è£¹"

        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        self.logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.config.model}, åŸºç¡€URL: {self.config.base_url}")

    def _prepare_messages(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None
    ) -> List[Message]:
        """
        å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼Œç»Ÿä¸€å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œå…¨å±€ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            å‡†å¤‡å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        prepared_messages = []

        # æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯
        if self.global_system_prompt:
            prepared_messages.append({"role": "system", "content": self.global_system_prompt})

        # æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            prepared_messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ å¯¹è¯æ¶ˆæ¯
        if messages:
            prepared_messages.extend(messages)

        return prepared_messages

    def _prepare_request_params(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡APIè¯·æ±‚å‚æ•°

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å‡†å¤‡å¥½çš„è¯·æ±‚å‚æ•°
        """
        # å‡†å¤‡æ¶ˆæ¯
        prepared_messages = self._prepare_messages(messages, system_prompt)

        # åˆå¹¶é…ç½®å‚æ•°
        params = self.config.to_chat_completion_kwargs()
        params.update(kwargs)
        params["messages"] = prepared_messages

        if stream:
            params["stream"] = True

        # æ·»åŠ å·¥å…·æ”¯æŒ
        if tools and self.config.function_calling_enabled:
            params["tools"] = tools
            if "tool_choice" not in params:
                params["tool_choice"] = self.config.tool_choice

        return params

    def _update_success_stats(self, response: Any) -> None:
        """æ›´æ–°æˆåŠŸç»Ÿè®¡ä¿¡æ¯"""
        self.stats["successful_requests"] += 1
        if hasattr(response, 'usage') and response.usage:
            self.stats["total_tokens"] += response.usage.total_tokens

    def _update_failure_stats(self) -> None:
        """æ›´æ–°å¤±è´¥ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["failed_requests"] += 1



    async def chat_completion(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion", params)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒAPIè°ƒç”¨
            async def _api_call():
                return await self.async_client.chat.completions.create(**params)

            response = await self.retry_manager.execute_with_retry(_api_call)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_success_stats(response)

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response("chat_completion", response)

            return response

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time
    
    async def chat_completion_stream(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                stream=True,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion_stream", params)

            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = await self.async_client.chat.completions.create(**params)

            chunk_count = 0
            full_content = ""
            total_tokens = 0

            async for chunk in stream:
                chunk_count += 1

                # æ”¶é›†å“åº”å†…å®¹ç”¨äºæ—¥å¿—è®°å½•
                if chunk.choices and chunk.choices[0].delta.content:
                    full_content += chunk.choices[0].delta.content

                # æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
                if hasattr(chunk, 'usage') and chunk.usage:
                    total_tokens = chunk.usage.total_tokens

                yield chunk

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["successful_requests"] += 1
            if total_tokens > 0:
                self.stats["total_tokens"] += total_tokens

            # è®°å½•å“åº”æ—¥å¿—ï¼ˆæµå¼å“åº”ï¼‰
            self._log_stream_response("chat_completion_stream", chunk_count, full_content)

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time




    
    def _handle_error(self, error: Exception) -> OpenAIClientError:
        """
        å¤„ç†å’Œè½¬æ¢é”™è¯¯

        Args:
            error: åŸå§‹é”™è¯¯

        Returns:
            è½¬æ¢åçš„é”™è¯¯
        """
        import openai

        # OpenAI SDKç‰¹å®šé”™è¯¯
        if isinstance(error, openai.RateLimitError):
            return OpenAIRateLimitError(f"API rate limit exceeded: {error}")

        if isinstance(error, openai.APITimeoutError):
            return OpenAITimeoutError(f"API request timeout: {error}")

        if isinstance(error, openai.APIConnectionError):
            return OpenAIRetryableError(f"API connection error: {error}")

        if isinstance(error, openai.InternalServerError):
            return OpenAIRetryableError(f"Internal server error: {error}")

        if isinstance(error, openai.BadRequestError):
            return OpenAIClientError(f"Bad request: {error}")

        if isinstance(error, openai.AuthenticationError):
            return OpenAIClientError(f"Authentication failed: {error}")

        if isinstance(error, openai.PermissionDeniedError):
            return OpenAIClientError(f"Permission denied: {error}")

        if isinstance(error, openai.NotFoundError):
            return OpenAIClientError(f"Resource not found: {error}")

        # é€šç”¨é”™è¯¯å¤„ç†
        error_message = str(error)

        # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "rate_limit" in error_message.lower() or "429" in error_message:
            return OpenAIRateLimitError(f"API rate limit exceeded: {error_message}")

        # è¶…æ—¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "timeout" in error_message.lower() or "timed out" in error_message.lower():
            return OpenAITimeoutError(f"API request timeout: {error_message}")

        # ç½‘ç»œé”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(keyword in error_message.lower() for keyword in ["connection", "network", "dns"]):
            return OpenAIRetryableError(f"Network error: {error_message}")

        # æœåŠ¡å™¨é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(code in error_message for code in ["500", "502", "503", "504"]):
            return OpenAIRetryableError(f"Server error: {error_message}")

        # å…¶ä»–é”™è¯¯
        return OpenAIClientError(f"OpenAI API error: {error_message}")
    
    def _log_request(self, method: str, params: Dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        if self.config.log_requests:
            self.logger.info(f"ğŸ”„ OpenAI {method} è¯·æ±‚:")
            self.logger.info(f"ï¿½ å®Œæ•´è¯·æ±‚å‚æ•°: {params}")

    def _log_response(self, method: str, response: Any) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} å“åº”:")
            self.logger.info(f"ğŸ“ å®Œæ•´å“åº”: {response}")

    def _log_stream_response(self, method: str, chunk_count: int, content: str = "") -> None:
        """è®°å½•æµå¼å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} æµå¼å“åº”å®Œæˆ: æ¥æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
            if content:
                self.logger.info(f"ğŸ“ å®Œæ•´æµå¼å“åº”å†…å®¹: {content}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_client: Optional[OpenAIClient] = None


def get_openai_client(config: Optional[SimpleOpenAIConfig] = None) -> OpenAIClient:
    """
    è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Args:
        config: OpenAIé…ç½®å¯¹è±¡

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    global _global_client

    if _global_client is None or config is not None:
        _global_client = OpenAIClient(config)

    return _global_client

