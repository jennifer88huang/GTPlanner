"""
OpenAI SDKå°è£…å±‚

æä¾›ç»Ÿä¸€çš„OpenAI SDKæ¥å£ï¼Œä¿æŒä¸ç°æœ‰APIçš„å…¼å®¹æ€§ï¼Œæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥è°ƒç”¨ã€‚
é›†æˆé…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’ŒFunction CallingåŠŸèƒ½ã€‚
"""

import asyncio
import json
import time
import random
from typing import Dict, List, Any, Optional, AsyncIterator, Iterator, Union, Callable
from openai import AsyncOpenAI, OpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from openai.types.chat.chat_completion_chunk import ChoiceDelta

from config.openai_config import get_openai_config, OpenAIConfig


class OpenAIClientError(Exception):
    """OpenAIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»"""
    pass


class OpenAIRateLimitError(OpenAIClientError):
    """APIé€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass


class OpenAITimeoutError(OpenAIClientError):
    """APIè¶…æ—¶é”™è¯¯"""
    pass


class OpenAIRetryableError(OpenAIClientError):
    """å¯é‡è¯•çš„APIé”™è¯¯"""
    pass


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå‡½æ•°å¹¶åœ¨å¤±è´¥æ—¶é‡è¯•

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

            except Exception as e:
                last_error = e

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    break

                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt)

                print(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                print(f"ğŸ”„ {delay:.1f}ç§’åé‡è¯•...")

                await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise last_error

    def _should_retry(self, error: Exception, attempt: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        Args:
            error: é”™è¯¯å¯¹è±¡
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        if attempt >= self.max_retries:
            return False

        error_str = str(error).lower()

        # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
        retryable_errors = [
            "rate_limit",
            "timeout",
            "connection",
            "network",
            "server_error",
            "503",
            "502",
            "500",
            "429"
        ]

        return any(err in error_str for err in retryable_errors)

    def _calculate_delay(self, attempt: int) -> float:
        """
        è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰

        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        import random

        # æŒ‡æ•°é€€é¿
        delay = self.base_delay * (2 ** attempt)

        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆÂ±25%ï¼‰
        jitter = delay * 0.25 * (random.random() * 2 - 1)

        return max(0.1, delay + jitter)


class OpenAIClient:
    """OpenAI SDKå°è£…å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[OpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: OpenAIé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or get_openai_config()

        # åˆ›å»ºå¼‚æ­¥å’ŒåŒæ­¥å®¢æˆ·ç«¯
        client_kwargs = self.config.to_openai_client_kwargs()
        self.async_client = AsyncOpenAI(**client_kwargs)
        self.sync_client = OpenAI(**client_kwargs)

        # åˆ›å»ºé‡è¯•ç®¡ç†å™¨
        self.retry_manager = RetryManager(
            max_retries=self.config.max_retries,
            base_delay=self.config.retry_delay
        )

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "retry_attempts": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }

        # å…¨å±€ç³»ç»Ÿæç¤ºè¯
        self.global_system_prompt = "å¦‚æœæ˜¯JSONè¾“å‡ºï¼Œæœ€ç»ˆè¾“å‡ºåªåŒ…å«JSONæ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—åŒ…è£¹"

    def _prepare_messages_with_global_system_prompt(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        ä¸ºæ¶ˆæ¯åˆ—è¡¨æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ·»åŠ äº†å…¨å±€ç³»ç»Ÿæç¤ºè¯çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not messages:
            return [{"role": "system", "content": self.global_system_prompt}]

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç³»ç»Ÿæ¶ˆæ¯
        has_system_message = any(msg.get("role") == "system" for msg in messages)

        if has_system_message:
            # å¦‚æœå·²æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåœ¨ç¬¬ä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯å‰æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯
            prepared_messages = []
            global_system_added = False

            for msg in messages:
                if msg.get("role") == "system" and not global_system_added:
                    # åœ¨ç¬¬ä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯å‰æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯
                    prepared_messages.append({"role": "system", "content": self.global_system_prompt})
                    global_system_added = True
                prepared_messages.append(msg)

            return prepared_messages
        else:
            # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåœ¨å¼€å¤´æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯
            return [{"role": "system", "content": self.global_system_prompt}] + messages

    async def chat_completion_async(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©å®Œæˆè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯ï¼‰
            prepared_messages = self._prepare_messages_with_global_system_prompt(messages)

            # åˆå¹¶é…ç½®å‚æ•°
            params = self.config.to_chat_completion_kwargs()
            params.update(kwargs)
            params["messages"] = prepared_messages

            # æ·»åŠ å·¥å…·æ”¯æŒ
            if tools and self.config.function_calling_enabled:
                params["tools"] = tools
                if "tool_choice" not in params:
                    params["tool_choice"] = self.config.tool_choice

            # è®°å½•è¯·æ±‚æ—¥å¿—
            if self.config.log_requests:
                self._log_request("chat_completion", params)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒAPIè°ƒç”¨
            async def _api_call():
                return await self.async_client.chat.completions.create(**params)

            response = await self.retry_manager.execute_with_retry(_api_call)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["successful_requests"] += 1
            if hasattr(response, 'usage') and response.usage:
                self.stats["total_tokens"] += response.usage.total_tokens

            # è®°å½•å“åº”æ—¥å¿—
            if self.config.log_responses:
                self._log_response("chat_completion", response)

            return response

        except Exception as e:
            self.stats["failed_requests"] += 1
            raise self._handle_error(e)
        
        finally:
            self.stats["total_time"] += time.time() - start_time
    
    async def chat_completion_stream_async(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯ï¼‰
            prepared_messages = self._prepare_messages_with_global_system_prompt(messages)

            # åˆå¹¶é…ç½®å‚æ•°
            params = self.config.to_chat_completion_kwargs()
            params.update(kwargs)
            params["messages"] = prepared_messages
            params["stream"] = True
            
            # æ·»åŠ å·¥å…·æ”¯æŒ
            if tools and self.config.function_calling_enabled:
                params["tools"] = tools
                if "tool_choice" not in params:
                    params["tool_choice"] = self.config.tool_choice
            
            # è®°å½•è¯·æ±‚æ—¥å¿—
            if self.config.log_requests:
                self._log_request("chat_completion_stream", params)
            
            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = await self.async_client.chat.completions.create(**params)
            
            async for chunk in stream:
                yield chunk
            
            self.stats["successful_requests"] += 1
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            raise self._handle_error(e)
        
        finally:
            self.stats["total_time"] += time.time() - start_time
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        åŒæ­¥èŠå¤©å®Œæˆè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯ï¼‰
            prepared_messages = self._prepare_messages_with_global_system_prompt(messages)

            # åˆå¹¶é…ç½®å‚æ•°
            params = self.config.to_chat_completion_kwargs()
            params.update(kwargs)
            params["messages"] = prepared_messages
            
            # æ·»åŠ å·¥å…·æ”¯æŒ
            if tools and self.config.function_calling_enabled:
                params["tools"] = tools
                if "tool_choice" not in params:
                    params["tool_choice"] = self.config.tool_choice
            
            # è®°å½•è¯·æ±‚æ—¥å¿—
            if self.config.log_requests:
                self._log_request("chat_completion", params)
            
            # æ‰§è¡ŒAPIè°ƒç”¨
            response = self.sync_client.chat.completions.create(**params)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["successful_requests"] += 1
            if hasattr(response, 'usage') and response.usage:
                self.stats["total_tokens"] += response.usage.total_tokens
            
            # è®°å½•å“åº”æ—¥å¿—
            if self.config.log_responses:
                self._log_response("chat_completion", response)
            
            return response
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            raise self._handle_error(e)
        
        finally:
            self.stats["total_time"] += time.time() - start_time
    
    def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> Iterator[ChatCompletionChunk]:
        """
        åŒæ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯ï¼‰
            prepared_messages = self._prepare_messages_with_global_system_prompt(messages)

            # åˆå¹¶é…ç½®å‚æ•°
            params = self.config.to_chat_completion_kwargs()
            params.update(kwargs)
            params["messages"] = prepared_messages
            params["stream"] = True
            
            # æ·»åŠ å·¥å…·æ”¯æŒ
            if tools and self.config.function_calling_enabled:
                params["tools"] = tools
                if "tool_choice" not in params:
                    params["tool_choice"] = self.config.tool_choice
            
            # è®°å½•è¯·æ±‚æ—¥å¿—
            if self.config.log_requests:
                self._log_request("chat_completion_stream", params)
            
            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = self.sync_client.chat.completions.create(**params)
            
            for chunk in stream:
                yield chunk
            
            self.stats["successful_requests"] += 1
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            raise self._handle_error(e)
        
        finally:
            self.stats["total_time"] += time.time() - start_time
    
    def _handle_error(self, error: Exception) -> OpenAIClientError:
        """
        å¤„ç†å’Œè½¬æ¢é”™è¯¯

        Args:
            error: åŸå§‹é”™è¯¯

        Returns:
            è½¬æ¢åçš„é”™è¯¯
        """
        import openai

        # OpenAI SDKç‰¹å®šé”™è¯¯
        if isinstance(error, openai.RateLimitError):
            return OpenAIRateLimitError(f"API rate limit exceeded: {error}")

        if isinstance(error, openai.APITimeoutError):
            return OpenAITimeoutError(f"API request timeout: {error}")

        if isinstance(error, openai.APIConnectionError):
            return OpenAIRetryableError(f"API connection error: {error}")

        if isinstance(error, openai.InternalServerError):
            return OpenAIRetryableError(f"Internal server error: {error}")

        if isinstance(error, openai.BadRequestError):
            return OpenAIClientError(f"Bad request: {error}")

        if isinstance(error, openai.AuthenticationError):
            return OpenAIClientError(f"Authentication failed: {error}")

        if isinstance(error, openai.PermissionDeniedError):
            return OpenAIClientError(f"Permission denied: {error}")

        if isinstance(error, openai.NotFoundError):
            return OpenAIClientError(f"Resource not found: {error}")

        # é€šç”¨é”™è¯¯å¤„ç†
        error_message = str(error)

        # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "rate_limit" in error_message.lower() or "429" in error_message:
            return OpenAIRateLimitError(f"API rate limit exceeded: {error_message}")

        # è¶…æ—¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "timeout" in error_message.lower() or "timed out" in error_message.lower():
            return OpenAITimeoutError(f"API request timeout: {error_message}")

        # ç½‘ç»œé”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(keyword in error_message.lower() for keyword in ["connection", "network", "dns"]):
            return OpenAIRetryableError(f"Network error: {error_message}")

        # æœåŠ¡å™¨é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(code in error_message for code in ["500", "502", "503", "504"]):
            return OpenAIRetryableError(f"Server error: {error_message}")

        # å…¶ä»–é”™è¯¯
        return OpenAIClientError(f"OpenAI API error: {error_message}")
    
    def _log_request(self, method: str, params: Dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        if self.config.debug_enabled:
            # éšè—æ•æ„Ÿä¿¡æ¯
            safe_params = params.copy()
            if "messages" in safe_params:
                safe_params["messages"] = f"[{len(safe_params['messages'])} messages]"
            
            print(f"ğŸ”„ OpenAI {method} request: {safe_params}")
    
    def _log_response(self, method: str, response: Any) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        if self.config.debug_enabled:
            if hasattr(response, 'usage') and response.usage:
                print(f"âœ… OpenAI {method} response: {response.usage.total_tokens} tokens")
            else:
                print(f"âœ… OpenAI {method} response received")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_client: Optional[OpenAIClient] = None


def get_openai_client(config: Optional[OpenAIConfig] = None) -> OpenAIClient:
    """
    è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Args:
        config: OpenAIé…ç½®å¯¹è±¡

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    global _global_client

    if _global_client is None or config is not None:
        _global_client = OpenAIClient(config)

    return _global_client





# ============================================================================
# Function Callingå·¥å…·è°ƒç”¨æ”¯æŒ
# ============================================================================

class FunctionCallResult:
    """Function Callingè°ƒç”¨ç»“æœ"""

    def __init__(
        self,
        function_name: str,
        arguments: Dict[str, Any],
        result: Any,
        success: bool = True,
        error: Optional[str] = None
    ):
        self.function_name = function_name
        self.arguments = arguments
        self.result = result
        self.success = success
        self.error = error

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "function_name": self.function_name,
            "arguments": self.arguments,
            "result": self.result,
            "success": self.success,
            "error": self.error
        }


async def execute_function_calls(
    messages: List[Dict[str, str]],
    tools: List[Dict],
    tool_executor: Callable[[str, Dict[str, Any]], Any],
    max_iterations: int = 5
) -> List[FunctionCallResult]:
    """
    æ‰§è¡ŒFunction Callingè°ƒç”¨

    Args:
        messages: æ¶ˆæ¯å†å²
        tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        tool_executor: å·¥å…·æ‰§è¡Œå™¨å‡½æ•°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

    Returns:
        å·¥å…·è°ƒç”¨ç»“æœåˆ—è¡¨
    """
    client = get_openai_client()
    results = []
    current_messages = messages.copy()

    for _ in range(max_iterations):
        # è°ƒç”¨LLM
        response = await client.chat_completion_async(
            current_messages,
            tools=tools,
            tool_choice="auto"
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if not response.choices[0].message.tool_calls:
            break

        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
        current_messages.append({
            "role": "assistant",
            "content": response.choices[0].message.content or "",
            "tool_calls": [
                {
                    "id": tc.id,
                    "type": tc.type,
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments
                    }
                }
                for tc in response.choices[0].message.tool_calls
            ]
        })

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        for tool_call in response.choices[0].message.tool_calls:
            try:
                # è§£æå‚æ•°
                arguments = json.loads(tool_call.function.arguments)

                # æ‰§è¡Œå·¥å…·
                result = await tool_executor(tool_call.function.name, arguments)

                # è®°å½•ç»“æœ
                function_result = FunctionCallResult(
                    function_name=tool_call.function.name,
                    arguments=arguments,
                    result=result,
                    success=True
                )
                results.append(function_result)

                # æ·»åŠ å·¥å…·ç»“æœæ¶ˆæ¯
                current_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result, ensure_ascii=False)
                })

            except Exception as e:
                # è®°å½•é”™è¯¯
                function_result = FunctionCallResult(
                    function_name=tool_call.function.name,
                    arguments=json.loads(tool_call.function.arguments) if tool_call.function.arguments else {},
                    result=None,
                    success=False,
                    error=str(e)
                )
                results.append(function_result)

                # æ·»åŠ é”™è¯¯æ¶ˆæ¯
                current_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": f"Error: {str(e)}"
                })

    return results
