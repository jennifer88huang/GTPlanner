"""
OpenAI SDKå°è£…å±‚

æä¾›ç»Ÿä¸€çš„OpenAI SDKå¼‚æ­¥æ¥å£ï¼Œé›†æˆé…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶å’ŒFunction CallingåŠŸèƒ½ã€‚
"""

import asyncio
import os
import time
from typing import Dict, List, Any, Optional, AsyncIterator, Callable, TypedDict
import copy
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk

from utils.logger_config import get_openai_logger

try:
    from dynaconf import Dynaconf
    DYNACONF_AVAILABLE = True
except ImportError:
    DYNACONF_AVAILABLE = False


class Message(TypedDict):
    """æ¶ˆæ¯ç±»å‹å®šä¹‰"""
    role: str
    content: str


class ToolCallTagFilter:
    """
    å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨å’Œè½¬æ¢å™¨ï¼ˆçŠ¶æ€æœºæ¨¡å¼ï¼‰

    ä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼å¤„ç†è·¨chunkçš„å·¥å…·è°ƒç”¨æ ‡ç­¾åˆ†å‰²æƒ…å†µï¼Œ
    æ”¯æŒæ ‡ç­¾è¢«ä»»æ„åˆ†å‰²çš„è¾¹ç•Œæƒ…å†µï¼ˆå¦‚ chunk1="<tool_", chunk2="call>{"name""ï¼‰
    """

    def __init__(self):
        # çŠ¶æ€æœºçŠ¶æ€
        self.state = "NORMAL"  # NORMAL, COLLECTING_START_TAG, IN_TOOL_CALL, COLLECTING_END_TAG

        # æ ‡ç­¾å®šä¹‰
        self.start_tag = "<tool_call>"
        self.end_tag = "</tool_call>"

        # æ ‡ç­¾æ”¶é›†ç¼“å†²åŒº
        self.tag_buffer = ""
        self.tag_target = ""

        # å†…å®¹ç¼“å†²åŒº
        self.content_buffer = ""
        self.tool_call_content = ""

        # è¾“å‡ºç¼“å†²åŒº
        self.output_buffer = ""

        # æå–çš„å·¥å…·è°ƒç”¨
        self.extracted_tool_calls = []

    def process_chunk(self, content: str) -> str:
        """
        å¤„ç†æµå¼å†…å®¹å—ï¼Œä½¿ç”¨çŠ¶æ€æœºæ¨¡å¼è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾

        Args:
            content: åŸå§‹å†…å®¹å—

        Returns:
            è¿‡æ»¤åçš„å¯æ˜¾ç¤ºå†…å®¹
        """
        if not content:
            return ""

        output = ""

        for char in content:
            if self.state == "NORMAL":
                output += self._process_normal_char(char)
            elif self.state == "COLLECTING_START_TAG":
                output += self._process_start_tag_char(char)
            elif self.state == "IN_TOOL_CALL":
                self._process_tool_call_char(char)
            elif self.state == "COLLECTING_END_TAG":
                self._process_end_tag_char(char)

        return output

    def _process_normal_char(self, char: str) -> str:
        """å¤„ç†æ­£å¸¸çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¼€å§‹æ”¶é›†å¼€å§‹æ ‡ç­¾
            self.state = "COLLECTING_START_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.start_tag
            return ""  # ä¸è¾“å‡ºï¼Œç­‰å¾…ç¡®è®¤æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨æ ‡ç­¾
        else:
            return char

    def _process_start_tag_char(self, char: str) -> str:
        """å¤„ç†å¼€å§‹æ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…å¼€å§‹æ ‡ç­¾
                    self.state = "IN_TOOL_CALL"
                    self.tool_call_content = ""
                    self.tag_buffer = ""
                    return ""  # ä¸è¾“å‡ºæ ‡ç­¾
                else:
                    # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
                    return ""
            else:
                # ä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
                output = self.tag_buffer
                self.state = "NORMAL"
                self.tag_buffer = ""
                return output
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹å¹¶å›åˆ°æ­£å¸¸çŠ¶æ€
            output = self.tag_buffer
            self.state = "NORMAL"
            self.tag_buffer = ""
            return output

    def _process_tool_call_char(self, char: str):
        """å¤„ç†å·¥å…·è°ƒç”¨å†…å®¹çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        if char == '<':
            # å¯èƒ½æ˜¯ç»“æŸæ ‡ç­¾çš„å¼€å§‹
            self.state = "COLLECTING_END_TAG"
            self.tag_buffer = '<'
            self.tag_target = self.end_tag
        else:
            self.tool_call_content += char

    def _process_end_tag_char(self, char: str):
        """å¤„ç†ç»“æŸæ ‡ç­¾æ”¶é›†çŠ¶æ€ä¸‹çš„å­—ç¬¦"""
        self.tag_buffer += char

        if len(self.tag_buffer) <= len(self.tag_target):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®æ ‡æ ‡ç­¾
            if self.tag_buffer == self.tag_target[:len(self.tag_buffer)]:
                if self.tag_buffer == self.tag_target:
                    # å®Œæ•´åŒ¹é…ç»“æŸæ ‡ç­¾ï¼Œå®Œæˆå·¥å…·è°ƒç”¨æå–
                    self._parse_and_store_tool_call(self.tool_call_content)
                    self.state = "NORMAL"
                    self.tag_buffer = ""
                    self.tool_call_content = ""
                # éƒ¨åˆ†åŒ¹é…ï¼Œç»§ç»­æ”¶é›†
            else:
                # ä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹ï¼Œç»§ç»­æ”¶é›†å·¥å…·è°ƒç”¨
                self.tool_call_content += self.tag_buffer
                self.state = "IN_TOOL_CALL"
                self.tag_buffer = ""
        else:
            # è¶…å‡ºæ ‡ç­¾é•¿åº¦ï¼Œä¸åŒ¹é…ï¼Œå°†ç¼“å†²çš„å†…å®¹åŠ å…¥å·¥å…·è°ƒç”¨å†…å®¹
            self.tool_call_content += self.tag_buffer
            self.state = "IN_TOOL_CALL"
            self.tag_buffer = ""

    def finalize(self) -> str:
        """
        å®Œæˆå¤„ç†ï¼Œè¿”å›å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹

        Returns:
            å‰©ä½™çš„å¯æ˜¾ç¤ºå†…å®¹
        """
        output = ""

        # å¤„ç†æœªå®Œæˆçš„çŠ¶æ€
        if self.state == "COLLECTING_START_TAG":
            # æœªå®Œæˆçš„å¼€å§‹æ ‡ç­¾æ”¶é›†ï¼Œè¾“å‡ºç¼“å†²çš„å†…å®¹
            output += self.tag_buffer
        elif self.state == "IN_TOOL_CALL":
            # æœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼Œä¸è¾“å‡ºï¼ˆå·¥å…·è°ƒç”¨ä¸å®Œæ•´ï¼‰
            pass
        elif self.state == "COLLECTING_END_TAG":
            # æœªå®Œæˆçš„ç»“æŸæ ‡ç­¾æ”¶é›†ï¼Œå°†ç¼“å†²å†…å®¹ä½œä¸ºå·¥å…·è°ƒç”¨å†…å®¹çš„ä¸€éƒ¨åˆ†
            # ä½†ç”±äºå·¥å…·è°ƒç”¨æœªå®Œæˆï¼Œä¸è¾“å‡º
            pass

        # é‡ç½®çŠ¶æ€
        self.state = "NORMAL"
        self.tag_buffer = ""
        self.tool_call_content = ""

        return output

    def _parse_and_store_tool_call(self, tool_call_content: str) -> None:
        """
        è§£æå·¥å…·è°ƒç”¨å†…å®¹å¹¶å­˜å‚¨ä¸ºæ ‡å‡†æ ¼å¼

        Args:
            tool_call_content: å·¥å…·è°ƒç”¨çš„JSONå†…å®¹
        """
        import json
        import uuid

        try:
            # è§£æJSONå†…å®¹
            tool_call_data = json.loads(tool_call_content)

            # éªŒè¯å¿…éœ€å­—æ®µ
            if "name" not in tool_call_data:
                return

            # ç”Ÿæˆå”¯ä¸€çš„call_id
            call_id = f"call_{uuid.uuid4().hex[:8]}"

            # ç¡®ä¿argumentsæ˜¯å­—ç¬¦ä¸²æ ¼å¼
            arguments = tool_call_data.get("arguments", {})
            if isinstance(arguments, dict):
                arguments_str = json.dumps(arguments, ensure_ascii=False)
            else:
                arguments_str = str(arguments)

            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨
            standard_tool_call = {
                "id": call_id,
                "type": "function",
                "function": {
                    "name": tool_call_data["name"],
                    "arguments": arguments_str
                }
            }

            self.extracted_tool_calls.append(standard_tool_call)

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            # è§£æå¤±è´¥ï¼Œå¿½ç•¥è¿™ä¸ªå·¥å…·è°ƒç”¨
            pass

    def get_extracted_tool_calls(self) -> list:
        """
        è·å–æå–çš„å·¥å…·è°ƒç”¨åˆ—è¡¨

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        return self.extracted_tool_calls.copy()




class SimpleOpenAIConfig:
    """ç®€åŒ–çš„OpenAIé…ç½®ç±»"""

    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-4",
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        timeout: float = 120.0,
        max_retries: int = 3,
        retry_delay: float = 2.0,
        log_requests: bool = True,
        log_responses: bool = True,
        function_calling_enabled: bool = True,
        tool_choice: str = "auto",
    ):
        # å°è¯•ä» settings.toml åŠ è½½é…ç½®
        settings = self._load_settings()

        self.api_key = api_key or self._get_setting(settings, "llm.api_key") or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or self._get_setting(settings, "llm.base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = self._get_setting(settings, "llm.model") or os.getenv("OPENAI_MODEL", model)
        self.temperature = self._get_setting(settings, "llm.temperature", temperature)
        self.max_tokens = self._get_setting(settings, "llm.max_tokens", max_tokens)
        self.timeout = self._get_setting(settings, "llm.timeout", timeout)
        self.max_retries = self._get_setting(settings, "llm.max_retries", max_retries)
        self.retry_delay = self._get_setting(settings, "llm.retry_delay", retry_delay)
        self.log_requests = self._get_setting(settings, "llm.log_requests", log_requests)
        self.log_responses = self._get_setting(settings, "llm.log_responses", log_responses)
        self.function_calling_enabled = self._get_setting(settings, "llm.function_calling_enabled", function_calling_enabled)
        self.tool_choice = self._get_setting(settings, "llm.tool_choice", tool_choice)

        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or configure llm.api_key in settings.toml.")

    def _load_settings(self):
        """åŠ è½½ Dynaconf è®¾ç½®"""
        if not DYNACONF_AVAILABLE:
            return None

        try:
            return Dynaconf(
                settings_files=["settings.toml", "settings.local.toml", ".secrets.toml"],
                environments=True,
                env_switcher="ENV_FOR_DYNACONF",
                load_dotenv=True,
            )
        except Exception:
            return None

    def _get_setting(self, settings, key: str, default: Any = None) -> Any:
        """ä»è®¾ç½®ä¸­è·å–å€¼"""
        if settings is None:
            return default

        try:
            return settings.get(key, default)
        except Exception:
            return default

    def to_openai_client_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å‚æ•°"""
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "timeout": self.timeout,
            "max_retries": self.max_retries,
        }

    def to_chat_completion_kwargs(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºchat completionè°ƒç”¨å‚æ•°"""
        kwargs = {
            "model": self.model,
            "temperature": self.temperature,
        }

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        return kwargs


class OpenAIClientError(Exception):
    """OpenAIå®¢æˆ·ç«¯é”™è¯¯åŸºç±»"""
    pass


class OpenAIRateLimitError(OpenAIClientError):
    """APIé€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass


class OpenAITimeoutError(OpenAIClientError):
    """APIè¶…æ—¶é”™è¯¯"""
    pass


class OpenAIRetryableError(OpenAIClientError):
    """å¯é‡è¯•çš„APIé”™è¯¯"""
    pass


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œå‡½æ•°å¹¶åœ¨å¤±è´¥æ—¶é‡è¯•

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            *args: å‡½æ•°å‚æ•°
            **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None

        for attempt in range(self.max_retries + 1):
            try:
                if asyncio.iscoroutinefunction(func):
                    return await func(*args, **kwargs)
                else:
                    return func(*args, **kwargs)

            except Exception as e:
                last_error = e

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    break

                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt)

                # ä½¿ç”¨æ—¥å¿—è®°å½•é‡è¯•ä¿¡æ¯
                from utils.logger_config import get_logger
                logger = get_logger("retry_manager")
                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                logger.info(f"ğŸ”„ ç­‰å¾… {delay:.1f}ç§’åé‡è¯•...")

                await asyncio.sleep(delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise last_error

    def _should_retry(self, error: Exception, attempt: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•

        Args:
            error: é”™è¯¯å¯¹è±¡
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            æ˜¯å¦åº”è¯¥é‡è¯•
        """
        if attempt >= self.max_retries:
            return False

        error_str = str(error).lower()

        # å¯é‡è¯•çš„é”™è¯¯ç±»å‹
        retryable_errors = [
            "rate_limit",
            "timeout",
            "connection",
            "network",
            "server_error",
            "503",
            "502",
            "500",
            "429"
        ]

        return any(err in error_str for err in retryable_errors)

    def _calculate_delay(self, attempt: int) -> float:
        """
        è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰

        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°

        Returns:
            å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        import random

        # æŒ‡æ•°é€€é¿
        delay = self.base_delay * (2 ** attempt)

        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆÂ±25%ï¼‰
        jitter = delay * 0.25 * (random.random() * 2 - 1)

        return max(0.1, delay + jitter)


class OpenAIClient:
    """OpenAI SDKå°è£…å®¢æˆ·ç«¯"""

    def __init__(self, config: Optional[SimpleOpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: OpenAIé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or SimpleOpenAIConfig()

        # è·å–æ—¥å¿—å™¨ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼‰
        self.logger = get_openai_logger()

        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        client_kwargs = self.config.to_openai_client_kwargs()
        self.async_client = AsyncOpenAI(**client_kwargs)

        # åˆ›å»ºé‡è¯•ç®¡ç†å™¨
        self.retry_manager = RetryManager(
            max_retries=self.config.max_retries,
            base_delay=self.config.retry_delay
        )

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }

        # å…¨å±€ç³»ç»Ÿæç¤ºè¯
        self.global_system_prompt = "å¦‚æœæ˜¯JSONè¾“å‡ºï¼Œæœ€ç»ˆè¾“å‡ºåªåŒ…å«JSONæ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—åŒ…è£¹"

        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        self.logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.config.model}, åŸºç¡€URL: {self.config.base_url}")

    def _prepare_messages(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None
    ) -> List[Message]:
        """
        å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼Œç»Ÿä¸€å¤„ç†ç³»ç»Ÿæç¤ºè¯å’Œå…¨å±€ç³»ç»Ÿæç¤ºè¯

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            å‡†å¤‡å¥½çš„æ¶ˆæ¯åˆ—è¡¨
        """
        prepared_messages = []

        # æ·»åŠ å…¨å±€ç³»ç»Ÿæç¤ºè¯
        if self.global_system_prompt:
            prepared_messages.append({"role": "system", "content": self.global_system_prompt})

        # æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
        if system_prompt:
            prepared_messages.append({"role": "system", "content": system_prompt})

        # æ·»åŠ å¯¹è¯æ¶ˆæ¯
        if messages:
            prepared_messages.extend(messages)

        return prepared_messages

    def _prepare_request_params(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‡†å¤‡APIè¯·æ±‚å‚æ•°

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å·¥å…·åˆ—è¡¨
            stream: æ˜¯å¦æµå¼å“åº”
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å‡†å¤‡å¥½çš„è¯·æ±‚å‚æ•°
        """
        # å‡†å¤‡æ¶ˆæ¯
        prepared_messages = self._prepare_messages(messages, system_prompt)

        # åˆå¹¶é…ç½®å‚æ•°
        params = self.config.to_chat_completion_kwargs()

        # è¿‡æ»¤æ‰å†…éƒ¨å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ['filter_tool_tags']}
        params.update(filtered_kwargs)
        params["messages"] = prepared_messages

        if stream:
            params["stream"] = True

        # æ·»åŠ å·¥å…·æ”¯æŒ
        if tools and self.config.function_calling_enabled:
            params["tools"] = tools
            if "tool_choice" not in params:
                params["tool_choice"] = self.config.tool_choice

        return params

    def _update_success_stats(self, response: Any) -> None:
        """æ›´æ–°æˆåŠŸç»Ÿè®¡ä¿¡æ¯"""
        self.stats["successful_requests"] += 1
        if hasattr(response, 'usage') and response.usage:
            self.stats["total_tokens"] += response.usage.total_tokens

    def _update_failure_stats(self) -> None:
        """æ›´æ–°å¤±è´¥ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["failed_requests"] += 1



    async def chat_completion(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> ChatCompletion:
        """
        å¼‚æ­¥èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            èŠå¤©å®Œæˆå“åº”
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion", params)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡ŒAPIè°ƒç”¨
            async def _api_call():
                return await self.async_client.chat.completions.create(**params)

            response = await self.retry_manager.execute_with_retry(_api_call)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_success_stats(response)

            # è®°å½•å“åº”æ—¥å¿—
            self._log_response("chat_completion", response)

            return response

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time
    
    async def chat_completion_stream(
        self,
        messages: Optional[List[Message]] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        filter_tool_tags: bool = False,
        **kwargs
    ) -> AsyncIterator[ChatCompletionChunk]:
        """
        å¼‚æ­¥æµå¼èŠå¤©å®Œæˆè°ƒç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tools: Function Callingå·¥å…·åˆ—è¡¨
            filter_tool_tags: æ˜¯å¦è¿‡æ»¤å·¥å…·è°ƒç”¨æ ‡ç­¾ï¼ˆé»˜è®¤Falseï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            èŠå¤©å®Œæˆæµå¼å“åº”å—ï¼ˆå¦‚æœå¯ç”¨filter_tool_tagsï¼Œdelta.contentå°†è¢«è¿‡æ»¤ï¼‰
        """
        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # æå–filter_tool_tagså‚æ•°ï¼Œé¿å…ä¼ é€’ç»™OpenAI API
            filter_tool_tags_param = filter_tool_tags

        

            # å‡†å¤‡è¯·æ±‚å‚æ•°ï¼ˆä¸åŒ…å«filter_tool_tagsï¼‰
            params = self._prepare_request_params(
                messages=messages,
                system_prompt=system_prompt,
                tools=tools,
                stream=True,
                **kwargs
            )

            # è®°å½•è¯·æ±‚æ—¥å¿—
            self._log_request("chat_completion_stream", params)

            # æ‰§è¡Œæµå¼APIè°ƒç”¨
            stream = await self.async_client.chat.completions.create(**params)

            chunk_count = 0
            full_content = ""
            total_tokens = 0

            # åˆå§‹åŒ–å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tag_filter = ToolCallTagFilter() if filter_tool_tags_param else None

            async for chunk in stream:
                chunk_count += 1

                # æ”¶é›†å“åº”å†…å®¹ç”¨äºæ—¥å¿—è®°å½•
                if chunk.choices and chunk.choices[0].delta.content:
                    full_content += chunk.choices[0].delta.content

                # æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
                if hasattr(chunk, 'usage') and chunk.usage:
                    total_tokens = chunk.usage.total_tokens



                # å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨æ ‡ç­¾è¿‡æ»¤ï¼Œå¤„ç†delta.content
                if filter_tool_tags_param and tag_filter and chunk.choices and chunk.choices[0].delta.content:
                    # åˆ›å»ºchunkçš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡
                    filtered_chunk = copy.deepcopy(chunk)
                    original_content = chunk.choices[0].delta.content
                    filtered_content = tag_filter.process_chunk(original_content)

                    # æ›´æ–°å‰¯æœ¬çš„content
                    filtered_chunk.choices[0].delta.content = filtered_content

                    # å¦‚æœæå–åˆ°äº†å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°delta.tool_calls
                    extracted_calls = tag_filter.get_extracted_tool_calls()
                    if extracted_calls and not filtered_chunk.choices[0].delta.tool_calls:
                        # å°†æå–çš„å·¥å…·è°ƒç”¨è½¬æ¢ä¸ºdeltaæ ¼å¼
                        tool_call_deltas = []
                        for i, tool_call in enumerate(extracted_calls):
                            from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction

                            tool_call_delta = ChoiceDeltaToolCall(
                                index=i,
                                id=tool_call["id"],
                                function=ChoiceDeltaToolCallFunction(
                                    name=tool_call["function"]["name"],
                                    arguments=tool_call["function"]["arguments"]
                                ),
                                type="function"
                            )
                            tool_call_deltas.append(tool_call_delta)

                        filtered_chunk.choices[0].delta.tool_calls = tool_call_deltas
                        # æ¸…ç©ºå·²å¤„ç†çš„å·¥å…·è°ƒç”¨ï¼Œé¿å…é‡å¤
                        tag_filter.extracted_tool_calls.clear()

                    yield filtered_chunk
                else:
                    yield chunk

            # å¦‚æœå¯ç”¨äº†è¿‡æ»¤ï¼Œå¤„ç†å‰©ä½™çš„å†…å®¹
            if filter_tool_tags_param and tag_filter:
                remaining_content = tag_filter.finalize()
                if remaining_content:
                    # åˆ›å»ºä¸€ä¸ªåŒ…å«å‰©ä½™å†…å®¹çš„chunk
                    from openai.types.chat.chat_completion_chunk import ChatCompletionChunk, Choice, ChoiceDelta

                    # åˆ›å»ºæœ€åä¸€ä¸ªchunkæ¥è¾“å‡ºå‰©ä½™å†…å®¹
                    final_choice = Choice(
                        delta=ChoiceDelta(content=remaining_content),
                        index=0,
                        finish_reason=None
                    )
                    final_chunk = ChatCompletionChunk(
                        id="filtered_final",
                        choices=[final_choice],
                        created=int(time.time()),
                        model="filtered",
                        object="chat.completion.chunk"
                    )
                    yield final_chunk

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["successful_requests"] += 1
            if total_tokens > 0:
                self.stats["total_tokens"] += total_tokens

            # è®°å½•å“åº”æ—¥å¿—ï¼ˆæµå¼å“åº”ï¼‰
            self._log_stream_response("chat_completion_stream", chunk_count, full_content)

        except Exception as e:
            self._update_failure_stats()
            raise self._handle_error(e)

        finally:
            self.stats["total_time"] += time.time() - start_time




    
    def _handle_error(self, error: Exception) -> OpenAIClientError:
        """
        å¤„ç†å’Œè½¬æ¢é”™è¯¯

        Args:
            error: åŸå§‹é”™è¯¯

        Returns:
            è½¬æ¢åçš„é”™è¯¯
        """
        import openai

        # OpenAI SDKç‰¹å®šé”™è¯¯
        if isinstance(error, openai.RateLimitError):
            return OpenAIRateLimitError(f"API rate limit exceeded: {error}")

        if isinstance(error, openai.APITimeoutError):
            return OpenAITimeoutError(f"API request timeout: {error}")

        if isinstance(error, openai.APIConnectionError):
            return OpenAIRetryableError(f"API connection error: {error}")

        if isinstance(error, openai.InternalServerError):
            return OpenAIRetryableError(f"Internal server error: {error}")

        if isinstance(error, openai.BadRequestError):
            return OpenAIClientError(f"Bad request: {error}")

        if isinstance(error, openai.AuthenticationError):
            return OpenAIClientError(f"Authentication failed: {error}")

        if isinstance(error, openai.PermissionDeniedError):
            return OpenAIClientError(f"Permission denied: {error}")

        if isinstance(error, openai.NotFoundError):
            return OpenAIClientError(f"Resource not found: {error}")

        # é€šç”¨é”™è¯¯å¤„ç†
        error_message = str(error)

        # é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "rate_limit" in error_message.lower() or "429" in error_message:
            return OpenAIRateLimitError(f"API rate limit exceeded: {error_message}")

        # è¶…æ—¶é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if "timeout" in error_message.lower() or "timed out" in error_message.lower():
            return OpenAITimeoutError(f"API request timeout: {error_message}")

        # ç½‘ç»œé”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(keyword in error_message.lower() for keyword in ["connection", "network", "dns"]):
            return OpenAIRetryableError(f"Network error: {error_message}")

        # æœåŠ¡å™¨é”™è¯¯ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ï¼‰
        if any(code in error_message for code in ["500", "502", "503", "504"]):
            return OpenAIRetryableError(f"Server error: {error_message}")

        # å…¶ä»–é”™è¯¯
        return OpenAIClientError(f"OpenAI API error: {error_message}")
    
    def _log_request(self, method: str, params: Dict[str, Any]) -> None:
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        if self.config.log_requests:
            self.logger.info(f"ğŸ”„ OpenAI {method} è¯·æ±‚:")
            self.logger.info(f"ï¿½ å®Œæ•´è¯·æ±‚å‚æ•°: {params}")

    def _log_response(self, method: str, response: Any) -> None:
        """è®°å½•å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} å“åº”:")
            self.logger.info(f"ğŸ“ å®Œæ•´å“åº”: {response}")

    def _log_stream_response(self, method: str, chunk_count: int, content: str = "") -> None:
        """è®°å½•æµå¼å“åº”æ—¥å¿—"""
        if self.config.log_responses:
            self.logger.info(f"âœ… OpenAI {method} æµå¼å“åº”å®Œæˆ: æ¥æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
            if content:
                self.logger.info(f"ğŸ“ å®Œæ•´æµå¼å“åº”å†…å®¹: {content}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self) -> None:
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_global_client: Optional[OpenAIClient] = None


def get_openai_client(config: Optional[SimpleOpenAIConfig] = None) -> OpenAIClient:
    """
    è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Args:
        config: OpenAIé…ç½®å¯¹è±¡

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    global _global_client

    if _global_client is None or config is not None:
        _global_client = OpenAIClient(config)

    return _global_client

