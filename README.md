# GTPlanner: AI-Powered PRD Generation Tool

<p align="center">
  <img src="./assets/banner.png" width="800" alt="GTPlanner Banner"/>
</p>

<p align="center">
  <strong>An intelligent Product Requirement Document (PRD) generation tool that transforms natural language descriptions into structured technical documents suitable for Vibe coding.</strong>
</p>

<p align="center">
  <a href="#-overview">Overview</a> â€¢
  <a href="#-web-ui-recommended">Web UI</a> â€¢
  <a href="#mcp-integration">MCP Integration</a> â€¢
  <a href="#-quickstart">Quickstart</a> â€¢
  <a href="#-features">Features</a> â€¢
  <a href="#-environment-requirements-backend-and-cli">Environment Requirements</a> â€¢
  <a href="#-installation-backend-and-cli">Installation</a> â€¢
  <a href="#ï¸-usage">Usage</a> â€¢
  <a href="#ï¸-system-architecture">System Architecture</a> â€¢
  <a href="#-project-structure">Project Structure</a> â€¢
  <a href="#-dependencies">Dependencies</a> â€¢
  <a href="#-multilingual-support">Multilingual Support</a> â€¢
  <a href="#-contributing">Contributing</a> â€¢
  <a href="#-license">License</a> â€¢
  <a href="#-acknowledgements">Acknowledgements</a>
</p>

<p align="center">
  <strong>Languages:</strong>
  <a href="README.md">ğŸ‡ºğŸ‡¸ English</a> â€¢
  <a href="README_zh.md">ğŸ‡¨ğŸ‡³ ç®€ä½“ä¸­æ–‡</a> â€¢
  <a href="README_ja.md">ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª</a>
</p>

---

## ğŸ¯ Overview

GTPlanner is an advanced AI tool designed for "vibe coding," aimed at efficiently transforming high-level ideas and requirements into clearly structured and detailed technical documents. We recommend experiencing the full capabilities of GTPlanner through our modern **Web Interface**.

For developers looking for deep integration and custom development, we also provide a powerful backend engine. It features an asynchronous, node-based architecture and supports various usage methods, including an interactive CLI, REST API, and MCP service.

### ğŸš€ Core Features

- **ğŸ§  Intelligent Reasoning**: Provides intelligent task analysis and planning capabilities.
- **ğŸ”„ Streaming Response Experience**: Natively supports Server-Sent Events (SSE) for a real-time user interaction experience.
- **âš¡ Stateless Architecture**: Supports high concurrency and horizontal scaling with a stateless design, suitable for production environments.
- **ğŸ› ï¸ Function Calling**: Integrates with OpenAI Function Calling for intelligent tool invocation and task execution.
- **ğŸŒ Multi-Interface Support**: Offers multiple integration methods, including CLI, FastAPI REST API, and MCP services.

This project consists of two core parts:
- **ğŸ’» GTPlanner-frontend (Web UI)**: Provides a feature-rich and interactive online planning experience. (Recommended) [ğŸš€ Try the Live Demo!](https://the-agent-builder.com/)
- **âš™ï¸ GTPlanner (Backend)**: A powerful backend engine based on an Agent architecture, offering various integration methods like CLI and API.

## ğŸ’» Web UI (Recommended)

For the best and most convenient experience, we highly recommend using our Web UI. It provides a seamless AI planning workflow tailored for modern developers.

![GTPlanner Web UI](assets/web.gif)

**Core Advantages:**
- **Intelligent Planning Assistant**: Quickly generate complex system architectures and project plans with AI assistance.
- **Instant Document Generation**: Automatically create comprehensive technical documentation from your planning sessions.
- **Born for Vibe Coding**: Optimized output that perfectly adapts to modern AI development tools like Cursor, Windsurf, and GitHub Copilot.
- **Team Collaboration**: Supports exporting in multiple formats for easy sharing and collaboration with your team.

## MCP Integration
Plans generated by GTPlanner can be used directly in your favorite AI programming tools, seamlessly integrating into your development workflow:

- In Cherry Studio:
  - ![MCP usage in Cherry Studio](assets/Cherry_Studio_2025-06-24_01-05-49.png)
- In Cursor:
  - ![MCP usage in Cursor](assets/Cursor_2025-06-24_01-12-05.png)


---

## âš¡ Quickstart

Here's the **smoothest** and **ready-to-use** GTPlanner experience path â€” from 0 to generating your first PRD, just a few commands to complete.

### 1) Online Experience (No Installation Required)

* Open Web UI Online Demo: [ğŸš€ Try the Live Demo!](https://the-agent-builder.com/)
  ğŸ‘‰ Perfect for "getting a feel for the results" â€” a what-you-see-is-what-you-get planning and document generation experience.

### 2) Local Setup (5 Minutes to Get Started)

#### Environment Setup

* **Python â‰¥ 3.10** (3.11+ recommended)
* Package manager: **uv** (recommended) or **pip**
* Prepare an OpenAI-compatible LLM API Key (e.g., `OpenAI` / `Anthropic` / `Azure OpenAI` / self-hosted endpoints)

#### Clone and Install

```bash
git clone https://github.com/OpenSQZ/GTPlanner.git
cd GTPlanner

# Recommended: uv one-click install
uv sync

# Or use pip
pip install -e .
```

#### Configure API Key

GTPlanner supports multiple configuration methods. **Required environment variables** include:

```bash
# Core Configuration (Required)
export LLM_API_KEY="your-api-key-here"        # API Key
export LLM_BASE_URL="https://api.openai.com/v1"  # API Base URL
export LLM_MODEL="gpt-4"                       # Model Name

# Windows PowerShell users:
# $env:LLM_API_KEY="your-api-key-here"
# $env:LLM_BASE_URL="https://api.openai.com/v1"
# $env:LLM_MODEL="gpt-4"

# Optional Configuration
export JINA_API_KEY="your-jina-key"           # Jina AI search service key (for web search)

# Langfuse Configuration (Optional, for PocketFlow Tracing)
export LANGFUSE_SECRET_KEY="your-secret-key"  # Langfuse Secret Key
export LANGFUSE_PUBLIC_KEY="your-public-key"  # Langfuse Public Key  
export LANGFUSE_HOST="https://cloud.langfuse.com"  # Langfuse Host
```

##### Common Provider Configuration Examples

**OpenAI Official:**
```bash
export LLM_API_KEY="sk-your-openai-key"
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL="gpt-4"
```

**Azure OpenAI:**
```bash
export LLM_API_KEY="your-azure-key"
export LLM_BASE_URL="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
export LLM_MODEL="gpt-4"
```

**Proxy Services:**
```bash
export LLM_API_KEY="your-proxy-key"
export LLM_BASE_URL="https://api.chatanywhere.com.cn/v1"
export LLM_MODEL="gpt-4"
```

**Local Deployment:**
```bash
export LLM_API_KEY="local-key"
export LLM_BASE_URL="http://localhost:8000/v1"
export LLM_MODEL="your-local-model"
```

##### Langfuse Tracing Configuration (Optional but Recommended)

GTPlanner integrates PocketFlow Tracing for execution tracking. To enable:

**Method 1: Use Configuration Script (Recommended)**
```bash
# Run configuration wizard
bash configure_langfuse.sh
```

**Method 2: Manual Configuration**
1. Visit [Langfuse Cloud](https://cloud.langfuse.com) to register an account
2. Create a new project and get API keys
3. Set environment variables:
   ```bash
   export LANGFUSE_SECRET_KEY="sk-lf-..."
   export LANGFUSE_PUBLIC_KEY="pk-lf-..."
   export LANGFUSE_HOST="https://cloud.langfuse.com"
   ```

**Method 3: Temporarily Disable Tracing**
If you don't need tracing functionality temporarily, you can ignore Langfuse configuration. The system will automatically skip tracing.

> You can further configure other parameters in `settings.toml`. Default language is English, supports Chinese, Japanese, Spanish, French.

### 3) Method A: CLI One-Click Generate Your First PRD (Recommended)

#### Interactive Mode

```bash
python gtplanner.py
# or
python agent/cli/gtplanner_cli.py
```

After entering, directly input your requirements, for example:

```
Generate a PRD for an online course platform: user registration/login, course search, preview, purchase, learning progress and assignment grading.
```

> CLI comes with session management (/sessions, /load), streaming output, and multilingual interface.

#### Direct Execution (Non-Interactive)

```bash
python gtplanner.py "Design a project management platform with SaaS billing and team collaboration, and output PRD"
```

### 4) Method B: Start REST API (For Integrating with Your Own Frontend/Automation)

#### Start Service

```bash
uv run fastapi_main.py
# Default: http://0.0.0.0:11211
# Docs: http://0.0.0.0:11211/docs
```

#### One curl Command to Get Started (SSE/Streaming Agent API)

```bash
curl -X POST "http://127.0.0.1:11211/api/chat/agent" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "quickstart-demo",
    "dialogue_history": [
      {"role":"user","content":"Generate PRD for an e-commerce platform: SKU, shopping cart, coupons, inventory, payment and risk control"}
    ],
    "language": "en"
  }'
```

> This endpoint returns **SSE streaming** responses, using `StatelessGTPlanner` backend with tool execution status updates.

### 5) Method C: MCP Integration (Connect to Your AI IDE / Assistant)

#### Environment Configuration

MCP service requires the same environment variables as the main service. Please ensure you have set:

```bash
# Required environment variables (same as main service)
export LLM_API_KEY="your-api-key-here"
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL="gpt-4"

# Optional configuration
export JINA_API_KEY="your-jina-key"  # For web search functionality
```

#### Start Service

1. Start MCP service

   ```bash
   cd mcp
   uv sync
   uv run python mcp_service.py
   ```

   > **Note**: MCP service runs in an independent environment but inherits the main project's configuration. Ensure environment variables are correctly set before starting.

2. Configure in MCP client:

   ```json
   {
     "mcpServers": {
       "GT-planner": { 
         "command": "uv",
         "args": ["run", "python", "mcp_service.py"],
         "cwd": "/path/to/GTPlanner/mcp"
       }
     }
   }
   ```

   Or connect to running service directly:

   ```json
   {
     "mcpServers": {
       "GT-planner": { "url": "http://127.0.0.1:8001/mcp" }
     }
   }
   ```

3. Available tools:
   - `generate_flow` (Generate planning flow from requirements)
   - `generate_design_doc` (Generate detailed PRD)
   
   Supports multiple languages: `en`, `zh`, `ja`, `es`, `fr`

### 6) Success Validation (What You Should See)

* **CLI**: Real-time streaming of "Analysis â†’ Planning â†’ Research/Architecture â†’ Document Output" in terminal, with structured PRD fragments.
* **API**: Open `/docs` for Swagger, or use `curl` to get streaming/segmented responses; response body contains final document content and step-by-step process.
* **MCP**: Directly call corresponding tools in MCP-supporting editors (like Cursor / Cherry Studio) to generate planning/PRD.

### 7) Common Issues

* **Network/Dependency Issues**: Prioritize using `uv sync`, which significantly reduces environment pitfalls.
* **Models and Costs**: Any OpenAI-compatible server works; start with minimal context and short inputs to test the pipeline, then expand requirements.
* **Environment Variables**: Ensure `LLM_API_KEY`, `LLM_BASE_URL`, `LLM_MODEL` three core variables are correctly set.
* **MCP Service Configuration**: MCP service requires the same environment variables as the main service, confirm environment configuration before starting.
* **Tracing Configuration**: Langfuse is optional for execution tracking. Run `bash configure_langfuse.sh` for quick setup, or temporarily ignore.
* **Language**: `language` can be set to `zh | en | ja | es | fr`, or let the system auto-detect.
* **Compatibility**: Supports OpenAI, Azure OpenAI, Anthropic Claude (via proxy), and major domestic model service providers.

---

## âœ¨ Features

### ğŸ§  Intelligent Agent Capabilities
- **ğŸ¤– Intelligent Reasoning**: Intelligently analyzes user requirements to provide professional planning suggestions.
- **ğŸ”§ Function Calling**: Integrates with OpenAI Function Calling to support intelligent tool invocation.
- **ğŸ“Š Intelligent Planning**: Specialized planning capabilities for short-term plans, long-term designs, and architectural designs.
- **ğŸ” Technical Research**: Intelligent technical research and information gathering based on Jina Search.
- **ğŸ› ï¸ Tool Recommendation**: A vectorized tool recommendation system that intelligently matches the most suitable development tools.

### ğŸš€ Modern Architecture
- **âš¡ Stateless Design**: A stateless architecture that supports high concurrency and horizontal scaling.
- **ğŸ”„ Streaming Response**: Native support for Server-Sent Events (SSE) to provide a real-time user experience.
- **ğŸ’¾ Smart Storage**: Session management based on SQLite, supporting smart compression and data persistence.
- **ğŸ“ˆ Execution Tracing**: Integrated with pocketflow-tracing and Langfuse for detailed execution tracing.

### ğŸŒ Multi-Interface Support
- **ğŸ–¥ï¸ Modern CLI**: A command-line tool that supports session management, streaming display, and a multilingual interface.
- **ğŸŒ REST API**: A high-performance REST API service based on FastAPI.
- **ğŸ”Œ MCP Integration**: Supports the Model Context Protocol for seamless integration with AI assistants.
- **ğŸ–¼ï¸ Web UI**: Provides a complete web user interface when combined with the frontend.

### ğŸŒ Globalization Support
- **ğŸŒ Multilingual Support**: Full support for Chinese, English, Japanese, Spanish, and French, with automatic language detection.
- **ğŸ¯ Smart Language Detection**: Automatically identifies the user's language and provides corresponding localized responses.
- **ğŸ”§ LLM Compatibility**: Supports various large language models (OpenAI, Anthropic, etc.).

---

## ğŸ“‹ Environment Requirements (Backend and CLI)

- **Python**: 3.10 or higher
- **Package Manager**: [uv](https://github.com/astral-sh/uv) (recommended) or pip
- **LLM API Access**: Any OpenAI-compatible API endpoint (e.g., OpenAI, Anthropic, or a local model)

## ğŸš€ Installation (Backend and CLI)

1. Clone this repository

```bash
git clone https://github.com/OpenSQZ/GTPlanner.git
cd GTPlanner
```

2. Install dependencies

Using uv (recommended):
```bash
uv sync
```

Using pip:
```bash
pip install -r requirements.txt
```

3. Configuration

GTPlanner supports any OpenAI-compatible API. You can configure your LLM, API key, environment variables, and language in the `settings.toml` file. The default language is English.

```bash
export LLM_API_KEY="your-api-key-here"
```

## ğŸ› ï¸ Usage

### ğŸ–¥ï¸ CLI Mode

GTPlanner offers a modern CLI based on a new streaming response architecture, supporting real-time streaming display, session management, and a multilingual interface.

![GTPlanner CLI](assets/cil.png)

#### Interactive Mode

Start the interactive CLI for a conversational experience:
```bash
python gtplanner.py
# or
python agent/cli/gtplanner_cli.py
```

**Core Features:**
- ğŸ”„ **Real-time Streaming Response**: See the AI's thought process and tool execution in real-time.
- ğŸ’¾ **Session Management**: Automatic persistence of conversation history, with support for loading and switching sessions.
- ğŸ¤– **Function Calling**: Native support for OpenAI Function Calling.
- ğŸ“Š **Multiple Tools**: Professional tools for requirements analysis, technical research, architectural design, and more.
- ğŸŒ **Multilingual Interface**: Supports interfaces in Chinese, English, Japanese, Spanish, and French.

#### Direct Execution Mode

Process requirements directly without entering interactive mode:
```bash
python gtplanner.py "Design a user management system"
python agent/cli/gtplanner_cli.py "Analyze the requirements for an e-commerce platform"
```

#### Session Management

**Load an existing session:**
```bash
python gtplanner.py --load <session_id>
```

**Available commands in interactive mode:**
- `/help` - Show available commands
- `/new` - Create a new session
- `/sessions` - List all sessions
- `/load <id>` - Load a specific session
- `/delete <id>` - Delete a specific session
- `/stats` - Show performance statistics
- `/verbose` - Toggle verbose mode
- `/quit` - Exit the CLI

**Common Parameters:**
- `--verbose, -v`: Display detailed processing information.
- `--load <session_id>`: Load a specific conversation session.
- `--language <zh|en|ja|es|fr>`: Set the interface language.

### ğŸŒ FastAPI Backend

Start the REST API service:

```bash
uv run fastapi_main.py
```

The service runs by default at `http://0.0.0.0:11211`. Access `http://0.0.0.0:11211/docs` to view the interactive API documentation.

**Core Features:**
- **ğŸ”„ SSE Streaming Response**: Real-time data transmission based on Server-Sent Events.
- **ğŸ¤– Agent API**: Uses `StatelessGTPlanner` to provide stateless, high-concurrency processing.
- **ğŸ“Š Real-time Tool Invocation**: Real-time display of tool execution status and progress.
- **ğŸŒ Multilingual Support**: The API natively supports multilingual processing and responses.

**Main Endpoints:**

*   **Agent Streaming Chat Endpoint (Recommended)**
    *   `POST /api/chat/agent`: A streaming Agent chat endpoint based on SSE that integrates intelligent reasoning, tool invocation, and real-time responses. This is the preferred interface for building interactive applications.

*   **Health Check Endpoints**
    *   `GET /health`: An enhanced health check endpoint that includes API status information.
    *   `GET /api/status`: Get detailed API status information.



### ğŸ”Œ MCP Service (Recommended for AI Integration)

The MCP service can be seamlessly integrated with AI assistants and supports direct function calls.

1. Start the MCP service.

```bash
cd mcp
uv sync
uv run python mcp_service.py
```

2. Configure your MCP client.

```json
{
  "mcpServers": {
    "GT-planner": {
      "url": "http://127.0.0.1:8001/mcp"
    }
  }
}
```

**Available MCP Tools:**
- `generate_flow`: Generate a planning flow from requirements.
- `generate_design_doc`: Create a detailed PRD.

---

## ğŸ—ï¸ System Architecture

GTPlanner adopts a modern Agent architecture built using the PocketFlow asynchronous workflow engine:

### ğŸ§  Core Agent Architecture

1.  **Main Controller Flow** (`agent/flows/react_orchestrator_refactored/`)
    -   Intelligent task orchestration and flow control.
    -   Supports execution tracing with `pocketflow_tracing`.
    -   Coordinates node execution and context passing.

2.  **StatelessGTPlanner** (`agent/stateless_planner.py`)
    -   A completely stateless implementation of GTPlanner that supports high concurrency.
    -   Native support for streaming responses.
    -   Purely functional design, where each call is completely independent.

3.  **Function Calling System** (`agent/function_calling/`)
    -   Intelligent tool invocation integrated with OpenAI Function Calling.
    -   Professional tools for short-term planning, technical research, architectural design, tool recommendation, and more.
    -   Supports asynchronous tool execution and result handling.

4.  **Streaming System** (`agent/streaming/`)
    -   A streaming response system based on Server-Sent Events.
    -   Supports real-time message transmission and tool invocation status updates.
    -   Type-safe stream event handling.

### ğŸ”„ Intelligent Workflow

```mermaid
flowchart TD
    A[User Input] --> B[Main Controller]
    B --> C{Requirement Analysis}
    C --> D[Understand Requirements]
    D --> E{Task Planning}
    E --> F[Invoke Tools]
    F --> G[Short-Term Planning]
    F --> H[Technical Research]
    F --> I[Architecture Design]
    F --> J[Tool Recommendation]
    G --> K[Integrate Results]
    H --> K
    I --> K
    J --> K
    K --> L{Continue?}
    L -->|Yes| C
    L -->|No| M[Generate Final Result]
```

### ğŸ› ï¸ Specialized Subflows

- **Short-Term Planning** (`agent/subflows/short_planning/`): Generates high-level project plans and task breakdowns.
- **Technical Research** (`agent/subflows/research/`): Intelligent technical research based on Jina Search.
- **Architecture Design** (`agent/subflows/architecture/`): In-depth architecture design and technology selection.
- **Tool Recommendation** (`tools/`): A vectorized tool recommendation system that supports API and Python package recommendations.

---

## ğŸ“¦ Project Structure

```
GTPlanner/
â”œâ”€â”€ gtplanner.py               # Main CLI startup script
â”œâ”€â”€ fastapi_main.py           # FastAPI backend service
â”œâ”€â”€ settings.toml             # Configuration file
â”œâ”€â”€ pyproject.toml            # Project metadata and dependencies
â”œâ”€â”€ agent/                     # Core Agent system
â”‚   â”œâ”€â”€ __init__.py           # Agent module entry point
â”‚   â”œâ”€â”€ gtplanner.py          # Stateful GTPlanner main controller
â”‚   â”œâ”€â”€ stateless_planner.py  # Stateless GTPlanner implementation
â”‚   â”œâ”€â”€ context_types.py      # Stateless data type definitions
â”‚   â”œâ”€â”€ pocketflow_factory.py # PocketFlow data conversion factory
â”‚   â”œâ”€â”€ flows/                # Main control flows
â”‚   â”‚   â””â”€â”€ react_orchestrator_refactored/ # Main controller flow
â”‚   â”œâ”€â”€ subflows/             # Specialized Agent subflows
â”‚   â”‚   â”œâ”€â”€ short_planning/   # Short-term planning subflow
â”‚   â”‚   â”œâ”€â”€ research/         # Technical research subflow
â”‚   â”‚   â””â”€â”€ architecture/     # Architecture design subflow
â”‚   â”œâ”€â”€ nodes/                # Atomic capability nodes
â”‚   â”‚   â”œâ”€â”€ node_search.py    # Search engine node
â”‚   â”‚   â”œâ”€â”€ node_url.py       # URL parsing node
â”‚   â”‚   â”œâ”€â”€ node_compress.py  # Context compression node
â”‚   â”‚   â””â”€â”€ node_output.py    # Output document node
â”‚   â”œâ”€â”€ function_calling/     # Function Calling tools
â”‚   â”‚   â””â”€â”€ agent_tools.py    # Agent tool definitions
â”‚   â”œâ”€â”€ streaming/            # Streaming response system
â”‚   â”‚   â”œâ”€â”€ stream_types.py   # Stream event type definitions
â”‚   â”‚   â”œâ”€â”€ stream_interface.py # Streaming session interface
â”‚   â”‚   â””â”€â”€ sse_handler.py    # SSE handler
â”‚   â”œâ”€â”€ api/                  # Agent API implementation
â”‚   â”‚   â””â”€â”€ agent_api.py      # SSE GTPlanner API
â”‚   â”œâ”€â”€ cli/                  # Modern CLI implementation
â”‚   â”‚   â”œâ”€â”€ gtplanner_cli.py  # Main CLI implementation
â”‚   â”‚   â””â”€â”€ cli_text_manager.py # CLI multilingual text manager
â”‚   â””â”€â”€ persistence/          # Data persistence
â”‚       â”œâ”€â”€ sqlite_session_manager.py # SQLite session manager
â”‚       â””â”€â”€ smart_compressor.py # Smart compressor
â”œâ”€â”€ mcp/                      # MCP service
â”‚   â”œâ”€â”€ mcp_service.py       # MCP server implementation
â”‚   â””â”€â”€ pyproject.toml       # MCP-specific dependencies
â”œâ”€â”€ tools/                    # Tool recommendation system
â”‚   â”œâ”€â”€ apis/                # API-type tool definitions
â”‚   â””â”€â”€ python_packages/     # Python package-type tool definitions
â”œâ”€â”€ utils/                    # Utility functions
â”‚   â””â”€â”€ config_manager.py    # Configuration manager
â”œâ”€â”€ docs/                     # Design documents
â””â”€â”€ assets/                   # Project assets
```

---

## ğŸ“š Dependencies

### Core Dependencies
- **Python** >= 3.11 - Runtime environment
- **openai** >= 1.0.0 - LLM API communication
- **pocketflow** == 0.0.3 - Asynchronous workflow engine
- **pocketflow-tracing** >= 0.1.4 - Execution tracing system
- **dynaconf** >= 3.1.12 - Configuration management
- **aiohttp** >= 3.8.0 - Asynchronous HTTP client
- **json-repair** >= 0.45.0 - JSON response repair
- **python-dotenv** >= 1.0.0 - Environment variable loading

### API Dependencies
- **fastapi** == 0.115.9 - REST API framework
- **uvicorn** == 0.23.1 - ASGI server
- **pydantic** >= 2.5.0 - Data validation

### CLI Dependencies
- **rich** >= 13.0.0 - Terminal beautification and interaction

### MCP Dependencies
- **fastmcp** - Model Context Protocol (MCP) implementation

### Development Dependencies
- **pytest** >= 8.4.1 - Testing framework
- **pytest-asyncio** >= 1.1.0 - Asynchronous test support

---

## ğŸŒ Multilingual Support

GTPlanner offers comprehensive multilingual support, allowing developers worldwide to use their native language for project planning.

### Supported Languages

| Language | Code | Native Name |
|----------|------|-------------|
| English  | `en` | English     |
| Chinese  | `zh` | ä¸­æ–‡        |
| Spanish  | `es` | EspaÃ±ol     |
| French   | `fr` | FranÃ§ais    |
| Japanese | `ja` | æ—¥æœ¬èª      |

### Core Features

- **ğŸ” Automatic Language Detection**: Intelligently identifies the language of the user's input.
- **ğŸ¯ Language Priority System**: Automatically selects the most appropriate language based on user preference and the request.
- **ğŸ“ Localized Prompt Templates**: Provides culturally adapted prompt templates for each language.
- **ğŸ”„ Smart Fallback Mechanism**: Automatically falls back to the default language when the requested language is unavailable.

### Usage

#### CLI Mode
```bash
# Specify the language
python gtplanner.py --language zh ""Summarize the WeChat group chat and create user profiles for members"

# Automatic detection (inputting Chinese will be automatically recognized)
python gtplanner.py ""Summarize the WeChat group chat and create user profiles for members"
```

#### API Mode
```python
# Explicitly specify the language
response = requests.post("/api/chat/agent", json={
    "session_id": "test-session",
    "dialogue_history": [{"role": "user", "content": ""Summarize the WeChat group chat and create user profiles for members"}],
    "language": "zh"
})

# Automatic detection
response = requests.post("/api/chat/agent", json={
    "session_id": "test-session",
    "dialogue_history": [{"role": "user", "content": ""Summarize the WeChat group chat and create user profiles for members"}]
})
```

### Configuration

Configure multilingual settings in `settings.toml`:

```toml
[default.multilingual]
default_language = "en"
auto_detect = true
fallback_enabled = true
supported_languages = ["en", "zh", "es", "fr", "ja"]
```

For detailed multilingual feature descriptions and configuration guides, please refer to the [Multilingual Guide](docs/multilingual-guide.md).

---

## ğŸ¤ Contributing

We believe that an excellent tool is built on the wisdom and collaboration of the community. GTPlanner welcomes your participation to jointly shape a more powerful planning ecosystem:

### ğŸ”§ Contribute Tools - Expand the Planner's Knowledge Base
Help GTPlanner learn about more available solutions so it can make accurate recommendations during planning:
- **ğŸŒ API Tools** - Web APIs, REST services, platform integrations
- **ğŸ“¦ Python Packages** - PyPI libraries, data analysis packages, utility tools
- **ğŸ”Œ MCP Services** - Private services that follow the MCP specification

### ğŸ’» Contribute Core Code - Prove Optimizations with Data
Improve planning quality and system performance through evaluation-driven development.

### ğŸ“š Share Case Studies - Inspire the Community
Share your use cases, tutorials, and best practices to help the community unlock the full potential of GTPlanner.

### ğŸ“– Detailed Guide
For complete contribution methods, technical specifications, and submission processes, please see:
**[Contribution Guide](CONTRIBUTING.md)** - Includes detailed contribution flows, templates, and examples.

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE.md) file for details.

## ğŸ™ Acknowledgements

- Built on the [PocketFlow](https://github.com/The-Pocket/PocketFlow) asynchronous workflow engine
- Configuration management powered by [Dynaconf](https://www.dynaconf.com/)
- Designed for seamless integration with AI assistants via the MCP protocol


---

**GTPlanner** - Use the power of AI to transform your ideas into structured technical documents.